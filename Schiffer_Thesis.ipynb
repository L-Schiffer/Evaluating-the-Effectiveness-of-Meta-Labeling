{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Master Thesis - Lars Schiffer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys used for FRED, Alpha Vantage and SEC\n",
    "fred_api_key = ''\n",
    "alpha_vantage_api_key = ''\n",
    "sec_api_key = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from math import sqrt\n",
    "from time import sleep\n",
    "\n",
    "# Third-party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import joblib\n",
    "import torch.nn.init as init\n",
    "import lightgbm as lgb\n",
    "import wandb\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import statsmodels.tsa.stattools as stattools\n",
    "# import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from sec_api import MappingApi\n",
    "from arch import arch_model\n",
    "from scipy.stats import skew, kurtosis, linregress\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    log_loss,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "from torch.nn.modules.activation import Sigmoid\n",
    "from torch import nn\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air import session\n",
    "\n",
    "# Additional Libraries which were used on Windows due to dependency issues on Linux and conda\n",
    "\n",
    "# import talib\n",
    "# from fracdiff import Fracdiff, fdiff\n",
    "# from fracdiff.sklearn import FracdiffStat\n",
    "# import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "path_default = '/home/lschiff_ext' # default path\n",
    "path_data_raw = '/home/lschiff_ext/data_v3/raw' # concatenated 16k raw files\n",
    "path_data_trading_daily = '/home/lschiff_ext/data_v2/trading_daily' # initial data with 16k files for stock data\n",
    "path_data_strategy = '/home/lschiff_ext/data_v5/strategy' # applied features + trading strategy\n",
    "path_data_features = '/home/lschiff_ext/data_v2/features' # applied features\n",
    "path_data_external = '/home/lschiff_ext/data_v3/external' # external apis\n",
    "path_data_combined = '/home/lschiff_ext/data_v5/combined' # combined data from strategy and external features\n",
    "path_data_buy_hold = '/home/lschiff_ext/data_v5/buy_and_hold' # combined data from strategy and external features\n",
    "path_data_features_json_tbm = '/home/lschiff_ext/data_v2/features/v1_features_tbm.json' # applied features\n",
    "path_data_features_json_bb = '/home/lschiff_ext/data_v2/features/v1_features_bb.json' # applied features\n",
    "path_data_final_models = '/home/lschiff_ext/data_v5/predictions/models'\n",
    "\n",
    "\n",
    "# final data used for training\n",
    "# TBM with fees\n",
    "path_data_partitioned_tbm_with_fees_train = '/home/lschiff_ext/data_v5/partitioned/with_fees/tbm/train'\n",
    "path_data_partitioned_tbm_with_fees_test = '/home/lschiff_ext/data_v5/partitioned/with_fees/tbm/test'\n",
    "path_data_partitioned_tbm_with_fees_pca = '/home/lschiff_ext/data_v5/partitioned/with_fees/tbm/pca'\n",
    "path_data_partitioned_tbm_with_fees_scaled = '/home/lschiff_ext/data_v5/partitioned/with_fees/tbm/scaled'\n",
    "\n",
    "# BB with fees\n",
    "path_data_partitioned_bb_with_fees_train = '/home/lschiff_ext/data_v5/partitioned/with_fees/bb/train'\n",
    "path_data_partitioned_bb_with_fees_test = '/home/lschiff_ext/data_v5/partitioned/with_fees/bb/test'\n",
    "path_data_partitioned_bb_with_fees_pca = '/home/lschiff_ext/data_v5/partitioned/with_fees/bb/pca'\n",
    "path_data_partitioned_bb_with_fees_scaled ='/home/lschiff_ext/data_v5/partitioned/with_fees/bb/scaled'\n",
    "\n",
    "# Feature Selection\n",
    "path_data_features_rfe_bb_xgboost = '/home/lschiff_ext/data_v2/features/v1_features_bb_RFE_XGBoost.json'\n",
    "path_data_features_rfe_bb_lgbm = '/home/lschiff_ext/data_v2/features/v2_features_bb_RFE_LGBM_w_vert_barrier.json'\n",
    "path_data_features_rfe_tbm_scaled_rob_lgbm = '/home/lschiff_ext/data_v2/features/v1_features_tbm_RFE_LGBM_Scaled_rob.json'\n",
    "\n",
    "\n",
    "# Model Training Results\n",
    "path_data_results_bb_rf = '/home/lschiff_ext/data_v2/results/bb/random_forest'\n",
    "path_data_results_tbm_rf = '/home/lschiff_ext/data_v2/results/tbm/random_forest'\n",
    "\n",
    "path_data_results_bb_nn = '/home/lschiff_ext/data_v3/results/bb/neural_network'\n",
    "path_data_results_tbm_nn = '/home/lschiff_ext/data_v3/results/tbm/neural_network'\n",
    "\n",
    "path_data_results_bb_log = '/home/lschiff_ext/data_v2/results/bb/logistic_regression'\n",
    "path_data_results_tbm_log = '/home/lschiff_ext/data_v2/results/tbm/logistic_regression'\n",
    "\n",
    "path_data_results_bb_xgb = '/home/lschiff_ext/data_v2/results/bb/xgboost'\n",
    "path_data_results_tbm_xgb = '/home/lschiff_ext/data_v2/results/tbm/xgboost'\n",
    "\n",
    "hold_out_date = pd.to_datetime('2020-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_rows(df, end = False, no_rows = 1):\n",
    "    \"\"\"\n",
    "    Print rows of a DataFrame to get insights into the data.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The DataFrame to print rows from.\n",
    "        end (boolean, optional): If True, print rows from the end of the DataFrame. \n",
    "                              If False, print rows from the beginning. \n",
    "                              Default is set to False.\n",
    "        no_rows (integer, optional): The number of rows to print. Default is set to 1.\n",
    "    \"\"\"\n",
    "    # Get the specified number of rows from the DataFrame\n",
    "    if end:\n",
    "        idx_row = df.tail(no_rows)\n",
    "    else:\n",
    "        idx_row = df.head(no_rows)\n",
    "    \n",
    "    # Print each row\n",
    "    for index, row in idx_row.iterrows():\n",
    "        print(f\"{index}\" + '-'*30)\n",
    "        for col_name in idx_row.columns:\n",
    "            print(f\"{col_name}: {row[col_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    Selects specific columns from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): The input dataframe.\n",
    "        columns (list): The list of column names to select.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe: A new dataframe containing only the selected columns.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns are missing from the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the missing columns\n",
    "    missing_columns = set(columns) - set(df.columns)\n",
    "    \n",
    "    # Raise an error if there are missing columns\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following columns are missing from the dataframe: {missing_columns}\")\n",
    "    \n",
    "    # Select the specified columns and create a copy of the dataframe\n",
    "    return df.loc[:, columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_stock_data(asset, start_date, end_date, save = False):\n",
    "    \"\"\"\n",
    "    Retrieves stock data for a given share, start date, and end date.\n",
    "    \n",
    "    Args:\n",
    "        asset (string): The stock symbol (CUSIP).\n",
    "        start_date (string): The start date in 'YYYY-MM-DD' format.\n",
    "        end_date (string): The end date in 'YYYY-MM-DD' format.\n",
    "        save (boolean, optional): Whether to save the data to a file. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: The stock data with columns: ['open', 'high', 'low', 'close', 'volume'].\n",
    "    \"\"\"\n",
    "    name = f\"{asset}--{start_date}--{end_date}.csv\"\n",
    "    location = os.path.join(path_data_raw, name)\n",
    "    \n",
    "    if not os.path.exists(location):\n",
    "        # Data does not exist, download and save\n",
    "        data = yf.download(asset, start=start_date, end=end_date)\n",
    "        data.to_csv(os.path.join(path_data_raw, name))\n",
    "        print('Data received from Yahoo.')\n",
    "    \n",
    "    data = pd.read_csv(location)\n",
    "    data = data.set_index(pd.DatetimeIndex(data['Date'].values))\n",
    "    data = select_columns(data, ['Open', 'High', 'Low', 'Adj Close', 'Volume'])\n",
    "    data = data.rename(columns={'Open': 'open', 'High': 'high', 'Adj Close': 'close', 'Low' : 'low', 'Volume' : 'volume'})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_from_json(X_features, path):\n",
    "    \"\"\"\n",
    "    Selects the specified features from a JSON file and returns the filtered dataframe.\n",
    "\n",
    "    Args:\n",
    "        X_features (dataframe): The input dataframe.\n",
    "        path (string): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The filtered dataframe containing only the specified features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the JSON file as a dictionary\n",
    "    with open(path) as f:\n",
    "        features_dict = json.load(f)\n",
    "        \n",
    "    # Create a list of column names to keep\n",
    "    columns_to_keep = [k for k, v in features_dict.items() if v == 1]\n",
    "\n",
    "    # Filter the dataframe based on the columns to keep\n",
    "    filtered_df = X_features[columns_to_keep]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_csv():\n",
    "    \"\"\"\n",
    "    Reads data from a CSV file and returns a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Define the data type dictionary for the columns\n",
    "    dtype_dict = {\n",
    "        'cusip': str,\n",
    "        'open': float,\n",
    "        'high': float,\n",
    "        'low': float,\n",
    "        'close': float,\n",
    "        'volume': float\n",
    "    }\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv('/home/lschiff_ext/data_v2/raw/v1_trading_combined.csv', dtype=dtype_dict, parse_dates=['date'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_processed_csv(full_path, prefix = 'feature_'):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and processes the data.\n",
    "\n",
    "    Args:\n",
    "        full_path (string): The full path to the CSV file.\n",
    "        prefix (string, optional): The prefix to be added to the column names. Defaults to 'feature_'.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The processed data.\n",
    "    \"\"\"\n",
    "    # Define the data types for the columns\n",
    "    dtype_dict = {\n",
    "        'cusip': object,\n",
    "        'open': float,\n",
    "        'high': float,\n",
    "        'low': float,\n",
    "        'close': float,\n",
    "        'volume': float,\n",
    "        'bin': int\n",
    "    }\n",
    "\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(full_path, dtype=dtype_dict, parse_dates=['date', 'vertical_barrier', 't1'])\n",
    "\n",
    "    # Format the date columns\n",
    "    data['date'] = data['date'].dt.strftime('%Y-%m-%d')\n",
    "    data['vertical_barrier'] = data['vertical_barrier'].dt.strftime('%Y-%m-%d')\n",
    "    data['t1'] = data['t1'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert the date columns to datetime\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['vertical_barrier'] = pd.to_datetime(data['vertical_barrier'])\n",
    "    data['t1'] = pd.to_datetime(data['t1'])\n",
    "\n",
    "    # Add OHLC data as features, add prefix\n",
    "    new_columns = {col: prefix + col for col in ['open', 'high', 'low', 'close']}\n",
    "    data = data.rename(columns=new_columns)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_raw_csv(full_path):\n",
    "    \"\"\"\n",
    "    Read data from a CSV file and return a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        full_path (string): The full path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: A DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Define the data types for each column\n",
    "    dtype_dict = {\n",
    "        'cusip': str,\n",
    "        'open': float,\n",
    "        'high': float,\n",
    "        'low': float,\n",
    "        'close': float,\n",
    "        'volume': float,\n",
    "    }\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(full_path, dtype=dtype_dict, parse_dates=['date'])\n",
    "\n",
    "    # Format the date column as 'YYYY-MM-DD'\n",
    "    data['date'] = data['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert the date column to datetime\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_data(path, file_name, vertical_barrier = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ML data from a file.\n",
    "\n",
    "    Args:\n",
    "        path (string): The path to the directory containing the file.\n",
    "        file_name (string): The name of the file to load.\n",
    "        vertical_barrier (boolean, optional): Whether to include the vertical barrier column. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The loaded ML data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the data types for the columns\n",
    "    dtype_dict = {\n",
    "        'cusip': str,\n",
    "        'bin': int,\n",
    "        'side': int,\n",
    "        'profit_abs': np.float32,\n",
    "        't1_price': np.float32,\n",
    "    }\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    if vertical_barrier:\n",
    "        data = pd.read_csv(path + '/' + file_name, dtype=dtype_dict, parse_dates=['date', 't1', 'vertical_barrier'])\n",
    "    else:\n",
    "        data = pd.read_csv(path + '/' + file_name, dtype=dtype_dict, parse_dates=['date', 't1'])\n",
    "\n",
    "    # Set data type of columns starting with \"feature_\" to float32\n",
    "    feature_cols_floats = [col for col in data.columns if col.startswith('feature_')]\n",
    "    data[feature_cols_floats] = data[feature_cols_floats].astype(np.float32)\n",
    "\n",
    "    # Set data type for features with int (which are the CDL features)\n",
    "    feature_cols_int = [col for col in data.columns if col.startswith('feature_CDL')]  # all CDL features are int\n",
    "    data[feature_cols_int] = data[feature_cols_int].astype(int)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractional Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the fractional differentiation is tested\n",
    "# The following lines of code are not directly related to the feature engineering\n",
    "# Was used to get an idea of the fractional differentiation\n",
    "\n",
    "# define Apple stock and timeframe\n",
    "share = \"AAPL\"\n",
    "start_date = \"2005-01-01\"\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Retrieve stock_data\n",
    "data = retrieve_stock_data(share, start_date, end_date, True)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(24, 24))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, d in enumerate(np.linspace(0.1, 0.9, 9)):\n",
    "    diff = fdiff(data['close'], d)\n",
    "    diff = pd.Series(diff, index=data.index[-diff.size :])\n",
    "    plt.subplot(9, 1, i + 1)\n",
    "    plt.title(f\"Stock Portfolio, {d:.1f}th differentiated\")\n",
    "    plt.plot(diff, linewidth=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationary of fractional differentiation\n",
    "def adfstat(d: float) -> float:\n",
    "    diff = fdiff(data['close'], d)\n",
    "    stat, *_ = stattools.adfuller(diff)\n",
    "    return stat\n",
    "\n",
    "def correlation(d: float) -> np.ndarray:\n",
    "    diff = fdiff(data['close'], d)\n",
    "    return np.corrcoef(data['close'][-diff.size :], diff)[0, 1]\n",
    "\n",
    "ds = np.linspace(0.0, 1.0, 10)\n",
    "stats = np.vectorize(adfstat)(ds)\n",
    "corrs = np.vectorize(correlation)(ds)\n",
    "\n",
    "# 5% critical value of stationarity\n",
    "_, _, _, _, crit, _ = stattools.adfuller(data['close'])\n",
    "\n",
    "# plot\n",
    "fig, ax_stat = plt.subplots(figsize=(24, 8))\n",
    "ax_corr = ax_stat.twinx()\n",
    "\n",
    "ax_stat.plot(ds, stats, color=\"blue\", label=\"ADF statistics (left)\")\n",
    "ax_corr.plot(ds, corrs, color=\"orange\", label=\"correlation (right)\")\n",
    "ax_stat.axhline(y=crit[\"5%\"], linestyle=\"--\", color=\"k\", label=\"5% critical value\")\n",
    "\n",
    "plt.title(\"Stationarity and memory of fractionally differentiated Stock\")\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['close'].values.reshape(-1, 1)\n",
    "\n",
    "fs = FracdiffStat()\n",
    "\n",
    "Xdiff = fs.fit_transform(X)\n",
    "_, pvalue, _, _, _, _ = stattools.adfuller(Xdiff.reshape(-1))\n",
    "corr = np.corrcoef(X[-Xdiff.size :, 0], Xdiff.reshape(-1))[0][1]\n",
    "\n",
    "print(\"* Order: {:.2f}\".format(fs.d_[0]))\n",
    "print(\"* ADF p-value: {:.2f} %\".format(100 * pvalue))\n",
    "print(\"* Correlation with the original time-series: {:.2f}\".format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frac_diff'] = fdiff(data['close'], fs.d_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frac_diff'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FREDÂ® API (Federal Reserve Bank of St. Louis) & yFinance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_fred_api(series_id, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Requests data from the FRED API for a given series ID and date range.\n",
    "    \n",
    "    Args:\n",
    "        series_id (string): The series ID to fetch data for.\n",
    "        start_date (string): The start date of the date range.\n",
    "        end_date (string): The end date of the date range.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: The fetched data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Construct the API URL\n",
    "    url = f'https://api.stlouisfed.org/fred/series/observations'\n",
    "    url += f'?series_id={series_id}&api_key={fred_api_key}'\n",
    "    url += f'&file_type=json&observation_start={start_date}&observation_end={end_date}'\n",
    "    \n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    try:\n",
    "        # Extract the data from the API response\n",
    "        data = json.loads(response.text)['observations']\n",
    "    except KeyError:\n",
    "        # Handle error if data couldn't be fetched\n",
    "        print(f\"Error fetching data for {series_id}. Response: {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create a DataFrame from the fetched data\n",
    "    df = pd.DataFrame(data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df[['value']]\n",
    "    df.columns = [series_id]\n",
    "    df[series_id] = pd.to_numeric(df.iloc[:,0], errors='coerce')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_yfinance_data(ticker, start_date, end_date) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Requests stock data from Yahoo Finance API.\n",
    "\n",
    "    Args:\n",
    "        ticker (string): The stock ticker symbol.\n",
    "        start_date (string): The start date for the data.\n",
    "        end_date (string): The end date for the data.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The stock data for the specified dates.\n",
    "    \"\"\"\n",
    "    # Download stock data from Yahoo Finance API\n",
    "    yf_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    \n",
    "    return yf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_percentile(series, rolling_window = None):\n",
    "    \"\"\"\n",
    "    Calculates the rolling percentile of a given series.\n",
    "\n",
    "    Args:\n",
    "        series (series): The input series (most often a given feature).\n",
    "        rolling_window (integer, optional): The size of the rolling window. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Series: The rolling percentile values.\n",
    "    \"\"\"\n",
    "    if rolling_window is None:\n",
    "        # Calculate the expanding percentile\n",
    "        percentiles = series.expanding().apply(lambda x: pd.Series(x).rank(pct=True).iloc[-1])\n",
    "    else:\n",
    "        # Calculate the rolling percentile\n",
    "        percentiles = series.rolling(window=rolling_window, min_periods=1).apply(lambda x: pd.Series(x).rank(pct=True).iloc[-1])\n",
    "    \n",
    "    return percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile_and_change(data, feature, rolling_window = None):\n",
    "    \"\"\"\n",
    "    Compute the rolling percentile and percentage change for the given feature in the data.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): The input data (FRED and yFinance requested data)\n",
    "        feature (string): The name of the feature to compute the percentile and change for.\n",
    "        rolling_window (integer, optional): The size of the rolling window. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The input data with additional columns for the rolling percentile and percentage change.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate rolling percentile for the entire DataFrame\n",
    "    percentile_df = data[feature].transform(lambda x: rolling_percentile(x, rolling_window)).to_frame(f'{feature}_rolling_percentile')\n",
    "\n",
    "    # Calculate percentage change for the entire DataFrame\n",
    "    change_df = data[feature].transform(lambda x: x.pct_change()).to_frame(f'{feature}_change')\n",
    "\n",
    "    # Combine the original data with the change and percentile dataframes\n",
    "    combined = pd.concat([data, change_df, percentile_df], axis=1)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMA(series, period):\n",
    "    \"\"\"\n",
    "    Weighted Moving Average (WMA) function.\n",
    "\n",
    "    Args:\n",
    "        series (series): The input series.\n",
    "        period (integer): The number of periods to calculate the weighted moving average.\n",
    "\n",
    "    Returns:\n",
    "        Series: The weighted moving average of the input series.\n",
    "    \"\"\"\n",
    "    # Create an array of weights from 1 to period\n",
    "    weights = np.arange(1, period + 1)\n",
    "    \n",
    "    # Calculate the weighted moving average using rolling window and dot product\n",
    "    wma = series.rolling(window=period).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n",
    "    \n",
    "    return wma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMA(series, period):\n",
    "    \"\"\"\n",
    "    Calculates the Hull Moving Average (HMA) of a series.\n",
    "\n",
    "    Args:\n",
    "        series (series): The input series.\n",
    "        period (integer): The period for calculating the HMA.\n",
    "\n",
    "    Returns:\n",
    "        Series: The calculated HMA value.\n",
    "    \"\"\"\n",
    "    # Calculate the half period\n",
    "    half_period = int(period / 2)\n",
    "    \n",
    "    # Calculate the weighted moving average for the first half period\n",
    "    wma1 = WMA(series, half_period)\n",
    "    \n",
    "    # Calculate the weighted moving average for the full period\n",
    "    wma2 = WMA(series, period)\n",
    "    \n",
    "    # Calculate the weighted moving average of the difference between 2*wma1 and wma2\n",
    "    hma = WMA(2*wma1 - wma2, int(np.sqrt(period)))\n",
    "    \n",
    "    # Return the calculated HMA value\n",
    "    return hma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_garch_and_get_volatility(series):\n",
    "    \"\"\"\n",
    "    Fits a GARCH model to a time series and returns the conditional volatility.\n",
    "\n",
    "    Args:\n",
    "        series (series): The time series data (here: the closing prices of the asset).\n",
    "\n",
    "    Returns:\n",
    "        Series: The conditional volatility of the time series.\n",
    "    \"\"\"\n",
    "    # Define the number of lag variances to include in the GARCH model.\n",
    "    p = 1\n",
    "    # Define the number of lag residual errors to include in the GARCH model.\n",
    "    q = 1\n",
    "    \n",
    "    # Create the GARCH model with the specified parameters.\n",
    "    model = arch_model(series, vol='Garch', p=p, q=q)\n",
    "    \n",
    "    # Fit the model to the data.\n",
    "    results = model.fit(update_freq=0, disp='off')\n",
    "    \n",
    "    # Get the conditional volatility from the fitted model.\n",
    "    volatility = results.conditional_volatility\n",
    "    \n",
    "    # Return the conditional volatility.\n",
    "    return volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the z-score of a series using a rolling window.\n",
    "    \n",
    "    Args:\n",
    "        series (series): The input series (here: the closing prices of the asset).\n",
    "        window (integer): The size of the rolling window.\n",
    "    \n",
    "    Returns:\n",
    "        Series: The z-score of the input series.\n",
    "    \"\"\"\n",
    "    # Calculate the rolling mean with a shifted window\n",
    "    rolling = series.rolling(window=window)\n",
    "    rolling_mean = rolling.mean().shift(1)\n",
    "    \n",
    "    # Calculate the rolling standard deviation with a shifted window\n",
    "    std_dev = rolling.std(ddof=0).shift(1)\n",
    "    \n",
    "    # Calculate the z-score\n",
    "    z = (series - rolling_mean) / std_dev\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_hurst(series):\n",
    "    \"\"\"\n",
    "    Calculates the Hurst exponent for a given time series using a rolling window approach.\n",
    "    \n",
    "    Args:\n",
    "        series (series): The input time series data (here: applied to closing prices of the asset).\n",
    "        \n",
    "    Returns:\n",
    "        float: The calculated Hurst exponent.\n",
    "    \"\"\"\n",
    "    # Calculate log returns\n",
    "    log_ret = np.log(series).diff()\n",
    "\n",
    "    # Define the range of lags\n",
    "    lags = range(2, 9)  # Use a smaller range of lags, e.g., 2 to 8\n",
    "\n",
    "    # Calculate the variance of log returns for each lag\n",
    "    variances = [log_ret.shift(lag).dropna().var() for lag in lags]\n",
    "\n",
    "    # Perform linear regression on the log-log plot of variances\n",
    "    regression = linregress(np.log10(lags), np.log10(variances))\n",
    "\n",
    "    # Calculate the Hurst exponent (slope of the regression line)\n",
    "    hurst_exponent = regression[0] / 2\n",
    "\n",
    "    return hurst_exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dpo(data, period, min_period):\n",
    "    \"\"\"\n",
    "    Calculate the Detrended Price Oscillator (DPO) for a given data series.\n",
    "\n",
    "    Args:\n",
    "        data (series): The input data series (here: the closing prices of the asset).\n",
    "        period (integer): The number of periods to consider for the moving average.\n",
    "        min_period (integer): The minimum number of periods required to calculate the moving average.\n",
    "\n",
    "    Returns:\n",
    "        Series: The calculated Detrended Price Oscillator (DPO) series.\n",
    "    \"\"\"\n",
    "    # Shift the price series by half the period + 1\n",
    "    shifted_price = data.shift(period // 2 + 1)\n",
    "\n",
    "    # Calculate the simple moving average using the given period and min_period\n",
    "    simple_moving_average = data.rolling(window=period, min_periods=min_period).mean()\n",
    "\n",
    "    # Calculate the Detrended Price Oscillator by subtracting the simple moving average from the shifted price\n",
    "    dpo = shifted_price - simple_moving_average\n",
    "\n",
    "    return dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_certain_ta_lib_features(df, prefix = 'feature_'):\n",
    "    \"\"\"\n",
    "    Add certain technical analysis features to the given dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): The input dataframe (here: the data of one asset).\n",
    "        prefix (string, optional): The prefix to be added to the feature names. Defaults to 'feature_'.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe: The dataframe with the added technical analysis features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dataframe to store the technical features\n",
    "    df_technical_features = pd.DataFrame()\n",
    "    \n",
    "    # Calculate log return\n",
    "    df[prefix + 'log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Calculate various Ta-Lib features\n",
    "    df_technical_features[prefix + 'BOP'] = talib.BOP(df['open'], df['high'], df['low'], df['close'])\n",
    "    df_technical_features[prefix + 'NATR'] = talib.NATR(df['high'], df['low'], df['close'], 4)\n",
    "    df_technical_features[prefix + 'WMA'] = talib.WMA(df['close'], 4)\n",
    "    df_technical_features[prefix + 'DEMA'] = talib.DEMA(df['close'], 4)\n",
    "    df_technical_features[prefix + 'TEMA'] = talib.TEMA(df['close'], 4)\n",
    "    df_technical_features[prefix + 'PROC'] = talib.ROC(df['close'], 4) / df['close'].shift(4)\n",
    "    df_technical_features[prefix + 'ADX'] = talib.ADX(df['high'], df['low'], df['close'], 4)\n",
    "    df_technical_features[prefix + 'ADX2'] = talib.ADX(df['high'], df['low'], df['close'], 6)\n",
    "    df_technical_features[prefix + 'ADX3'] = talib.ADX(df['high'], df['low'], df['close'], 8)\n",
    "    df_technical_features[prefix + 'ATR'] = talib.ATR(df['high'], df['low'], df['close'], 4)\n",
    "    df_technical_features[prefix + 'NATR'] = talib.NATR(df['high'], df['low'], df['close'], 4)\n",
    "    df_technical_features[prefix + 'BETA1'] = talib.BETA(df['high'], df['low'], 4)\n",
    "    df_technical_features[prefix + 'BETA2'] = talib.BETA(df['high'], df['low'], 6)\n",
    "    df_technical_features[prefix + 'BETA3'] = talib.BETA(df['high'], df['low'], 8)\n",
    "    df_technical_features[prefix + 'MOM'] = talib.MOM(df['close'], 4)\n",
    "    df_technical_features[prefix + 'KAMA'] = talib.KAMA(df['close'], 4)\n",
    "    df_technical_features[prefix + 'SAR'] = talib.SAR(df['high'], df['low'])\n",
    "    df_technical_features[prefix + 'ROC'] = talib.ROC(df['close'], 4)\n",
    "    df_technical_features[prefix + 'RSI1'] = talib.RSI(df['close'], 4)\n",
    "    df_technical_features[prefix + 'RSI2'] = talib.RSI(df['close'], 8)\n",
    "    df_technical_features[prefix + 'CCI'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    \n",
    "    # Pattern Recognition\n",
    "    df_technical_features[prefix + 'CDLSHOOTINGSTAR'] = talib.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close']) # Shooting Star\n",
    "    df_technical_features[prefix + 'CDLRISEFALL3METHODS'] = talib.CDLRISEFALL3METHODS(df['open'], df['high'], df['low'], df['close']) # Rising/Falling Three Methods\n",
    "    df_technical_features[prefix + 'CDLINVERTEDHAMMER'] = talib.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close']) # Inverted Hammer\n",
    "    df_technical_features[prefix + 'CDLHARAMI'] = talib.CDLHARAMI(df['open'], df['high'], df['low'], df['close']) # Harami Patterns\n",
    "    df_technical_features[prefix + 'CDLHAMMER'] = talib.CDLHAMMER(df['open'], df['high'], df['low'], df['close']) # Hammer\n",
    "    df_technical_features[prefix + 'CDLGRAVESTONEDOJI'] = talib.CDLGRAVESTONEDOJI(df['open'], df['high'], df['low'], df['close']) # Gravestone Doji\n",
    "    df_technical_features[prefix + 'CDLDOJI'] = talib.CDLDOJI(df['open'], df['high'], df['low'], df['close']) # Doji\n",
    "    df_technical_features[prefix + 'CDL3WHITESOLDIERS'] = talib.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close']) # Three White Soldiers\n",
    "\n",
    "    df_technical_features[prefix + 'min_ret'] = df['close'].pct_change().rolling(window=8).min()\n",
    "    df_technical_features[prefix + 'max_ret'] = df['close'].pct_change().rolling(window=8).max()\n",
    "\n",
    "    # Autocorrelation\n",
    "    autocorrelation_window = 20\n",
    "    df_technical_features[prefix + 'autocorr_1'] = df['close'].rolling(window=autocorrelation_window, min_periods=4, center=False).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "    df_technical_features[prefix + 'autocorr_2'] = df['close'].rolling(window=autocorrelation_window, min_periods=4, center=False).apply(lambda x: x.autocorr(lag=2), raw=False)\n",
    "\n",
    "    # Additional Features\n",
    "    df_technical_features[prefix + 'EMA'] = talib.EMA(df['close'], 4) # Exponential Moving Average, period=4\n",
    "    df_technical_features[prefix + 'WMA'] = talib.WMA(df['close'], 4) # Weighted Moving Average, period=4\n",
    "    df_technical_features[prefix + 'DEMA'] = talib.DEMA(df['close'], 4) # Double Exponential Moving Average, period=4\n",
    "    df_technical_features[prefix + 'TEMA'] = talib.TEMA(df['close'], 4) # Triple Exponential Moving Average, period=4\n",
    "    df_technical_features[prefix + 'PROC'] = talib.ROC(df['close'], 4) / df['close'].shift(4) # Price Rate of Change\n",
    "    df_technical_features[prefix + 'stoch_slowk'], df_technical_features[prefix + 'stoch_slowd'] = talib.STOCH(df['high'], df['low'], df['close'], fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0) # Stochastic\n",
    "\n",
    "    df_technical_features[prefix + 'AROON_UP'], df_technical_features[prefix + 'AROON_DOWN'] = talib.AROON(df['high'], df['low'], timeperiod=14) #Aroon\n",
    "    df_technical_features[prefix + 'CORREL'] = talib.CORREL(df['high'], df['low'], 4) # Pearson's Correlation Coefficient (r)\n",
    "    df_technical_features[prefix + 'HT_DCPERIOD'] = talib.HT_DCPERIOD(df['close']) # Hilbert Transform - Dominant Cycle Period\n",
    "    df_technical_features[prefix + 'HT_DCPHASE'] = talib.HT_DCPHASE(df['close']) # Hilbert Transform - Dominant Cycle Phase\n",
    "    df_technical_features[prefix + 'HT_TRENDMODE'] = talib.HT_TRENDMODE(df['close']) # Hilbert Transform - Trend vs Cycle Mode\n",
    "\n",
    "    # Weighted Moving Average Mean\n",
    "    df_technical_features[prefix + 'hma_mean1'] = HMA(df[prefix + 'log_return'], 2)\n",
    "    df_technical_features[prefix + 'hma_mean2'] = HMA(df[prefix + 'log_return'], 4)\n",
    "    df_technical_features[prefix + 'hma_mean3'] = HMA(df[prefix + 'log_return'], 8)\n",
    "\n",
    "    # Garch Volatility for close and log return\n",
    "    df_technical_features[prefix + 'garch_vol_close'] = fit_garch_and_get_volatility(df['close'].dropna())\n",
    "\n",
    "    # Z-score\n",
    "    df_technical_features[prefix + 'z_score'] = z_score(df['close'], 8) # window=8\n",
    "\n",
    "    # Bollinger Bands\n",
    "    upper, middle, lower = talib.BBANDS(df['close'], 4)\n",
    "    df_technical_features[prefix + 'bb_upper'] = upper\n",
    "    df_technical_features[prefix + 'bb_middle'] = middle\n",
    "    df_technical_features[prefix + 'bb_lower'] = lower\n",
    "\n",
    "    # Hurst Exponent\n",
    "    df_technical_features[prefix + 'hurst'] = df['close'].rolling(8).apply(rolling_hurst, raw=False)\n",
    "\n",
    "    # Detrended Price Oscillator\n",
    "    df_technical_features[prefix + 'dpo'] = calculate_dpo(df['close'], 8, 2) # period=8, min_period=2\n",
    "\n",
    "    # TSF\n",
    "    df_technical_features[prefix + 'TSF'] = talib.TSF(df[prefix + 'log_return'], 2)\n",
    "    df_technical_features[prefix + 'TSF4'] = talib.TSF(df[prefix + 'log_return'], 4)\n",
    "    df_technical_features[prefix + 'TSF8'] = talib.TSF(df[prefix + 'log_return'], 8)\n",
    "\n",
    "    # STDDEV\n",
    "    df_technical_features[prefix +'STDDEV1'] = talib.STDDEV(df[prefix + 'log_return'], 1)\n",
    "    df_technical_features[prefix +'STDDEV2'] = talib.STDDEV(df[prefix + 'log_return'], 2)\n",
    "    df_technical_features[prefix +'STDDEV3'] = talib.STDDEV(df[prefix + 'log_return'], 3)\n",
    "    df_technical_features[prefix +'STDDEV4'] = talib.STDDEV(df[prefix + 'log_return'], 4)\n",
    "    df_technical_features[prefix +'STDDEV6'] = talib.STDDEV(df[prefix +'log_return'], 6)\n",
    "    df_technical_features[prefix +'STDDEV8'] = talib.STDDEV(df[prefix +'log_return'], 8)\n",
    "\n",
    "    df[prefix + 'lag_target'] = df.close.pct_change().shift(1)\n",
    "    df[prefix + 'std'] = df[prefix + 'lag_target'].rolling(3).std()\n",
    "\n",
    "    '''\n",
    "    # done on Windows PC as Conda had issues with the dependency\n",
    "    # Fractional Differentiation\n",
    "    df = fractional_differentiation(df)\n",
    "    '''\n",
    "\n",
    "    data = pd.concat([df, df_technical_features], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fred_data(start_date, end_date, prefix = 'feature_'):\n",
    "    \"\"\"\n",
    "    Retrieves different data from the FRED API.\n",
    "\n",
    "    Args:\n",
    "        start_date (string): The start date of the data in the format 'YYYY-MM-DD'.\n",
    "        end_date (string): The end date of the data in the format 'YYYY-MM-DD'.\n",
    "        prefix (string, optional): The prefix to add to the column names. Defaults to 'feature_'.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The retrieved data with column names prefixed with 'feature_'.\n",
    "    \"\"\"\n",
    "    # Retrieve basic economic indicators\n",
    "    gdp = request_fred_api('GDP', start_date, end_date)\n",
    "    unemployment = request_fred_api('UNRATE', start_date, end_date)\n",
    "    inflation = request_fred_api('T10YIE', start_date, end_date)\n",
    "    m2 = request_fred_api('WM2NS', start_date, end_date)\n",
    "\n",
    "    # Retrieve crypto data\n",
    "    crypto_market_cap = request_fred_api('CBBTCUSD', start_date, end_date)\n",
    "\n",
    "    # Combine data into a single DataFrame\n",
    "    data = pd.concat([gdp.iloc[:,0],\n",
    "                     unemployment.iloc[:,0],\n",
    "                     inflation.iloc[:,0],\n",
    "                     m2.iloc[:,0],\n",
    "                     crypto_market_cap.iloc[:,0]], axis=1)\n",
    "\n",
    "    # Set column names\n",
    "    data.columns = ['GDP', 'Unemployment', 'Inflation', 'M2', 'CryptoMarketCap']\n",
    "    data.columns = [prefix + col for col in data.columns]  # Add 'feature_' prefix\n",
    "\n",
    "    # Reindex the DataFrame to include all dates in the specified range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    data = data.reindex(date_range)\n",
    "\n",
    "    # Fill missing values using forward fill\n",
    "    data = data.fillna(method='ffill')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yfinance_data(start_date, end_date, prefix = 'feature_'):\n",
    "    \"\"\"\n",
    "    Retrieves financial data from Yahoo Finance API and preprocesses it.\n",
    "    \n",
    "    Args:\n",
    "      start_date (string): Start date of data retrieval (format: 'YYYY-MM-DD')\n",
    "      end_date (string): End date of data retrieval (format: 'YYYY-MM-DD')\n",
    "      prefix (string): Prefix to add to column names (default: 'feature_')\n",
    "    \n",
    "    Returns:\n",
    "      Dataframe: Preprocessed financial data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Commoditites\n",
    "    gold = request_yfinance_data('GC=F', start_date, end_date)\n",
    "    silver = request_yfinance_data('SI=F', start_date, end_date)\n",
    "    crude_oil = request_yfinance_data('CL=F', start_date, end_date)\n",
    "\n",
    "    # Volatility for Commoditites\n",
    "    gold_returns= np.log(gold['Close'] / gold['Close'].shift(1))\n",
    "    gold_vol = gold_returns.rolling(window=20).std() * np.sqrt(20) \n",
    "    silver_returns= np.log(silver['Close'] / silver['Close'].shift(1))\n",
    "    silver_vol = silver_returns.rolling(window=20).std() * np.sqrt(20) \n",
    "    crude_oil_returns = np.log(crude_oil['Close'] / silver['Close'].shift(1))\n",
    "    crude_oil_vol = crude_oil_returns.rolling(window=20).std() * np.sqrt(20) \n",
    "\n",
    "    # Equity market indices\n",
    "    sp500 = request_yfinance_data('^GSPC', start_date, end_date)\n",
    "    nasdaq = request_yfinance_data('^IXIC', start_date, end_date)\n",
    "    dowjones = request_yfinance_data('^DJI', start_date, end_date)\n",
    "\n",
    "    # Volatility for equity market indices\n",
    "    sp500_returns = np.log(sp500['Close'] / sp500['Close'].shift(1))\n",
    "    sp500_vol = sp500_returns.rolling(window=20).std() * np.sqrt(20)\n",
    "    nasdaq_returns = np.log(nasdaq['Close'] / nasdaq['Close'].shift(1))\n",
    "    nasdaq_vol = nasdaq_returns.rolling(window=20).std() * np.sqrt(20)\n",
    "    dowjones_returns = np.log(dowjones['Close'] / dowjones['Close'].shift(1))\n",
    "    dowjones_vol = dowjones_returns.rolling(window=20).std() * np.sqrt(20) \n",
    "\n",
    "    # Crypto\n",
    "    bitcoin = request_yfinance_data('BTC-USD', start_date, end_date)\n",
    "\n",
    "    # U.S. Dollar Index\n",
    "    usd = request_yfinance_data('DX-Y.NYB', start_date, end_date)\n",
    "\n",
    "    # Treasury yields on U.S. government-issued securities\n",
    "    treasury_yield1 = request_yfinance_data('^TNX', start_date, end_date)\n",
    "    treasury_yield2 = request_yfinance_data('^IRX', start_date, end_date)\n",
    "    treasury_yield3 = request_yfinance_data('^FVX', start_date, end_date)\n",
    "    treasury_yield4 = request_yfinance_data('^TYX', start_date, end_date)\n",
    "\n",
    "    # Calculate yield_curve_spread\n",
    "    yield_curve_spread = treasury_yield4['Close'] - treasury_yield2['Close']\n",
    "\n",
    "    # Calculate Moving Average for \"main\" treasury_yield\n",
    "    treasury_yield1_ma30 = treasury_yield1['Close'].rolling(window=30).mean()\n",
    "\n",
    "    # Volatility treasury_yield\n",
    "    treasury_yield1_vol = treasury_yield1['Close'].rolling(window=20).std() * np.sqrt(20) \n",
    "\n",
    "    # New highs, new lows\n",
    "    sp500_new_highs = (sp500['Close'] > sp500['Close'].rolling(window=52).max().shift(1)).astype(int)\n",
    "    sp500_new_lows = (sp500['Close'] < sp500['Close'].rolling(window=52).min().shift(1)).astype(int)\n",
    "\n",
    "    # Concatenate data\n",
    "    data = pd.concat([sp500['Close'],\n",
    "                    sp500_vol,\n",
    "                    nasdaq['Close'],\n",
    "                    nasdaq_vol,\n",
    "                    dowjones['Close'],\n",
    "                    dowjones_vol,\n",
    "                    gold['Close'],\n",
    "                    gold_vol,\n",
    "                    silver['Close'],\n",
    "                    silver_vol,\n",
    "                    crude_oil['Close'],\n",
    "                    crude_oil_vol,\n",
    "                    bitcoin['Close'],\n",
    "                    usd['Close'],\n",
    "                    treasury_yield1['Close'],\n",
    "                    treasury_yield2['Close'],\n",
    "                    treasury_yield3['Close'],\n",
    "                    treasury_yield4['Close'],\n",
    "                    yield_curve_spread,\n",
    "                    treasury_yield1_ma30,\n",
    "                    treasury_yield1_vol,\n",
    "                    sp500_new_highs,\n",
    "                    sp500_new_lows], axis=1)\n",
    "    \n",
    "    # Set column names\n",
    "    data.columns = [\n",
    "      'SP500',\n",
    "      'SP500_Vol',\n",
    "      'NASDAQ',\n",
    "      'NASDAQ_Vol',\n",
    "      'DowJones',\n",
    "      'DowJones_Vol',\n",
    "      'Gold',\n",
    "      'Gold_Daily_Vol',\n",
    "      'Silver',\n",
    "      'Silver_Daily_Vol',\n",
    "      'CrudeOil',\n",
    "      'CrudeOil_Vol',\n",
    "      'BTC',\n",
    "      'USDIndex',\n",
    "      'Treasury_Yield1',\n",
    "      'Treasury_Yield2',\n",
    "      'Treasury_Yield3',\n",
    "      'Treasury_Yield4',\n",
    "      'yield_curve_spread',\n",
    "      'treasury_yield1_ma30',\n",
    "      'treasury_yield1_vol',\n",
    "      'SP500_new_highs',\n",
    "      'SP500_new_lows'\n",
    "    ]\n",
    "\n",
    "    data.columns = [prefix + col for col in data.columns]  # Add 'feature_' prefix\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    data = data.reindex(date_range)\n",
    "\n",
    "    data = data.fillna(method='ffill')  # Forward fill missing data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_differentiation(data: pd.DataFrame, close: str = 'close', prefix: str = 'feature_', verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply fractional differentiation to the closing prices of the given data. Use the minimum order of fractional differentiation that makes the time series stationary.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): The input data.\n",
    "        close (string, optional): The name of the 'close' column in the data. Defaults to 'close'.\n",
    "        prefix (string, optional): The prefix to add to the new column name. Defaults to 'feature_'.\n",
    "        verbose (boolean, optional): Whether to print verbose information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The modified data with the new column added.\n",
    "    \"\"\"\n",
    "    X = data[close].values.reshape(-1, 1)\n",
    "\n",
    "    # Perform fractional differentiation\n",
    "    fs = FracdiffStat()\n",
    "    try:\n",
    "        Xdiff = fs.fit_transform(X)\n",
    "        _, pvalue, _, _, _, _ = stattools.adfuller(Xdiff.reshape(-1))\n",
    "        corr = np.corrcoef(X[-Xdiff.size :, 0], Xdiff.reshape(-1))[0][1]\n",
    "        data[prefix + 'frac_diff_close'] = fdiff(data[close], fs.d_[0])\n",
    "    except Exception as e:\n",
    "        # Set the new column to NaN if an error occurs\n",
    "        data[prefix + 'frac_diff_close'] = np.nan\n",
    "\n",
    "    if verbose:\n",
    "        # Print verbose information\n",
    "        print(\"* Order: {:.2f}\".format(fs.d_[0]))\n",
    "        print(\"* ADF p-value: {:.2f} %\".format(100 * pvalue))\n",
    "        print(\"* Correlation with the original time-series: {:.2f}\".format(corr))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy (without vertical barrier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bb(df, window_size = 20, num_std = 2):\n",
    "    \"\"\"\n",
    "    Calculate the Bollinger Bands for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): DataFrame containing the closing prices.\n",
    "        window_size (integer, optional): Size of the rolling window for calculating the mean and standard deviation. Default set to 20.\n",
    "        num_std (integer, optional): Number of standard deviations to use for calculating the upper and lower bands. Default set to 2.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: DataFrame with the Bollinger Bands added as new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the rolling mean and standard deviation of the closing prices\n",
    "    rolling_mean = df['close'].rolling(window=window_size).mean()\n",
    "    rolling_std = df['close'].rolling(window=window_size).std()\n",
    "\n",
    "    # Calculate the upper and lower Bollinger Bands\n",
    "    upper_band = rolling_mean + (rolling_std * num_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std)\n",
    "\n",
    "    # Add the calculated values to the DataFrame as new columns\n",
    "    df['sma_20'] = rolling_mean\n",
    "    df['upper_bb'] = upper_band\n",
    "    df['lower_bb'] = lower_band\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trade_signals_bb_tbm(df):\n",
    "    \"\"\"\n",
    "    Calculate trade signals based on Bollinger Bands and Moving Average. Method is used from TBM-Strategy.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): The input dataframe containing 'close', 'upper_bb', 'lower_bb', and 'sma_20' columns.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe: The modified dataframe with added 'side' column indicating long and short signals.\n",
    "    \"\"\"\n",
    "    # Check if the columns are already in df, if not calculate them\n",
    "    if ('upper_bb' not in df) or ('lower_bb' not in df) or ('sma_20' not in df):\n",
    "        df = calculate_bb(df)\n",
    "\n",
    "    df['side'] = np.nan\n",
    "    long_signals = (df['close'] <= df['lower_bb'])\n",
    "    short_signals = (df['close'] >= df['upper_bb'])\n",
    "\n",
    "    df.loc[long_signals, 'side'] = 1 # long signals = 1\n",
    "    df.loc[short_signals, 'side'] = -1 # short signals = -1\n",
    "\n",
    "    # Remove Look ahead bias by lagging the signal by a day\n",
    "    df['side'] = df['side'].shift(1)\n",
    "\n",
    "    # Drop NaN values so we only have entries with -1 and 1 (long and short signals)\n",
    "    df.dropna(axis=0, subset=['side'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bollinger_bands(data, fee = 0.006):\n",
    "    \"\"\"\n",
    "    Apply Bollinger Bands trading strategy to the given data.\n",
    "\n",
    "    Parameters:\n",
    "        data (dataframe): The input data containing OHLC (Open-High-Low-Close) values.\n",
    "        fee (float, optional): The transaction fee as a percentage. Default set to 0.06%\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The output data containing the trading signals, profits, and other information.\n",
    "    \"\"\"\n",
    "\n",
    "    output_list = []\n",
    "\n",
    "    # Calculate trade signals using Bollinger Bands\n",
    "    data = calculate_trade_signals_bb_tbm(data)\n",
    "\n",
    "    # Initialize variables for buying and selling values\n",
    "    trade_item = pd.DataFrame()\n",
    "    sell_p = np.nan\n",
    "    sell_date = np.nan\n",
    "\n",
    "    for index, item in data.iterrows():\n",
    "        # Skip if there is a sell signal before a buy signal\n",
    "        if item['side'] == -1 and trade_item.empty:\n",
    "            continue\n",
    "        # If there is a buy signal and no existing trade, start a new trade\n",
    "        elif item['side'] == 1 and trade_item.empty:\n",
    "            trade_item = item.to_frame().transpose()\n",
    "        # If there is a buy signal but there is already a previous buy signal before a sell signal came, skip\n",
    "        elif item['side'] == 1 and not trade_item.empty:\n",
    "            continue\n",
    "        # Sell the item again\n",
    "        elif item['side'] == -1 and np.isnan(sell_p):\n",
    "            sell_p = item['close']\n",
    "            sell_date =  pd.to_datetime(index.strftime('%Y-%m-%d'))\n",
    "            profit_rel = (sell_p - trade_item['close'].iloc[0]) / trade_item['close'].iloc[0]\n",
    "            \n",
    "            trade_item['profit_rel'] = profit_rel\n",
    "            \n",
    "            # Add fee to the profits if fee is greater than 0\n",
    "            if fee > 0:\n",
    "                profit_rel = trade_item['profit_rel'] - fee\n",
    "                trade_item['profit_rel'] = profit_rel\n",
    "            \n",
    "            trade_item['bin'] = 1 if profit_rel > 0 else 0\n",
    "            trade_item['t1'] = sell_date\n",
    "            trade_item['t1_price'] = sell_p\n",
    "\n",
    "            output_list.append(trade_item)\n",
    "\n",
    "            # Reset variables for the next trade\n",
    "            trade_item = pd.DataFrame()\n",
    "            sell_p = np.nan\n",
    "            sell_date = np.nan\n",
    "\n",
    "    try:\n",
    "        output = pd.concat(output_list, axis=0)\n",
    "    except Exception as e:\n",
    "        output = pd.DataFrame()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy (with vertical barrier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertical_barrier_item(trading_daily, trading_date, vertical_barrier_days, max_search_days = 7):\n",
    "    \"\"\"\n",
    "    Get the first row in the trading_daily DataFrame with a date that falls within the vertical barrier.\n",
    "    \n",
    "    Parameters:\n",
    "        trading_daily (dataframe): The DataFrame containing all OHLC data for an asset\n",
    "        trading_date (timestamp): The initial trading date.\n",
    "        vertical_barrier_days (integer): The number of days the barrier extends from the trading date.\n",
    "        max_search_days (integer, optional): The maximum number of days to search for a matching row. Defaults to 7.\n",
    "    \n",
    "    Returns:\n",
    "        Series: The matching row with the closest date to the vertical barrier (either on the day or the next day after the vertical barrier)\n",
    "    \"\"\"\n",
    "    exit_date = trading_date + timedelta(days=vertical_barrier_days)\n",
    "\n",
    "    # Search for the next available data point (necessary due to non-trading days)\n",
    "    for i in range(max_search_days + 1):\n",
    "        current_date = exit_date + timedelta(days=i)\n",
    "        matching_rows = trading_daily[trading_daily['date'] == current_date]\n",
    "        if not matching_rows.empty: # If a matching row is found\n",
    "            break\n",
    "        if i == max_search_days: # If no matching row is found take the last row available\n",
    "            matching_rows = trading_daily.nlargest(1, 'date')\n",
    "    \n",
    "    matching_rows = matching_rows.set_index('date')\n",
    "\n",
    "    return matching_rows.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_vertical_barrier_interferes(buying_item, selling_item, vertical_barrier_days):\n",
    "    \"\"\"\n",
    "    Check if a vertical barrier interferes between a buying item and a selling item.\n",
    "\n",
    "    Parameters:\n",
    "        buying_item (series): The buying item.\n",
    "        selling_item (series): The selling item.\n",
    "        vertical_barrier_days (integer): The number of days for the vertical barrier.\n",
    "\n",
    "    Returns:\n",
    "        Boolean: True if the vertical barrier interferes, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the buying date from the buying item\n",
    "    buying_date = buying_item.index[0].strftime('%Y-%m-%d')\n",
    "    buying_date = pd.to_datetime(buying_date)\n",
    "\n",
    "    # Get the possible selling date from the selling item\n",
    "    possible_selling_date = selling_item.name\n",
    "\n",
    "    # Check if the buying date plus the vertical barrier is before the selling item date\n",
    "    if buying_date + timedelta(days=vertical_barrier_days) < possible_selling_date:\n",
    "        return True\n",
    "    else: # selling item before vertical barrier\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selling_data(trade_item, selling_item, fee):\n",
    "    \"\"\"\n",
    "    Adds selling data to the trade (trade_item).\n",
    "\n",
    "    Parameters:\n",
    "        trade_item (dataframe): DataFrame containing trade data.\n",
    "        selling_item (series): Series containing selling data.\n",
    "        fee (float): Fee as a percentage.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Updated trade_item DataFrame.\n",
    "    \"\"\"\n",
    "    # Get selling price and date\n",
    "    sell_p = selling_item['close']\n",
    "    sell_date = pd.to_datetime(selling_item.name)\n",
    "\n",
    "    # Calculate relative profit\n",
    "    profit_rel = (sell_p - trade_item['close'].iloc[0]) / trade_item['close'].iloc[0]\n",
    "\n",
    "    # Add relative profit to trade_item DataFrame\n",
    "    trade_item['profit_rel'] = profit_rel\n",
    "\n",
    "    # Subtract fee from relative profit if fee is greater than 0\n",
    "    if fee > 0:\n",
    "        profit_rel = trade_item['profit_rel'].iloc[0] - fee\n",
    "        trade_item['profit_rel'] = profit_rel\n",
    "\n",
    "    # Set Meta-Labels (1: profitable, 0: unprofitable)\n",
    "    trade_item['bin'] = 1 if profit_rel > 0 else 0\n",
    "\n",
    "    # Set t1 and t1_price to sell_date and sell_p respectively\n",
    "    trade_item['t1'] = sell_date\n",
    "    trade_item['t1_price'] = sell_p\n",
    "\n",
    "    return trade_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bollinger_bands_v2(trading_daily, data, vertical_barrier_days = 93, fee = 0.006):\n",
    "    \"\"\"\n",
    "    Apply Bollinger Bands trading strategy to the input data (with vertical barrier).\n",
    "\n",
    "    Parameters:\n",
    "        trading_daily (dataframe): DataFrame containing daily trading data.\n",
    "        data (dataframe): DataFrame containing the input data.\n",
    "        vertical_barrier_days (integer, optional): Number of days for the vertical barrier. Defaults to double the average duration for profitable trades which is 93.\n",
    "        fee (float, optional): Fee as a percentage. Defaults to 0.06% for a trade with two transactions.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: DataFrame containing the trading results.\n",
    "    \"\"\"\n",
    "\n",
    "    output_list = []\n",
    "\n",
    "    data = calculate_trade_signals_bb_tbm(data)  # Calculate the trading signals\n",
    "\n",
    "    trade_item = pd.DataFrame()  # DataFrame to store the current trade item\n",
    "\n",
    "    for index, item in data.iterrows():\n",
    "        if item['side'] == -1 and trade_item.empty:  # Sell signal before buy signal, skip it\n",
    "            continue\n",
    "        if item['side'] == 1 and trade_item.empty:  # Trade initiation as trade item is still empty\n",
    "            trade_item = item.to_frame().transpose()\n",
    "        elif item['side'] == 1 and not trade_item.empty:  # Buy signal but already have a previous buy signal\n",
    "            if check_if_vertical_barrier_interferes(trade_item, item, vertical_barrier_days):\n",
    "                # vertical barrier hit, close previous trade and set the current buy signal as new trade\n",
    "                matching_row = get_vertical_barrier_item(trading_daily, trade_item.index[0], vertical_barrier_days)\n",
    "                trade_item = add_selling_data(trade_item, matching_row, fee)\n",
    "                output_list.append(trade_item)\n",
    "\n",
    "                trade_item = pd.DataFrame()\n",
    "                trade_item = item.to_frame().transpose()  # Set the second buy signal as the new trade\n",
    "            else: # buy signal but vertical barrier is not hit, thus second buy signal is ignored\n",
    "                continue\n",
    "        elif item['side'] == -1:\n",
    "            if check_if_vertical_barrier_interferes(trade_item, item, vertical_barrier_days):\n",
    "                # vertical barrier hit before sell signal, close previous trade with vertical barrier\n",
    "                matching_row = get_vertical_barrier_item(trading_daily, trade_item.index[0], vertical_barrier_days)\n",
    "                trade_item = add_selling_data(trade_item, matching_row, fee)\n",
    "                output_list.append(trade_item)\n",
    "\n",
    "                # Reset variables\n",
    "                trade_item = pd.DataFrame()\n",
    "            else:  # Sell signal occurred before the vertical barrier\n",
    "                trade_item = add_selling_data(trade_item, item, fee)\n",
    "                output_list.append(trade_item)\n",
    "\n",
    "                # Reset variables, prepare for new trade\n",
    "                trade_item = pd.DataFrame()\n",
    "\n",
    "    # Check if a trade is open but no data is available, then use last data point available\n",
    "    if len(trade_item) > 0:\n",
    "        matching_row = get_vertical_barrier_item(trading_daily, trade_item.index[0], vertical_barrier_days)\n",
    "        trade_item = add_selling_data(trade_item, matching_row, fee)\n",
    "        output_list.append(trade_item)\n",
    "\n",
    "    try:\n",
    "        output = pd.concat(output_list, axis=0)\n",
    "    except Exception as e:\n",
    "        output = pd.DataFrame()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the average time of profitable trades\n",
    "df = load_ml_data(path_data_combined, 'v2_bb_and_external_features.csv', vertical_barrier=False)\n",
    "\n",
    "df_profitable = df[df['bin'] == 1]\n",
    "\n",
    "df_profitable['duration'] = df_profitable['t1'] - df_profitable['date']\n",
    "\n",
    "# Calculate the average duration\n",
    "average_duration = df_profitable['duration'].mean()\n",
    "\n",
    "# Print the average duration\n",
    "print(f\"Average Duration: {average_duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_daily_volatility(data, timespan = 25):\n",
    "    \"\"\"\n",
    "    Adds a column 'daily_vol' to the data DataFrame, which represents the daily volatility.\n",
    "\n",
    "    Parameters:\n",
    "        data (dataframe): The DataFrame containing the asset data.\n",
    "        timespan (integer, optional): The number of periods to consider for calculating volatility. Default is 25.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The updated DataFrame with the 'daily_vol' column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # only add daily volatility if it has not been added before\n",
    "    if 'daily_vol' not in data:\n",
    "        # calculate daily volatility\n",
    "        df_daily_vol = data['close'].pct_change()  # percentage returns\n",
    "        df_daily_vol = df_daily_vol.ewm(span=timespan).std()  # exponential weighted moving average\n",
    "\n",
    "        # add column\n",
    "        data['daily_vol'] = df_daily_vol\n",
    "\n",
    "        # drop nan values where daily-volatility could not be calculated\n",
    "        data.dropna(subset=['daily_vol'], inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_CUSUM_events(data, threshold):\n",
    "    \"\"\"\n",
    "    Finds CUSUM events in the given data based on a threshold.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): The input data containing 'close' prices.\n",
    "        threshold (float): The threshold value for detecting CUSUM events.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Subset of the input data where CUSUM events occur.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    events = []  # CUSUM events\n",
    "    pos_cum_sum = 0\n",
    "    neg_cum_sum = 0\n",
    "\n",
    "    # Calculate log returns\n",
    "    diff = np.log(data['close']).diff().dropna()\n",
    "\n",
    "    # Iterate over the log returns\n",
    "    for i in diff.index[1:]:\n",
    "        positive = float(pos_cum_sum + diff.loc[i])\n",
    "        negative = float(neg_cum_sum + diff.loc[i])\n",
    "        pos_cum_sum = max(0.0, positive)\n",
    "        neg_cum_sum = min(0.0, negative)\n",
    "\n",
    "        # check for CUSUM (negative direction)\n",
    "        if neg_cum_sum < -threshold:\n",
    "            neg_cum_sum = 0  # Reset\n",
    "            events.append(i)  # Add CUSUM event\n",
    "\n",
    "        # check for CUSUM (positive direction)\n",
    "        elif pos_cum_sum > threshold:\n",
    "            pos_cum_sum = 0  # Reset\n",
    "            events.append(i)  # Add CUSUM event\n",
    "\n",
    "    # Convert events to timestamps\n",
    "    events_timestamps = pd.DatetimeIndex(events)\n",
    "\n",
    "    # Return subset of data where CUSUM events occured\n",
    "    return data.loc[events_timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vertical_barrier(data_events, data , num_days = 10):\n",
    "    \"\"\"\n",
    "    Add a vertical barrier to the data_events DataFrame based on the num_days parameter.\n",
    "\n",
    "    Parameters:\n",
    "        data_events (dataframe): DataFrame containing event timestamps.\n",
    "        data (dataframe): DataFrame containing the original data.\n",
    "        num_days (integer): Number of days to add as a vertical barrier.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Updated data_events DataFrame with vertical barrier added.\n",
    "    \"\"\"\n",
    "    # Only add vertical barrier if it is not already in our DataFrame\n",
    "    if 'vertical_barrier' not in data_events:\n",
    "        events = data_events.index  # Event timestamps\n",
    "        close = data['close']  # Close prices of all entries in our original data\n",
    "\n",
    "        # Find next entry in close prices after the event + the num_days\n",
    "        vb = close.index.searchsorted(events + pd.Timedelta(days=num_days))\n",
    "        vb = vb[vb < close.shape[0]]  # Check for out of bounds\n",
    "        vb = pd.Series(close.index[vb], index=events[:vb.shape[0]])  # NaNs at end\n",
    "\n",
    "        data_events['vertical_barrier'] = vb # Set vertical barrier to dataframe\n",
    "\n",
    "        # Remove rows with NaNs in the vertical_barrier column\n",
    "        data_events = data_events.dropna(subset=['vertical_barrier'])\n",
    "    \n",
    "    return data_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_horizontal_barriers(data, data_events, profit_taking, stop_loss):\n",
    "    \"\"\"\n",
    "    Adds horizontal barriers to the data_events DataFrame based on profit-taking and stop-loss limits.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): The main data DataFrame.\n",
    "        data_events (dataframe): The DataFrame containing the events.\n",
    "        profit_taking (float): The profit-taking limit.\n",
    "        stop_loss (float): The stop-loss limit.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The result DataFrame with horizontal barriers added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the vertical_barrier column\n",
    "    out = data_events[['vertical_barrier']].copy(deep=True)\n",
    "\n",
    "    # Set profit-taking and stop-loss limits\n",
    "    pt = profit_taking * data_events['daily_vol'] if profit_taking > 0 else pd.Series(index=data_events.index)\n",
    "    sl = -stop_loss * data_events['daily_vol'] if stop_loss > 0 else pd.Series(index=data_events.index)\n",
    "  \n",
    "    # Set the vertical barrier to the stop loss or profit taking limit\n",
    "    for index, vb in data_events['vertical_barrier'].fillna(data['close'].index[-1]).items():\n",
    "        df0 = data['close'][index:vb]  # Path prices from start of event to vertical barrier\n",
    "        df0 = (df0 / data['close'][index] - 1) * data_events.at[index, 'side']  # Get path returns\n",
    "\n",
    "        # Add stop loss and profit taking if possible to out\n",
    "        out.loc[index, 'sl'] = df0[df0 < sl[index]].index.min()  # Earliest stop loss\n",
    "        out.loc[index, 'pt'] = df0[df0 > pt[index]].index.min()  # Earliest profit taking\n",
    "  \n",
    "    # Get the minimum, which means the first that is reached (stop-loss, profit-taking, vertical)\n",
    "    data_events.loc[:, 't1'] = out.dropna(how='all').min(axis=1)  # t1 = touch no.1, min of pt and sl\n",
    "\n",
    "    # Get the prices at the exit t1\n",
    "    merged_data = pd.merge(data_events['t1'], data['close'], how='left', left_on='t1', right_index=True)\n",
    "    merged_data['t1_price'] = merged_data['close']\n",
    "    result_df = pd.concat([data_events, merged_data['t1_price']], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_labels(df, fee = 0.006):\n",
    "    \"\"\"\n",
    "    Set meta labels for a given dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (dataframe): The input dataframe containing the trades and the necessary columns (side, t1_price, close).\n",
    "    fee (float, optional): The fee to be subtracted from the profit_rel calculation. Default is 0.006 (0.06%).\n",
    "\n",
    "    Returns:\n",
    "    Dataframe: The input dataframe with additional columns for profit_rel, and bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate relative profit\n",
    "    df['profit_rel'] = df['side'] * ((df['t1_price'] - df['close']) / df['close'])\n",
    "    \n",
    "    # Subtract fee from relative profit\n",
    "    if fee > 0:\n",
    "        df['profit_rel'] = df['profit_rel'] - fee\n",
    "        df['profit_abs'] = df['close'] * df['profit_rel']\n",
    "        \n",
    "    # Create binary labels based on profit_abs\n",
    "    # 1: Profitable trade\n",
    "    # 0: Unprofitable trade\n",
    "    df['bin'] = df['profit_abs'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tbm(data, fee, vert_barrier_limit, pt, sl):\n",
    "    \"\"\"\n",
    "    Apply the TBM strategy to the data.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): The input data.\n",
    "        fee (float): The transaction fee for a trade that contains two transactions (buy and sell).\n",
    "        vert_barrier_limit (integer): The vertical barrier limit.\n",
    "        pt (float): The profit taking level.\n",
    "        sl (float): The stop loss level.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The modified data with TBM strategy applied.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original data\n",
    "    data_orig = data.copy(deep=True)\n",
    "\n",
    "    # Calculate daily volatility\n",
    "    data = add_daily_volatility(data)\n",
    "\n",
    "    # Calculate trade signals using Bollinger Bands\n",
    "    data = calculate_trade_signals_bb_tbm(data)\n",
    "\n",
    "    # Add CUSUM events\n",
    "    data = add_CUSUM_events(data, threshold=data['daily_vol'].mean() * 0.1)\n",
    "\n",
    "    # Add vertical barriers\n",
    "    data = add_vertical_barrier(data, data_orig, vert_barrier_limit)\n",
    "\n",
    "    # Add horizontal barriers\n",
    "    data = add_horizontal_barriers(data_orig, data, pt, sl)\n",
    "\n",
    "    # Set meta labels\n",
    "    data = get_meta_labels(data, fee)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_predictions(predictions, threshold):\n",
    "    \"\"\"\n",
    "    Labels predictions based on a threshold.\n",
    "\n",
    "    Args:\n",
    "        predictions (array): Array of predicted probabilities.\n",
    "        threshold (float): Threshold value for labeling.\n",
    "\n",
    "    Returns:\n",
    "        Array: Array of binary labels.\n",
    "    \"\"\"\n",
    "    # Convert predictions to numpy array if not already\n",
    "    if not isinstance(predictions, np.ndarray):\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "    # Extract positive probabilities from predictions\n",
    "    positive_probs = predictions[:, 1]\n",
    "\n",
    "    # Create a binary label array based on the threshold\n",
    "    predictions_abs = np.where(positive_probs > threshold, 1, 0)\n",
    "\n",
    "    return predictions_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_predictions_fraction(predictions, half_lower_thres, half_upper_thres):\n",
    "    \"\"\"\n",
    "    Labels predictions based on given thresholds for fractional trader.\n",
    "    \n",
    "    Args:\n",
    "        predictions (array): Array of predictions.\n",
    "        half_lower_thres (float): Lower threshold for labeling as 50% of the investment capital.\n",
    "        half_upper_thres (float): Upper threshold for labeling as 50% of the investment capital.\n",
    "        \n",
    "    Returns:\n",
    "        Array: Array of labeled predictions.\n",
    "    \"\"\"\n",
    "    # Convert predictions to numpy array if not already\n",
    "    if not isinstance(predictions, np.ndarray):\n",
    "        predictions = np.array(predictions)\n",
    "    \n",
    "    # Extract positive probabilities\n",
    "    positive_probs = predictions[:, 1]\n",
    "    \n",
    "    # Label predictions based on thresholds\n",
    "    # label as full investment where probability is higher than lower threshold\n",
    "    predictions_abs = np.where(positive_probs > half_lower_thres, 1, 0)\n",
    "    # label as 50% where probability is between lower and upper threshold\n",
    "    predictions_abs = np.where((positive_probs >= half_lower_thres) & \n",
    "                               (positive_probs <= half_upper_thres), \n",
    "                               0.5, predictions_abs)\n",
    "    \n",
    "    return predictions_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(array_1, array_2, array_3):\n",
    "    \"\"\"\n",
    "    Takes three arrays (predictions) as input and performs a majority vote for each element.\n",
    "    \n",
    "    Args:\n",
    "        array_1 (array): The first array.\n",
    "        array_2 (array): The second array.\n",
    "        array_3 (array): The third array.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The result of the majority vote, where each element is either 0 or 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the input arrays to numpy arrays\n",
    "    array1 = np.array(array_1)\n",
    "    array2 = np.array(array_2)\n",
    "    array3 = np.array(array_3)\n",
    "    \n",
    "    # Get the length of the arrays\n",
    "    length = array_1.size\n",
    "    \n",
    "    # Create an array of zeros with the same length as the input arrays\n",
    "    result = np.zeros(length, dtype=np.int64)\n",
    "    \n",
    "    # Perform majority vote for each element\n",
    "    for i in range(length):\n",
    "        sum_ones = array_1[i] + array_2[i] + array_3[i]\n",
    "        \n",
    "        if sum_ones > 1:\n",
    "            result[i] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_vote(array_1, array_2, array_3):\n",
    "    \"\"\"\n",
    "    Perform aggregation voting on three arrays (predictions).\n",
    "    \n",
    "    Args:\n",
    "        array_1 (array): First input array.\n",
    "        array_2 (array): Second input array.\n",
    "        array_3 (array): Third input array.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Resultant array after aggregation voting.\n",
    "    \"\"\"\n",
    "    # Convert arrays to numpy arrays\n",
    "    array_1 = np.array(array_1)\n",
    "    array_2 = np.array(array_2)\n",
    "    array_3 = np.array(array_3)\n",
    "    \n",
    "    # Get the length of the arrays\n",
    "    length = array_1.size\n",
    "    \n",
    "    # Initialize result array with zeros\n",
    "    result = np.zeros(length, dtype=np.int64)\n",
    "    \n",
    "    # Perform aggregation voting\n",
    "    for i in range(length):\n",
    "        sum_ones = array_1[i] + array_2[i] + array_3[i]\n",
    "        \n",
    "        # If sum of ones is equal to 3, set result to 1\n",
    "        if sum_ones == 3:\n",
    "            result[i] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_and_visualize_optimal_threshold(prediction, df_target, filename=None):\n",
    "    \"\"\"\n",
    "    Determine and visualize the optimal threshold based on precision.\n",
    "\n",
    "    Args:\n",
    "        prediction (array): The prediction probabilities.\n",
    "        df_target (dataframe): The target value (either 1 or 0, indicating profitable/unprofitable trade).\n",
    "        filename (string, optional): The filename to save the visualization. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing max_thres, max_val, and precisions.\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.5, 1.0, 0.01) # Thresholds to test\n",
    "\n",
    "    precisions = [] # For visualization store all precision scores\n",
    "    max_thres = -np.inf # Default value\n",
    "    max_val = -np.inf # Default value\n",
    "\n",
    "    for thres in tqdm(thresholds): # Check each threshold and get precision\n",
    "        prediction_w_thres = label_predictions(prediction, thres)\n",
    "\n",
    "        prec = precision_score(df_target, prediction_w_thres)\n",
    "    \n",
    "        # Check if current precision is highest\n",
    "        if prec > max_val:\n",
    "            max_thres = thres\n",
    "            max_val = prec\n",
    "        precisions.append(prec)\n",
    "\n",
    "    print('Max Value at Threshold:', max_thres)\n",
    "\n",
    "    plt.plot(thresholds, precisions)\n",
    "    plt.xlabel('Thresholds')\n",
    "    plt.ylabel('Precision Score')\n",
    "    plt.title('Threshold Comparison for Precision Score Optimization BB-Strategy')\n",
    "    plt.xticks(np.arange(0.5, 1.1, 0.1))  # Set x-axis ticks\n",
    "\n",
    "    # Set the x-axis and y-axis limits to start at 0.0\n",
    "    plt.xlim(0.5, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "\n",
    "    # save if filename is given\n",
    "    if filename is not None:\n",
    "        plt.savefig(path_default + '/visuals/' + filename, dpi='figure')\n",
    "    plt.show()\n",
    "    return max_thres, max_val, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overall_profit_for_vis(df, end_date = pd.Period('2023-05')):\n",
    "    \"\"\"\n",
    "    Calculate the overall profit for visualization purposes.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The input DataFrame containing the data.\n",
    "        end_date (time period, optional): The end date for the calculation. Defaults to 2023-05 (May 2023).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the total returns, months, and returns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the column names for grouping\n",
    "    group_by_t1 = 't1'\n",
    "    group_by_date = 'date'\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    df['t1'] = pd.to_datetime(df['t1'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Convert date columns to month periods\n",
    "    df['month_t1'] = df[group_by_t1].dt.to_period('M')\n",
    "    df['month_date'] = df[group_by_date].dt.to_period('M')\n",
    "\n",
    "    # Group the DataFrame by month_t1 and calculate the mean of profit_rel\n",
    "    grouped_df_t1 = df.groupby('month_t1', as_index=False)['profit_rel'].mean()\n",
    "\n",
    "    # Rename the column to profit_rel_sum\n",
    "    grouped_df_t1.rename(columns={'profit_rel': 'profit_rel_sum'}, inplace=True)\n",
    "\n",
    "    # Determine the start date for the date range\n",
    "    start_date = grouped_df_t1['month_t1'].min()\n",
    "\n",
    "    # Generate a date range from start_date to end_date with a monthly frequency\n",
    "    date_range = pd.period_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "    # Expand the DataFrame by reindexing with the date range and filling missing values with 0\n",
    "    expanded_df = grouped_df_t1.set_index('month_t1').reindex(date_range, fill_value=0).reset_index()\n",
    "\n",
    "    # Convert the Period objects to strings\n",
    "    months = expanded_df['index'].astype(str)\n",
    "\n",
    "    # Calculate the returns\n",
    "    returns = expanded_df['profit_rel_sum'].tolist()\n",
    "    returns_plus_one = np.array(returns) + 1\n",
    "    total_returns = returns_plus_one.cumprod().tolist()\n",
    "\n",
    "    return total_returns, months, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_performance_meta_labeling(df_test, prediction_loaded, show_exclusion = False, bb = True):\n",
    "    \"\"\"\n",
    "    Visualize the performance of Meta-Labeling.\n",
    "\n",
    "    Args:\n",
    "        df_test (dataframe): The test data.\n",
    "        prediction_loaded (array): The prediction values (0 or 1).\n",
    "        show_exclusion (boolean, optional): Flag to determine whether to show rejected trades. Defaults to False.\n",
    "        bb (boolean, optional): Flag to determine whether BB-Strategy or TBM-Strategy is used. Defaults to True.\n",
    "    \"\"\"\n",
    "    # Get inclusion and exclusion dataframes\n",
    "    df, inclusion, exclusion = get_inclusion_exclusion_df(df_test, prediction_loaded, russel=False)\n",
    "    df_russell, inclusion_russell, exclusion_russell = get_inclusion_exclusion_df(df_test, prediction_loaded, russel=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Calculate necessary values for visualization for all assets\n",
    "    total_returns_df, months_df, returns_df = calc_overall_profit_for_vis(inclusion)\n",
    "    total_returns_df_test, months_df_test, returns_df_test = calc_overall_profit_for_vis(df)\n",
    "\n",
    "    # Calculate necessary values for visualization for Russell assets\n",
    "    total_returns_df_russell, months_df_russell, returns_df_russell = calc_overall_profit_for_vis(inclusion_russell)\n",
    "    total_returns_df_test_russell, months_df_test_russell, returns_df_test_russell = calc_overall_profit_for_vis(df_russell)\n",
    "    \n",
    "    # Plot all assets total returns\n",
    "    plt.plot(months_df_test, total_returns_df_test, marker='o', linestyle='-', color='#0E21A0', label='All Assets')\n",
    "    \n",
    "    # Plot Russell assets total returns\n",
    "    plt.plot(months_df_test_russell, total_returns_df_test_russell, marker='o', linestyle='-', color='#900C3F', label='Russell Assets')\n",
    "    \n",
    "    if bb:\n",
    "        # Plot all assets total returns with LGBM 0.9 (best model for BB-Strategy)\n",
    "        plt.plot(months_df, total_returns_df, marker='o', linestyle='-', color='#9D44C0', label='All Assets w/ LGBM 0.9')\n",
    "        \n",
    "        # Plot Russell assets\n",
    "        plt.plot(months_df_russell, total_returns_df_russell, marker='o', linestyle='-', color='#F94C10', label='Russell Assets w/ LGBM 0.9')\n",
    "        \n",
    "        # Set title for BB-Strategy\n",
    "        plt.title('Total Returns Out-Of-Sample BB-Strategy')\n",
    "    else:\n",
    "        # Plot all assets total returns with XGBoost 0.6 (best model for TBM-Strategy)\n",
    "        plt.plot(months_df, total_returns_df, marker='o', linestyle='-', color='#9D44C0', label='All Assets w/ XGBoost 0.6')\n",
    "        \n",
    "        # Plot Russell assets total returns with XGBoost 0.6\n",
    "        plt.plot(months_df_russell, total_returns_df_russell, marker='o', linestyle='-', color='#F94C10', label='Russell Assets w/ XGBoost 0.6')\n",
    "        \n",
    "        # Set title for TBM-Strategy\n",
    "        plt.title('Total Returns Out-Of-Sample TBM-Strategy')\n",
    "\n",
    "    if show_exclusion:\n",
    "        total_returns_df_exc, months_df_exc, returns_df_exc  = calc_overall_profit_for_vis(exclusion)\n",
    "        total_returns_df_exc_russell, months_df_exc_russell, returns_df_exc_russell  = calc_overall_profit_for_vis(exclusion_russell)\n",
    "\n",
    "        plt.plot(months_df_exc, total_returns_df_exc, marker='o', linestyle='-', color='#EC53B0', label='All Assets Only Excluded Trades')\n",
    "        plt.plot(months_df_exc_russell, total_returns_df_exc_russell, marker='o', linestyle='-', color='#F8DE22', label='Russell Assets Only Excluded Trades')\n",
    "\n",
    "    # Set Configurations for visualization\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Total Return')\n",
    "    plt.title('Total Returns Out-Of-Sample BB-Strategy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_default + '/visuals/' + 'TBM_XGB_0_6_total_returns', dpi='figure') # Store figure\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Primary and Secondary Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, return_values = False, return_as_df = False, verbose = True):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a classification model based on precision, recall, f1-score, and accuracy.\n",
    "\n",
    "    Args:\n",
    "        y_true (series): True labels.\n",
    "        y_pred (series): Predicted labels.\n",
    "        return_values (boolean, optional): Whether to return performance metrics as separate values. Defaults to False.\n",
    "        return_as_df (boolean, optional): Whether to return performance metrics as a DataFrame. Defaults to False.\n",
    "        verbose (boolean, optional): Whether to print the performance metrics to console. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        If return_values is True:\n",
    "            If return_as_df is True:\n",
    "                dataframe: Performance metrics as a DataFrame.\n",
    "            else:\n",
    "                tuple: Performance metrics as separate values (confusion matrix, precision, accuracy, recall, f1-score).\n",
    "        else:\n",
    "            None: Performance metrics plotted as a confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the performance metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1_s = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if verbose:\n",
    "        # Print the metrics to console\n",
    "        print('')\n",
    "        print('-' * 40)\n",
    "        print('Accuracy:', accuracy)\n",
    "        print('Precision:', precision)\n",
    "        print('Recall:', recall)\n",
    "        print('F1-Score:', f1_s)\n",
    "        print('-' * 40)\n",
    "\n",
    "    # Generate and return a confusion matrix plot\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('CM', cm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "\n",
    "    if return_values:\n",
    "        if return_as_df:\n",
    "            data = {\n",
    "                'Accuracy': [accuracy],\n",
    "                'Precision': [precision],\n",
    "                'Recall': [recall],\n",
    "                'F1-Score': [f1_s],\n",
    "            }\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            return cm, precision, accuracy, recall, f1_s\n",
    "    else:\n",
    "        return disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_primary(df_trading_opportunities, return_values = False, return_as_df = False, verbose = True):\n",
    "    \"\"\"\n",
    "    Prepare necessary series for evaluation. Here, actual and predicted values.\n",
    "\n",
    "    Args:\n",
    "        df_trading_opportunities (dataframe): DataFrame all trades as determined by the trading strategy.\n",
    "        return_values (boolean, optional): If True, returns evaluation metrics as values. Defaults to False.\n",
    "        return_as_df (boolean, optional): If True, returns evaluation metrics as a DataFrame. Defaults to False.\n",
    "        verbose (boolean, optional): If True, prints evaluation metrics. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Method call: Calling evaluate_model().\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with the 'bin' column from df_trading_opportunities\n",
    "    primary_forecast = pd.DataFrame(df_trading_opportunities['bin'])\n",
    "\n",
    "    # Set all predictions as 1 as our primary model only predicts opportunities\n",
    "    primary_forecast['pred'] = 1\n",
    "\n",
    "    # Rename 'bin' column to 'actual'\n",
    "    primary_forecast.columns = ['actual', 'pred']\n",
    "\n",
    "    # Extract 'actual' and 'pred' columns\n",
    "    actual = primary_forecast['actual']\n",
    "    pred = primary_forecast['pred']\n",
    "\n",
    "    # Call evaluate_model function to evaluate the model's predictions\n",
    "    return evaluate_model(actual, pred, return_values, return_as_df, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_primary_and_secondary(bin, secondary_pred, threshold = None, return_values = False, return_as_df = False, verbose = True):\n",
    "    \"\"\"\n",
    "    Evaluate the primary and secondary predictions.\n",
    "\n",
    "    Args:\n",
    "        bin: The actual classes of the predictions.\n",
    "        secondary_pred: The secondary predictions.\n",
    "        threshold: The threshold for classifying secondary predictions.\n",
    "        return_values: Whether to return the evaluation values.\n",
    "        return_as_df: Whether to return the evaluation values as a DataFrame.\n",
    "        verbose: Whether to print verbose information.\n",
    "\n",
    "    Returns:\n",
    "        The evaluation results.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the secondary predictions contain invalid values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check input data\n",
    "    if np.all((secondary_pred == 0) | (secondary_pred == 1) & (threshold is not None)):\n",
    "        print(\"Threshold is given even though secondary_pred does not contain confidences.\")\n",
    "        return\n",
    "\n",
    "    # Apply threshold to the secondary predictions if provided\n",
    "    if threshold is not None:\n",
    "        mask = secondary_pred > threshold\n",
    "        secondary_pred = mask.astype(int)\n",
    "        \n",
    "    # Evaluate the model using the primary and secondary predictions\n",
    "    return evaluate_model(bin, secondary_pred, return_values, return_as_df, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trading_df(df_trading, secondary_pred):\n",
    "    \"\"\"\n",
    "    Update the trading dataframe based on a secondary prediction. Method only keeps trades where the secondary model predicted the trade as profitable.\n",
    "\n",
    "    Args:\n",
    "        df_trading (dataframe): The original trading dataframe.\n",
    "        secondary_pred (series): The prediction of the secondary models.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The updated trading dataframe.\n",
    "    \"\"\"\n",
    "    # Create a copy of the original dataframe\n",
    "    df_output = df_trading.copy(deep=True)\n",
    "    \n",
    "    # Get the trades where the secondary model predicted the trade as profitable\n",
    "    df_output = df_output[secondary_pred == 1]\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_full_evaluation_w_model(model, df, df_X, df_y, save = False):\n",
    "    \"\"\"\n",
    "    Calculate the full evaluation using a model on a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        model (machine learning model): The trained model, which is used as the secondary model.\n",
    "        df (dataframe): The dataframe to test on (here: the out-of-sample data)\n",
    "        df_X (dataframe): The features of the dataframe df.\n",
    "        df_y (series): The target variable of the dataframe df.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: A dataframe containing the evaluation for (1) primary strategy (2) Meta-Labeling that includes the secondary prediction (3) Only the trade that were retained by Meta-Labeling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate primary strategy\n",
    "    primary_df = evaluate_primary(df, return_values=True, return_as_df=True, verbose=False)\n",
    "\n",
    "    # Create secondary predictions using the model\n",
    "    secondary_prediction = model.predict(df_X)\n",
    "\n",
    "    # Evaluate inclusion of Meta-Labeling, but keep the trades that were predicted as unprofitable\n",
    "    secondary_df = evaluate_model(df_y, secondary_prediction, return_values=True, return_as_df=True, verbose=False)\n",
    "\n",
    "    # Create dataframe that only contains trades which were validated by Meta-Labeling\n",
    "    updated_tradings = update_trading_df(df, secondary_prediction)\n",
    "\n",
    "    # Evaluate performance of the trades that were validated by Meta-Labeling\n",
    "    updated_df = evaluate_primary(updated_tradings, return_values=True, return_as_df=True, verbose=False)\n",
    "\n",
    "    # Create dataframe\n",
    "    primary_df['model'] = 'primary'\n",
    "    secondary_df['model'] = 'secondary'\n",
    "    updated_df['model'] = 'updated'\n",
    "    performance = pd.concat([primary_df, secondary_df, updated_df], ignore_index=True)\n",
    "\n",
    "    if save:\n",
    "        performance_json = performance.to_json(orient='records')\n",
    "\n",
    "        with open(path_default + '/overall_performance.json', 'w') as file:\n",
    "            file.write(performance_json)\n",
    "\n",
    "    return performance\n",
    "\n",
    "\n",
    "\n",
    "def calc_full_evaluation_w_prediction(prediction, df, df_X, df_y, save = False):\n",
    "    \"\"\"\n",
    "    Calculate the full evaluation using the predictions of a secondary model on a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        prediction (array): The predictions of the secondary model on the out-of-sample data.\n",
    "        df (dataframe): The dataframe to test on (here: the out-of-sample data)\n",
    "        df_X (dataframe): The features of the dataframe df.\n",
    "        df_y (series): The target variable of the dataframe df.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: A dataframe containing the evaluation for (1) primary strategy (2) Meta-Labeling that includes the secondary prediction (3) Only the trade that were retained by Meta-Labeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate primary strategy\n",
    "    primary_df = evaluate_primary(df, return_values=True, return_as_df=True, verbose=False)\n",
    "    \n",
    "    # Evaluate inclusion of Meta-Labeling, but keep the trades that were predicted as unprofitable\n",
    "    secondary_df = evaluate_model(df_y, prediction, return_values=True, return_as_df=True, verbose=False)\n",
    "\n",
    "    # Create dataframe that only contains trades which were validated by Meta-Labeling\n",
    "    updated_tradings = update_trading_df(df, prediction)\n",
    "\n",
    "    # Evaluate performance of the trades that were validated by Meta-Labeling\n",
    "    updated_df = evaluate_primary(updated_tradings, return_values=True, return_as_df=True, verbose=False) # updated scores\n",
    "\n",
    "    # Create dataframe\n",
    "    primary_df['model'] = 'primary'\n",
    "    secondary_df['model'] = 'secondary'\n",
    "    updated_df['model'] = 'updated'\n",
    "    performance = pd.concat([primary_df, secondary_df, updated_df], ignore_index=True)\n",
    "    \n",
    "    if save:\n",
    "        performance_json = performance.to_json(orient='records')\n",
    "        with open(path_default + '/overall_performance.json', 'w') as file:\n",
    "            file.write(performance_json)\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods to assess performance metrics based on individual assets\n",
    "\n",
    "def order_trades_by_date(df):\n",
    "    \"\"\"\n",
    "    Orders trades within a DataFrame by date for each group of trades with the same 'cusip' value.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): All trades as determined by the primary strategy.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The dataframe with the trades ordered by date for each group of trades.\n",
    "    \"\"\"\n",
    "    # Sort each group by the 'date' column within the DataFrame\n",
    "    df = df.groupby('cusip', group_keys = False).apply(lambda x: x.sort_values('date')).reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def set_cusip_date_index(df):\n",
    "    \"\"\"\n",
    "    Set the 'date' column as a datetime and set the index of the dataframe based on 'cusip' and 'date' columns.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): All trades as determined by the primary strategy.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: The Dataframe with the 'date' column as a datetime and the index set based on 'cusip' and 'date' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_index = df.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "    return df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_maximum_loss(df):\n",
    "    \"\"\"\n",
    "    Calculate the maximum loss for each unique asset in the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The input dataframe that contains trades, whereby the cusip and date need to be the index.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the maximum loss and a list of all the individual maximum losses.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get all unique assets\n",
    "    tokens = df.index.get_level_values(0).unique()\n",
    "\n",
    "    # List to store the individual maximum losses\n",
    "    minimums = []\n",
    "\n",
    "    # Loop through each unique asset and calculate the maximum loss\n",
    "    for token in tqdm(tokens, desc='Maximum Loss Calculation'):\n",
    "\n",
    "        # Get sub-df which only contains assets for the given unique cusip identifier\n",
    "        df_token = df.loc[token]\n",
    "\n",
    "        # Maximum loss = minimum relative profit\n",
    "        min_loss = df_token['profit_rel'].min()\n",
    "\n",
    "        # Only add if it is a loss as the minimum relative profit can be positive, too\n",
    "        if min_loss < 0:\n",
    "            minimums.append(min_loss)\n",
    "\n",
    "    # Calculate the average\n",
    "    avg_loss = np.mean(minimums)\n",
    "\n",
    "    return avg_loss, minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_maximum_drawdown(df):\n",
    "    \"\"\"\n",
    "    Calculate the maximum drawdown for all trades in given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The input dataframe that contains trades, whereby the cusip and date need to be the index.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the maximum drawdown value and a list of drawdown values for each token.\n",
    "    \"\"\"\n",
    "    # Get all unique cusip values\n",
    "    tokens = df.index.get_level_values(0).unique()\n",
    "    \n",
    "    drawdowns = []\n",
    "    \n",
    "    for token in tqdm(tokens, desc='Maximum Drawdown Calculation'):\n",
    "        df_token = df.loc[token]\n",
    "        \n",
    "        # Calculate cumulative return\n",
    "        df_token['cumulative_return'] = df_token['profit_rel'].cumsum() + 1\n",
    "        \n",
    "        # Calculate peak value\n",
    "        df_token['peak_value'] = df_token['cumulative_return'].cummax()\n",
    "        \n",
    "        # Calculate drawdown\n",
    "        df_token['drawdown'] = (df_token['peak_value'] - df_token['cumulative_return']) / df_token['peak_value']\n",
    "        \n",
    "        # Calculate maximum drawdown and append to drawdowns list\n",
    "        maximum_drawdown = df_token['drawdown'].max()\n",
    "        drawdowns.append(maximum_drawdown)       \n",
    "\n",
    "    # Calculate average maximum drawdown\n",
    "    avg_maximum_drawdown = np.mean(drawdowns)\n",
    "    \n",
    "    return avg_maximum_drawdown, drawdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profits_trading_strat(df):\n",
    "    \"\"\"\n",
    "    Calculate the average profit per trade for the trading strategy.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "\n",
    "    Returns:\n",
    "        Float: The average profit per trade.\n",
    "    \"\"\"\n",
    "    return df['profit_rel'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profit_factor(df):\n",
    "    \"\"\"\n",
    "    Calculates the profit factor for given trades.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "\n",
    "    Returns:\n",
    "        Float: The calculated profit factor.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to get profitable and unprofitable trades.\n",
    "    df_pos = df[df['bin'] == 1]\n",
    "    df_neg = df[df['bin'] == 0]\n",
    "\n",
    "    # Calculate the sum of the relative profits for each group\n",
    "    pos_sum = df_pos['profit_rel'].sum()\n",
    "    neg_sum = df_neg['profit_rel'].sum()\n",
    "\n",
    "    profit_factor = pos_sum / abs(neg_sum)\n",
    "\n",
    "    return profit_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hit_ratio(df):\n",
    "    \"\"\"\n",
    "    Calculate the hit ratio.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "    Returns:\n",
    "        Float: The hit ratio as a percentage.\n",
    "    \"\"\"\n",
    "    number_of_trades = len(df)  \n",
    "    winning_trades = df[df['bin'] == 1] \n",
    "    return len(winning_trades) / number_of_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overall_profit(df):\n",
    "    \"\"\"\n",
    "    Calculate the overall profit that can be used to calculate the annualized profits.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the overall profit and the number of months where trades occurred.\n",
    "    \"\"\"\n",
    "\n",
    "    column_to_group_by = 't1' # Group by the selling date (t1 = selling date)\n",
    "\n",
    "    # Convert column to datetime\n",
    "    df[column_to_group_by] = pd.to_datetime(df[column_to_group_by])\n",
    "\n",
    "    # Group by month and calculate average profit_rel per month\n",
    "    df['month'] = df[column_to_group_by].dt.to_period('M')\n",
    "    grouped_df = df.groupby('month', as_index=False)['profit_rel'].mean()\n",
    "    grouped_df.rename(columns={'profit_rel': 'profit_rel_sum'}, inplace=True)\n",
    "\n",
    "    # Calculate returns\n",
    "    returns = grouped_df['profit_rel_sum'].tolist()\n",
    "    returns_plus_one = np.array(returns) + 1\n",
    "    total_returns = returns_plus_one.cumprod().tolist()\n",
    "\n",
    "    # Overall profit at the end of the available months - 1 (initial investment)\n",
    "    overall_profit = total_returns[-1] - 1\n",
    "    num_months = len(grouped_df)\n",
    "\n",
    "    return overall_profit, num_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_financial_performance_eval(df, prediction, fraction=False, russel=True):\n",
    "    \"\"\"\n",
    "    Conduct a full financial performance for given trades.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "        prediction (array): The prediction of the secondary model.\n",
    "        fraction (boolean, optional): If true, fractional investments are given in the prediction array. Defaults to False.\n",
    "        russel (boolean, optional): Indicates whether only Russell 3000 assets should be considered. Defaults to True.\n",
    "    \"\"\"\n",
    "    # Load buy and hold performance data\n",
    "    df_bnh = load_ml_data(path_data_buy_hold, 'v1_BB_buy_n_hold_with_fees.csv', vertical_barrier=False)\n",
    "\n",
    "    print('Russel Activated?:', russel)\n",
    "    if fraction:\n",
    "        mask_remove = prediction == 0\n",
    "        mask_half = prediction == 0.5\n",
    "        mask_keep = prediction == 1\n",
    "        \n",
    "        updated_trades_keep = df[mask_keep] # Trades to keep\n",
    "        updated_trades_half = df[mask_half] # Trades to half the investment capital\n",
    "        updated_trades_half['profit_rel'] /= 2\n",
    "\n",
    "        # Combine Trades that are conducted\n",
    "        updated_trades = updated_trades_keep.append(updated_trades_half, ignore_index = True)\n",
    "        \n",
    "        original_trades = df.copy(deep=True)\n",
    "\n",
    "        # Trades that were rejected\n",
    "        deleted_trades = df[mask_remove]\n",
    "    else:\n",
    "        mask_remove = prediction == 0\n",
    "        mask_keep = prediction == 1\n",
    "        updated_trades = df[mask_keep] # Validated trades\n",
    "        original_trades = df.copy(deep=True) # All trades\n",
    "        deleted_trades = df[mask_remove] # Rejected trades\n",
    "        \n",
    "    # check if you want to only take the russel assets\n",
    "    if russel:\n",
    "        # Read CUSIPs that were part of the Russell 3000\n",
    "        russel_2020 = pd.read_csv(path_data_combined + '/cusips_from_2020.csv')\n",
    "        russel_cusips = russel_2020['0'].tolist()\n",
    "        \n",
    "        updated_trades = updated_trades[updated_trades['cusip'].isin(russel_cusips)]\n",
    "        original_trades = original_trades[original_trades['cusip'].isin(russel_cusips)]\n",
    "        deleted_trades = deleted_trades[deleted_trades['cusip'].isin(russel_cusips)]      \n",
    "        \n",
    "        df_bnh = df_bnh[df_bnh['cusip'].isin(russel_cusips)]\n",
    "\n",
    "    # Filter buy and hold by trades that were validated by Meta-Labeling\n",
    "    remaining_cusips = updated_trades['cusip'].unique()\n",
    "    print('#Remainig:', len(remaining_cusips))\n",
    "    df_bnh = df_bnh[df_bnh['cusip'].isin(remaining_cusips)]\n",
    "        \n",
    "    print('Number of Trades Original', len(original_trades))\n",
    "    print('Number of Trades Updated', len(updated_trades))\n",
    "    \n",
    "    profit_overall_or, no_months_orig = calc_overall_profit(original_trades)\n",
    "    profit_overall_up, no_month_upd = calc_overall_profit(updated_trades)\n",
    "    # Annualized the profits by diving by the number of months and multiplying with 12\n",
    "    print('Overall Profits Original per year:', (profit_overall_or / no_months_orig) * 12)\n",
    "    print('Overall Profits Updated per year:', (profit_overall_up / no_month_upd) * 12)\n",
    "    \n",
    "    # Order trades by date and set index of dataframes as date and cusip\n",
    "    updated_trades = order_trades_by_date(updated_trades)\n",
    "    updated_trades = set_cusip_date_index(updated_trades)\n",
    "    original_trades = order_trades_by_date(original_trades)\n",
    "    original_trades = set_cusip_date_index(original_trades)\n",
    "\n",
    "    # Maximum Loss\n",
    "    avg_max_loss_original, _ = calc_avg_maximum_loss(original_trades)\n",
    "    avg_max_loss_updated, _ = calc_avg_maximum_loss(updated_trades)\n",
    "    print('AVG Max. Loss Original:', avg_max_loss_original)\n",
    "    print('AVG Max. Loss Updated:', avg_max_loss_updated)\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    avg_max_drawdown_original, _ = calc_avg_maximum_drawdown(original_trades)\n",
    "    avg_max_drawdown_updated, _ = calc_avg_maximum_drawdown(updated_trades)\n",
    "    print('Average Maximum Drawdown Original:', avg_max_drawdown_original)\n",
    "    print('Average Maximum Drawdown Updated:', avg_max_drawdown_updated)\n",
    "    \n",
    "    # Profit Factor\n",
    "    profit_factor_original = calc_profit_factor(original_trades)\n",
    "    profit_factor_updated = calc_profit_factor(updated_trades)\n",
    "    print('Profit Factor Original:', profit_factor_original)\n",
    "    print('Profit Factor Updated:', profit_factor_updated)\n",
    "    \n",
    "    # Hit Ratio\n",
    "    hit_ratio_original = calc_hit_ratio(original_trades)\n",
    "    hit_ratio_updated = calc_hit_ratio(updated_trades)\n",
    "    print('Hit Ratio Original:', hit_ratio_original)\n",
    "    print('Hit Ratio Updated:', hit_ratio_updated)\n",
    "    \n",
    "    # Average Profit per Trade\n",
    "    profit_avg_or = calc_profits_trading_strat(original_trades)\n",
    "    profit_avg_up = calc_profits_trading_strat(updated_trades)\n",
    "    print('Average Profit per Trade Original:', profit_avg_or)\n",
    "    print('Average Profit per Trade Updated:', profit_avg_up)\n",
    "    \n",
    "    # Buy and Hold Performance annualized\n",
    "    print('Profits Buy and Hold:', df_bnh['profit_rel'].mean() / no_months_orig * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inclusion_exclusion_df(df, prediction, russel=False):\n",
    "    \"\"\"\n",
    "    Get dataframe of validated and rejected trades.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The trades.\n",
    "        prediction (array): The prediction of the secondary model.\n",
    "        russel (boolean, optional): Indicates whether only Russell 3000 assets should be considered. Defaults to True.\n",
    "    Returns:\n",
    "        Tuple: A tuple containing all trades, validated trades, and rejected trades\n",
    "    \"\"\"\n",
    "    df_all = df.copy(deep=True)\n",
    "    df_excluded = df.copy(deep=True)\n",
    "    df_included = df.copy(deep=True)\n",
    "\n",
    "    df_included = df_included[prediction == 1] # Profitable trades\n",
    "    df_excluded = df_excluded[prediction == 0] # Unprofitable trades\n",
    "\n",
    "    if russel: # Filter by CUSIPs that were part of the Russell 3000\n",
    "        russel_2020 = pd.read_csv(path_data_combined + '/cusips_from_2020.csv')\n",
    "        russel_cusips = russel_2020['0'].tolist()\n",
    "        \n",
    "        df_included = df_included[df_included['cusip'].isin(russel_cusips)]\n",
    "        df_excluded = df_excluded[df_excluded['cusip'].isin(russel_cusips)]\n",
    "        df_all = df_all[df_all['cusip'].isin(russel_cusips)]\n",
    "    return df_all, df_included, df_excluded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_sklearn(model, path, model_name):\n",
    "    \"\"\"\n",
    "    Saves models from sklearn to filesystem.\n",
    "\n",
    "    Args:\n",
    "        model (sklearn model): Sklearn model.\n",
    "        path (string): The path to save the model to.\n",
    "        model_name (string): The name of the model.\n",
    "    \"\"\"\n",
    "    joblib.dump(model, path + '/' + model_name + '.joblib')\n",
    "\n",
    "def load_model_sklearn(path, model_name):\n",
    "    \"\"\"\n",
    "    Loads sklearn model from filesystem.\n",
    "\n",
    "    Args:\n",
    "        path (string): The path where the model is stored.\n",
    "        model_name (string): The name of the model.\n",
    "    Returns:\n",
    "        Model: The loaded model.\n",
    "    \"\"\"\n",
    "    model = joblib.load(path + '/' + model_name + '.joblib')\n",
    "    return model\n",
    "\n",
    "def store_prediction_csv(prediction, filename, path=path_default):\n",
    "    \"\"\"\n",
    "    Saves prediction of a model to filesystem.\n",
    "\n",
    "    Args:\n",
    "        prediction (array): The predicted values of the secondary model.\n",
    "        filename (string): The name of the file.\n",
    "        path (string, optional): The path to save the file. Defaults to the default file system path as configured in the notebook.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(prediction)\n",
    "    df.to_csv(path + '/data_v5/predictions/' + filename, index=False, header=False)\n",
    "\n",
    "def load_prediction_csv(filename, proba = False, path = path_default):\n",
    "    \"\"\"\n",
    "    Loads prediction of a model from filesystem.\n",
    "\n",
    "    Args:\n",
    "        filename (string): The name of the file.\n",
    "        proba (boolean, optional): \n",
    "        path (string, optional): The path to save the file. Defaults to the default file system path as configured in the notebook.\n",
    "    Returns:\n",
    "        Either the absolute values (0 or 1), or the predicted probabilities (depending on the value of proba).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path + '/data_v5/predictions/' + filename, header=None)\n",
    "    if proba:\n",
    "        return df.values\n",
    "    else:\n",
    "        return np.ravel(df.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_k_fold(model, config, X, y, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Train a model using k-fold cross validation.\n",
    "\n",
    "    Args:\n",
    "        model (model): The model that should be trained.\n",
    "        config (dict): The configuration for the model.\n",
    "        X (dataframe): The features.\n",
    "        y (series): The targets.\n",
    "        threshold (float, optional): The threshold to label a trade as profitable. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    k_fold = KFold(n_splits = 3, random_state = 42, shuffle = True)\n",
    "    pre, acc, rec, f1_s, mcc_s = [], [], [], [], [] # Lists to store the evaluation metrics\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(k_fold.split(X, y)), total=k_fold.get_n_splits(), desc='k-fold:          '):\n",
    "        model.set_params(**config) \n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        \n",
    "        # Predict Probabilities\n",
    "        y_pred_proba = model.predict_proba(X.iloc[test_idx])\n",
    "\n",
    "        # Get absolute predictions using the threshold\n",
    "        y_pred = label_predictions(y_pred_proba, threshold)\n",
    "\n",
    "        # Evaluate model\n",
    "        _, fold_precision, fold_accuracy, fold_recall, fold_f1, fold_mcc = evaluate_model(y.iloc[test_idx], y_pred, return_values=True, verbose=False) # evaluate\n",
    "        \n",
    "        # Store metrics\n",
    "        pre.append(fold_precision)\n",
    "        acc.append(fold_accuracy)\n",
    "        rec.append(fold_recall)\n",
    "        f1_s.append(fold_f1)\n",
    "        mcc_s.append(fold_mcc)\n",
    "    \n",
    "    # Average metrics\n",
    "    precision = sum(pre) / len(pre)\n",
    "    accuracy = sum(acc) / len(acc)\n",
    "    recalll = sum(rec) / len(rec)\n",
    "    f1 = sum(f1_s) / len(f1_s)\n",
    "    mcc = sum(mcc_s) / len(mcc_s)\n",
    "    \n",
    "    # Print performance\n",
    "    print('Precision', precision)\n",
    "    print('Accuracy', accuracy)\n",
    "    print('Recall', recalll)\n",
    "    print('F1', f1)\n",
    "    print('MCC', mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, X, y):\n",
    "    \"\"\"\n",
    "    Train a model by fitting the features to the targets.\n",
    "\n",
    "    Args:\n",
    "        model (model): The model that should be trained.\n",
    "        config (dict): The configuration for the model.\n",
    "        X (dataframe): The features.\n",
    "        y (series): The targets.\n",
    "    \"\"\"\n",
    "    model.set_params(**config)\n",
    "    model.fit(X.values, y.values)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_k_fold_feature_imp(model, config, X, y):\n",
    "    \"\"\"\n",
    "    Train a model and get feature importance as average across all folds.\n",
    "\n",
    "    Args:\n",
    "        model (model): The model that should be trained.\n",
    "        config (dict): The configuration for the model.\n",
    "        X (dataframe): The features.\n",
    "        y (series): The targets.\n",
    "    Returns:\n",
    "        Dataframe: The feature importances averaged across all folds.\n",
    "    \"\"\"\n",
    "    feature_imp = pd.DataFrame()\n",
    "    k_fold = KFold(n_splits = 3, random_state = 42, shuffle = True)\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(k_fold.split(X, y)), total = k_fold.get_n_splits(), desc = 'k-fold: '):\n",
    "        model.set_params(**config) # set given params\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx]) # fit on training data\n",
    "        \n",
    "        # Retrieve feature importances\n",
    "        fold_feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_, X.columns)), columns = ['Value','Feature'])\n",
    "        feature_imp = pd.concat([feature_imp, fold_feature_imp], axis=0)\n",
    "\n",
    "    # Group by 'Feature' and calculate the mean feature importance across all folds\n",
    "    feature_imp = feature_imp.groupby('Feature')['Value'].mean().reset_index()\n",
    "    feature_imp = feature_imp.sort_values(by = 'Value', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Configuration for BB-Strategy\n",
    "class CustomMLP_BB(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Neural Network Configuration for TBM-Strategy\n",
    "class CustomMLP_TBM(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset for the given data\n",
    "class CustomStockDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            self.X = torch.from_numpy(X.values)\n",
    "            self.y = torch.from_numpy(y.values)\n",
    "\n",
    "            self.X = self.X.to(torch.float32)\n",
    "            self.y = self.y.to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_nn(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a neural network.\n",
    "\n",
    "    Args:\n",
    "        y_pred (tensor): The predictions based on the neural network.\n",
    "        y_test (tensor): The actual values based of the trades (profitable or unprofitable).\n",
    "    Returns:\n",
    "        Tuple: A tuple containing the accuracy, precision, recall and the f1-score.\n",
    "    \"\"\"\n",
    "    y_pred_rounded = torch.round(y_pred)\n",
    "    y_pred_list = [item for sublist in y_pred_rounded.tolist() for item in sublist]\n",
    "    y_test_list = [item for sublist in y_test.tolist() for item in sublist]\n",
    "\n",
    "    acc = accuracy_score(y_test_list, y_pred_list)\n",
    "    precision = precision_score(y_test_list, y_pred_list)\n",
    "    recall = recall_score(y_test_list, y_pred_list)\n",
    "    f1 = f1_score(y_test_list, y_pred_list)\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_performance(path, df, fold):\n",
    "    \"\"\"\n",
    "    Saving the performance of a model to the filesystem.\n",
    "\n",
    "    Args:\n",
    "        path (string): The path to save the performance.\n",
    "        df (dataframe): The dataframe that contains the performance and which should be saved.\n",
    "        fold (int): The fold number of the given performance.\n",
    "    \"\"\"\n",
    "    output_dir = path + '/' + 'fold_' + str(fold)\n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except:\n",
    "        pass\n",
    "    df.to_csv(output_dir +  '/' +  'performance.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_dict(nn):\n",
    "    \"\"\"\n",
    "    Getting the dictionary with the structure of the neural network (PyTorch).\n",
    "\n",
    "    Args:\n",
    "        nn (model): The neural network.\n",
    "    Returns:\n",
    "        Dictionary: The structure of the neural network.\n",
    "    \"\"\"\n",
    "    structure_dict = {}\n",
    "    for name, module in nn.named_modules():\n",
    "        if name == '':\n",
    "            structure_dict[name] = {'input_shape': [None] + list(nn.layers[0].weight.size())[1:]}\n",
    "        else:\n",
    "            structure_dict[name] = str(module)\n",
    "    return structure_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(path, config):\n",
    "    \"\"\"\n",
    "    Saving the configuration of the neural network to the filesystem.\n",
    "    \n",
    "    Args:\n",
    "        path (string): The path to save the configuration.\n",
    "        config (dict): The configuration of the neural network.\n",
    "    \"\"\"\n",
    "    with open(path + '/configuration.json', 'w') as f:\n",
    "        json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    \"\"\"\n",
    "    Loading a neural network from the filesystem.\n",
    "\n",
    "    Args:\n",
    "        model (model): The neural network model.\n",
    "        path (string): The path to load the model from.\n",
    "    Returns:\n",
    "        model: The loaded neural network.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval() # Set to eval mode\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, epoch, path):\n",
    "    \"\"\"\n",
    "    Saving a neural network to the filesystem.\n",
    "\n",
    "    Args:\n",
    "        model (model): The neural network model.\n",
    "        epoch (int): The epoch number of the neural network.\n",
    "        path (string): The path to save the model to.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path + '/epoch_' + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_loss_plot_to_file(path, losses_train, losses_val):\n",
    "    \"\"\"\n",
    "    Saving the loss plots to an HTML file.\n",
    "    \n",
    "    Args:\n",
    "        path (string): The path to save the plot to.\n",
    "        losses_train (list): The training losses.\n",
    "        losses_val (list): The validation losses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate x-axis values (epochs)\n",
    "    epochs = list(range(1, len(losses_train) + 1))\n",
    "\n",
    "    losses_tr_cpu = [tensor.cpu().detach().numpy() for tensor in losses_train]\n",
    "    losses_vl_cpu = [tensor.cpu().detach().numpy() for tensor in losses_val]\n",
    "\n",
    "    # Create the interactive line plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add train and validation loss traces\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=losses_tr_cpu, mode='lines', name='Train Loss'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=losses_vl_cpu, mode='lines', name='Validation Loss'))\n",
    "\n",
    "    # Add labels and title\n",
    "    fig.update_layout(title='Train and Validation Losses', xaxis_title='Epochs', yaxis_title='Losses')\n",
    "\n",
    "    # Save the plot as an HTML file\n",
    "    fig.write_html(path + '/losses.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_performance_plot_to_file(path, pre_t, pre_v, rec_t, rec_v, acc_t, acc_v, f1_t, f1_v):\n",
    "    \"\"\"\n",
    "    Saving the plot of the performance metrics to an HTML file.\n",
    "    \n",
    "    Args:\n",
    "        path (string): The path to save the plot to.\n",
    "        pre_t (list): The precision performance metrics on training.\n",
    "        pre_v (list): The precision performance metrics on validation.\n",
    "        rec_t (list): The recall performance metrics on training.\n",
    "        rec_v (list): The recall performance metrics on validation.\n",
    "        acc_t (list): The accuracy performance metrics on training.\n",
    "        acc_v (list): The accuracy performance metrics on validation.\n",
    "        f1_t (list): The f1-score performance metrics on training.\n",
    "        f1_v (list): The f1-score performance metrics on validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate x-axis values (epochs)\n",
    "    epochs = list(range(1, len(pre_t) + 1))\n",
    "\n",
    "    # Create the interactive line plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add train and validation loss traces\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=pre_t, mode='lines', name='PRE Train'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=pre_v, mode='lines', name='PRE Valid'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=rec_t, mode='lines', name='REC Train'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=rec_v, mode='lines', name='REC Valid'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=acc_t, mode='lines', name='ACC Train'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=acc_v, mode='lines', name='ACC Valid'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=f1_t, mode='lines', name='F1 Train'))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=f1_v, mode='lines', name='F1 Valid'))\n",
    "\n",
    "    # Add labels and title\n",
    "    fig.update_layout(title='Train and Validation Performances', xaxis_title='Epochs', yaxis_title='Percentages')\n",
    "\n",
    "    # Save the plot as an HTML file\n",
    "    fig.write_html(path + '/performance.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl , epochs, optimizer, loss_func, fold, execution_path):\n",
    "    \"\"\"\n",
    "    Training loop for neural network (PyTorch).\n",
    "\n",
    "    Args:\n",
    "        model (model): The neural network model.\n",
    "        train_dl (dataloader): The dataloader containing the training data.\n",
    "        val_dl (dataloader): The dataloader containing the validation data.\n",
    "        epochs (int): The maximum number of epochs.\n",
    "        optimizer (optimizer): The optimizer.\n",
    "        loss_func (loss_func): The loss function.\n",
    "        fold (int): The fold number.\n",
    "        execution_path (string): The path to save the model to.\n",
    "    Returns:\n",
    "        Tuple: Training and validation losses.\n",
    "    \"\"\"\n",
    "    # Get GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    wandb.watch(model, loss_func, log='all', log_freq = 100)\n",
    "\n",
    "    print('Device: ', torch.cuda.get_device_name(0))\n",
    "    print(\"_\"*100)\n",
    "    print(\"epoch  | tr-loss  | tr-acc   | tr-f1    | te-loss  | te-acc    | te-f1    \")\n",
    "\n",
    "    # Move model to GPU\n",
    "    model = model.to(device = device)\n",
    "    performance_list = []\n",
    "    \n",
    "    # Loss\n",
    "    all_losses_train = []\n",
    "    all_losses_val = []\n",
    "    \n",
    "    # Precision\n",
    "    all_precision_train = []\n",
    "    all_precision_val = []\n",
    "    \n",
    "    # Accuracy\n",
    "    all_accuracy_train = []\n",
    "    all_accuracy_val = []\n",
    "    \n",
    "    # Recall\n",
    "    all_recall_train = []\n",
    "    all_recall_val = []\n",
    "    \n",
    "    # F1-score\n",
    "    all_f1_train = []\n",
    "    all_f1_val = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_acc, total_precision, total_recall, total_f1 = 0.0, 0.0, 0.0, 0.0, 0.0 # Initialize values\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb  = xb.to(device = device), yb.to(device = device)\n",
    "            \n",
    "            pred = model(xb)\n",
    "            \n",
    "            yb = yb.unsqueeze(1)\n",
    "            yb = yb.float()\n",
    "            \n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            acc, precision, recall, f1 = evaluate_model_nn(pred, yb)\n",
    "            \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_f1 += f1\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        total_loss /= len(train_dl)\n",
    "        total_acc /= len(train_dl)\n",
    "        total_precision /= len(train_dl)\n",
    "        total_recall /= len(train_dl)\n",
    "        total_f1 /= len(train_dl)\n",
    "        \n",
    "\n",
    "        total_loss_val, total_acc_val, total_precision_val, total_recall_val, total_f1_val = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb_val, yb_val in val_dl:\n",
    "                xb_val, yb_val = xb_val.to(device = device), yb_val.to(device = device)\n",
    "                \n",
    "                pred_val = model(xb_val)\n",
    "                \n",
    "                yb_val = yb_val.unsqueeze(1)\n",
    "                yb_val = yb_val.float()\n",
    "                \n",
    "                if torch.all(pred_val == 0):\n",
    "                    pass\n",
    "                elif torch.all(pred_val == 1):\n",
    "                    pass\n",
    "\n",
    "                loss_val = loss_func(pred_val, yb_val)\n",
    "                \n",
    "                acc_val, precision_val, recall_val, f1_val = evaluate_model_nn(pred_val, yb_val)\n",
    "                \n",
    "                total_loss_val += loss_val\n",
    "                total_acc_val += acc_val\n",
    "                total_precision_val += precision_val\n",
    "                total_recall_val += recall_val\n",
    "                total_f1_val += f1_val\n",
    "                \n",
    "        total_loss_val /= len(val_dl)\n",
    "        total_acc_val /= len(val_dl)\n",
    "        total_precision_val /= len(val_dl)\n",
    "        total_recall_val /= len(val_dl)\n",
    "        total_f1_val /= len(val_dl)\n",
    "        \n",
    "        model = model.to(device) \n",
    "\n",
    "        performance = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': total_loss.item(),\n",
    "            'val_loss': total_loss_val.item(),\n",
    "            'train_acc': total_acc.item(),\n",
    "            'val_acc': total_acc_val.item(),\n",
    "            'train_f1': total_f1.item(),\n",
    "            'val_f1': total_f1_val.item(),\n",
    "            'train_precision': total_precision.item(),\n",
    "            'val_precision': total_precision_val.item(),\n",
    "            'train_recall': total_recall.item(),\n",
    "            'val_recall': total_recall_val.item(),\n",
    "        }\n",
    "\n",
    "        performance_list.append(performance)\n",
    "        \n",
    "        all_losses_train.append(total_loss)\n",
    "        all_losses_val.append(total_loss_val)\n",
    "        \n",
    "        all_precision_train.append(total_precision)\n",
    "        all_precision_val.append(total_precision_val)\n",
    "        \n",
    "        all_recall_train.append(total_recall)\n",
    "        all_recall_val.append(total_recall_val)\n",
    "        \n",
    "        all_accuracy_train.append(total_acc)\n",
    "        all_accuracy_val.append(total_acc_val)\n",
    "        \n",
    "        all_f1_train.append(total_f1)\n",
    "        all_f1_val.append(total_f1_val)\n",
    "        \n",
    "        # Log performance to wandb\n",
    "        wandb.log({\n",
    "            f\"epoch\" : epoch, \n",
    "            f\"loss_train\" : total_loss, \n",
    "            f\"loss_val\" : total_loss_val, \n",
    "            f\"precision_train\" : total_precision, \n",
    "            f\"precision_val\" : total_precision_val, \n",
    "            f\"recall_train\" : total_recall, \n",
    "            f\"recall_val\" : total_recall_val, \n",
    "            f\"accuracy_train\" : total_acc, \n",
    "            f\"accuracy_val\" : total_acc_val, \n",
    "            f\"f1_train\" : total_f1, \n",
    "            f\"f1_val\" : total_f1_val, \n",
    "        })\n",
    "        \n",
    "        # Buffering\n",
    "        if (epoch % 50 == 0) & (epoch != 0):\n",
    "            save_model(model, epoch, execution_path)\n",
    "        \n",
    "        print(\"_\"*100)\n",
    "        print(f\"  {epoch + 1}    |  {total_loss.item():.4f}  |  {(total_acc.item()*100):.4f} | {(total_f1.item()*100):.4f}  | {total_loss_val.item():.4f}   |  {(total_acc_val.item()*100):.4f}  |{(total_f1_val.item()*100):.4f}   \")\n",
    "        print()\n",
    "\n",
    "    df_performance = pd.DataFrame(performance_list)\n",
    "    save_model_performance(execution_path, df_performance, fold)\n",
    "    save_html_loss_plot_to_file(execution_path, all_losses_train, all_losses_val)\n",
    "    save_html_performance_plot_to_file(execution_path, all_precision_train, all_precision_val, all_recall_train, all_recall_val, all_accuracy_train, all_accuracy_val, all_f1_train, all_f1_val )\n",
    "    save_model(model, epoch, execution_path)\n",
    "    return all_losses_train, all_losses_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis = read_data_csv()\n",
    "\n",
    "# AAPL = 037833100\n",
    "df_vis_sub = df_vis[df_vis['cusip'] == '037833100']\n",
    "\n",
    "# Get trading signals\n",
    "df_vis_trd = calculate_bb(df_vis_sub)\n",
    "\n",
    "df_vis_trd = df_vis_trd.set_index(df_vis_trd['date'], drop=True)\n",
    "\n",
    "df = df_vis_trd.copy(deep=True)\n",
    "\n",
    "df_sgnls = calculate_trade_signals_bb_tbm(df_vis_trd)\n",
    "\n",
    "# Set boundaries for visualization purposes\n",
    "df = df.loc['2018-01-01':'2018-12-31']\n",
    "df_sgnls = df_sgnls.loc['2018-01-01':'2018-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_axis = df.index\n",
    "\n",
    "# Plotting the Bollinger Bands\n",
    "ax.plot(df.index, df['close'], label='Closing Prices', color='blue')\n",
    "ax.plot(df.index, df['upper_bb'], label='Upper Band', color='turquoise')\n",
    "ax.plot(df.index, df['lower_bb'], label='Lower Band', color='orange')\n",
    "ax.plot(df.index, df['sma_20'], label='Mean', color='black')\n",
    "\n",
    "buy_signals = df_sgnls[df_sgnls['side'] == 1]\n",
    "sell_signals = df_sgnls[df_sgnls['side'] == -1]\n",
    "ax.scatter(buy_signals.index, buy_signals['close'], color='green', lw=0.001, label='Buy Signal', marker='^', s=100, zorder=10)\n",
    "ax.scatter(sell_signals.index, sell_signals['close'], color='red', lw=0.001, label='Sell Signal', marker='v', s=100, zorder=10)\n",
    "ax.fill_between(x_axis, df['upper_bb'], df['lower_bb'], color='grey', alpha=0.15)\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price in US-$')\n",
    "\n",
    "# Adding legend and grid\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Rotating x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "fig.tight_layout(pad=1.5)\n",
    "plt.savefig(path_default + '/visuals/bb.svg', format='svg')\n",
    "    \n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = read_data_csv()\n",
    "df_all = df_all.set_index('date', drop=True)\n",
    "\n",
    "# AAPL = 037833100\n",
    "df_aapl = df_all[df_all['cusip'] == '037833100']\n",
    "\n",
    "# Set boundaries\n",
    "df = df_aapl.loc['2021-01-01':'2021-04-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the date and corresponding price for the first box\n",
    "box1_date = '2021-01-12'  # Change this to your desired date\n",
    "box1_price = df.loc[box1_date, 'close']\n",
    "box1_height = 10  # Change this to your desired height\n",
    "\n",
    "# Specify the date and corresponding price for the second box\n",
    "box2_date = '2021-02-01'  # Change this to your desired date\n",
    "box2_price = df.loc[box2_date, 'close']\n",
    "box2_height = 10  # Change this to your desired height\n",
    "\n",
    "# Specify the date and corresponding price for the third box\n",
    "box3_date = '2021-03-01'  # Change this to your desired date\n",
    "box3_price = df.loc[box3_date, 'close']\n",
    "box3_height = 10  # Change this to your desired height\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_axis = df.index\n",
    "\n",
    "ax.plot(df.index, df['close'], label='Closing Price', color='blue')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price in US-$')\n",
    "\n",
    "# Adding legend and grid\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Set x-axis limits to ensure it goes until '2023-04-01'\n",
    "ax.set_xlim(df.index[0], pd.Timestamp('2021-04-01'))\n",
    "\n",
    "# Rotating x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add the first box\n",
    "box_width1 = pd.Timedelta(days=15)  # Box width of 15 days\n",
    "box_start1 = pd.to_datetime(box1_date)\n",
    "\n",
    "# Add the second box\n",
    "box_width2 = pd.Timedelta(days=15)  # Box width of 10 days\n",
    "box_start2 = pd.to_datetime(box2_date)\n",
    "\n",
    "# Add the third box\n",
    "box_width3 = pd.Timedelta(days=15)  # Box width of 15 days\n",
    "box_start3 = pd.to_datetime(box3_date)\n",
    "\n",
    "# Calculate the maximum and minimum prices for the vertical lines\n",
    "max_price1 = df['close'].max()\n",
    "min_price1 = df['close'].min()\n",
    "\n",
    "max_price2 = df['close'].max()\n",
    "min_price2 = df['close'].min()\n",
    "\n",
    "max_price3 = df['close'].max()\n",
    "min_price3 = df['close'].min()\n",
    "\n",
    "# Calculate the maximum height for the vertical lines\n",
    "max_height1 = min(box1_height / 2, max_price1 - box1_price)\n",
    "min_height1 = min(box1_height / 2, box1_price - min_price1)\n",
    "\n",
    "max_height2 = min(box2_height / 2, max_price2 - box2_price)\n",
    "min_height2 = min(box2_height / 2, box2_price - min_price2)\n",
    "\n",
    "max_height3 = min(box3_height / 2, max_price3 - box3_price)\n",
    "min_height3 = min(box3_height / 2, box3_price - min_price3)\n",
    "\n",
    "# Vertical lines for the first box\n",
    "ax.plot([box_start1, box_start1], [box1_price - min_height1, box1_price + max_height1], color='turquoise', linestyle='--', label='Asset Purchase')\n",
    "ax.plot([box_start1 + box_width1, box_start1 + box_width1], [box1_price - min_height1, box1_price + max_height1], color='grey', linestyle='--', label='Expiration Limit')\n",
    "\n",
    "# Horizontal lines for the first box\n",
    "ax.plot([box_start1, box_start1 + box_width1], [box1_price + max_height1, box1_price + max_height1], color='green', linestyle='--', label='Profit Taking Limit')\n",
    "ax.plot([box_start1, box_start1 + box_width1], [box1_price - min_height1, box1_price - min_height1], color='red', linestyle='--', label='Stop Loss Limit')\n",
    "\n",
    "# Vertical lines for the second box\n",
    "ax.plot([box_start2, box_start2], [box2_price - min_height2, box2_price + max_height2], color='turquoise', linestyle='--')\n",
    "ax.plot([box_start2 + box_width2, box_start2 + box_width2], [box2_price - min_height2, box2_price + max_height2], color='grey', linestyle='--')\n",
    "\n",
    "# Horizontal lines for the second box (using the same colors and legend label as the first box)\n",
    "ax.plot([box_start2, box_start2 + box_width2], [box2_price + max_height2, box2_price + max_height2], color='green', linestyle='--')\n",
    "ax.plot([box_start2, box_start2 + box_width2], [box2_price - min_height2, box2_price - min_height2], color='red', linestyle='--')\n",
    "\n",
    "# Vertical lines for the third box\n",
    "ax.plot([box_start3, box_start3], [box3_price - min_height3, box3_price + max_height3], color='turquoise', linestyle='--')\n",
    "ax.plot([box_start3 + box_width3, box_start3 + box_width3], [box3_price - min_height3, box3_price + max_height3], color='grey', linestyle='--')\n",
    "\n",
    "# Horizontal lines for the third box (using the same colors and legend label as the first box)\n",
    "ax.plot([box_start3, box_start3 + box_width3], [box3_price + max_height3, box3_price + max_height3], color='green', linestyle='--')\n",
    "ax.plot([box_start3, box_start3 + box_width3], [box3_price - min_height3, box3_price - min_height3], color='red', linestyle='--')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "fig.tight_layout(pad = 1.5)\n",
    "\n",
    "plt.savefig(path_default + '/visuals/TBM-Visualized.svg', format='svg') \n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annualized_return_log(df):\n",
    "    \"\"\"\n",
    "    Calculates the annualized log return for given assets.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of assets.\n",
    "    Returns:\n",
    "        float: annualized returns.\n",
    "    \"\"\"\n",
    "    df.loc[:, 'log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    total_log_return = df['log_return'].sum()\n",
    "    average_daily_log_return = total_log_return / len(df)\n",
    "    trading_days_per_year = 252\n",
    "    annualized_return = average_daily_log_return * trading_days_per_year\n",
    "    return annualized_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(path_data_raw + '/trading_daily_all.csv')\n",
    "\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "# Number of different stocks in our portfolio\n",
    "df_all['cusip'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum timestamp\n",
    "print(df_all['date'].min())\n",
    "print(df_all['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_all.groupby('cusip')['date']\n",
    "time_duration = grouped.max() - grouped.min()\n",
    "\n",
    "# Calculate the average time duration in days\n",
    "average_time = time_duration.dt.days.mean()\n",
    "print('AVG-Time', average_time)\n",
    "\n",
    "# Convert average time duration to years and months\n",
    "average_time_years = average_time // 365\n",
    "average_time_months = (average_time % 365) // 30\n",
    "\n",
    "print(average_time_years)\n",
    "print(average_time_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of entries per stock\n",
    "entries_per_stock = df_all.groupby('cusip').count()\n",
    "\n",
    "# Calculate the average number of entries per stock\n",
    "average_entries = entries_per_stock['date'].mean()\n",
    "average_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2 = df_all.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_all2.index.get_level_values(0).unique() # Get all unique CUSIP values\n",
    "\n",
    "volatilities = []\n",
    "\n",
    "for token in tqdm(tokens):\n",
    "    df_token = df_all2.loc[token] \n",
    "    \n",
    "    # Calculate the daily percentage change\n",
    "    df_token['daily_pct_change'] = df_token['close'].pct_change()\n",
    "\n",
    "    # Calculate the average daily volatility\n",
    "    average_volatility = df_token['daily_pct_change'].std()\n",
    "\n",
    "    if not np.isnan(average_volatility):\n",
    "        volatilities.append(average_volatility)\n",
    "\n",
    "print('Average for stock portfolio:', np.mean(volatilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison to the S&P 500\n",
    "sp500 = request_yfinance_data('^GSPC', '2004-04-01', '2023-05-01')\n",
    "russel3000 = request_yfinance_data('^RUA', '2004-04-01', '2023-05-01')\n",
    "\n",
    "sp500['daily_pct_change'] = sp500['Close'].pct_change()\n",
    "sp500_average_volatility = sp500['daily_pct_change'].std()\n",
    "russel3000['daily_pct_change'] = russel3000['Close'].pct_change()\n",
    "russel3000_average_volatility = russel3000['daily_pct_change'].std()\n",
    "print(sp500_average_volatility)\n",
    "print(russel3000_average_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MappingApi to get company information \n",
    "mappingApi = MappingApi(api_key=sec_api_key)\n",
    "\n",
    "random_indexes = df_all.sample(n=10)\n",
    "\n",
    "random_indexes = random_indexes.index\n",
    "\n",
    "for idx in random_indexes:\n",
    "    info = mappingApi.resolve('cusip', idx)\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Analysis Trading Daily Data\n",
    "df = df_all\n",
    "\n",
    "tokens = df.cusip.unique()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(df['cusip'])\n",
    "df = df.drop('cusip', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualized_returns = []\n",
    "\n",
    "for token in tqdm(tokens):\n",
    "    df_token = df.loc[token].copy()\n",
    "    if len(df_token) > 2 and isinstance(df_token, pd.DataFrame):\n",
    "        annualized_return = calc_annualized_return_log(df_token)\n",
    "        annualized_returns.append(annualized_return)        \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualized_returns_clean = annualized_returns[~np.isnan(annualized_returns)]\n",
    "annualized_returns_clean = annualized_returns_clean[np.isfinite(annualized_returns_clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "mean = np.mean(annualized_returns_clean)\n",
    "median = np.median(annualized_returns_clean)\n",
    "std = np.std(annualized_returns_clean)\n",
    "min_value = np.min(annualized_returns_clean)\n",
    "max_value = np.max(annualized_returns_clean)\n",
    "\n",
    "# Print the summary statistics\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std)\n",
    "print(\"Minimum Value:\", min_value)\n",
    "print(\"Maximum Value:\", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trades as determined by BB-Strategy\n",
    "data = load_ml_data(path_data_combined, '/v6_bb_w_vert_barrier_and_external_features.csv', vertical_barrier=False)\n",
    "\n",
    "# Processing\n",
    "# Drop columns as they have too many nan values\n",
    "data = data.drop(['feature_SP500_new_highs_change', 'feature_SP500_new_lows_change'], axis=1)\n",
    "\n",
    "exclude_cols = ['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Select columns to include by subtracting the exclude_cols from all columns\n",
    "include_cols = data.columns.difference(exclude_cols)\n",
    "\n",
    "# Drop rows with NaN values in include_cols\n",
    "data = data.dropna(subset=include_cols, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True)\n",
    "df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']] = df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']].fillna(0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profit_rel'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges\n",
    "ranges = [-float('inf'), -1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 1200]\n",
    "label_last = f\"> {int(ranges[-2] * 100)}%\"\n",
    "\n",
    "# Create a new column 'range_category' based on the ranges\n",
    "df['range_category'] = pd.cut(df['profit_rel'], ranges)\n",
    "\n",
    "# Count the number of entries in each range\n",
    "count_by_range = df['range_category'].value_counts().sort_index()\n",
    "\n",
    "# Define custom labels for x-axis\n",
    "labels = [f\"-90% to -100%\" if left == -1.0 else f\"< {int(right * 100)}%\" if left == -float('inf') else f\"{int(left * 100)}% to {int(right * 100)}%\" for left, right in zip(ranges[:-1], ranges[1:])]\n",
    "labels[-1] = label_last\n",
    "\n",
    "# Display the count for each range\n",
    "for i, count in enumerate(count_by_range):\n",
    "    print(f\"Range {i+1}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the count values and range categories\n",
    "counts = count_by_range.values\n",
    "range_categories = count_by_range.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Create a bar plot with adjusted alignment\n",
    "plt.bar(range(len(count_by_range)), count_by_range, align='center')  # Use 'align' parameter\n",
    "\n",
    "# Set x-axis tick labels, rotate them by 45 degrees, and adjust alignment\n",
    "plt.xticks(range(len(count_by_range)), labels, rotation=45, fontsize=18)  \n",
    "plt.yticks(fontsize=18)  # Updated fontsize to 14\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Relative Profit Range', fontsize=24)  \n",
    "plt.ylabel('Number of Trades', fontsize=24)  \n",
    "plt.title('Relative Profits Distribution', fontsize=24)  \n",
    "\n",
    "# Adjust the layout to prevent label overlapping if necessary\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate duration of trades\n",
    "df['duration'] = df['t1'] - df['date']\n",
    "\n",
    "# Calculate the average duration\n",
    "average_duration = df['duration'].mean()\n",
    "\n",
    "# Print the average duration\n",
    "print(f\"Average Duration: {average_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers\n",
    "z_scores = (df['profit_rel'] - df['profit_rel'].mean()) / df['profit_rel'].std()\n",
    "threshold = 3\n",
    "outliers = df[abs(z_scores) > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for outliers distinguised by profitable/unprofitable\n",
    "print(outliers[outliers['bin'] == 0]['profit_rel'].mean())\n",
    "print(len(outliers[outliers['bin'] == 0]))\n",
    "print('-'*50)\n",
    "print(outliers[outliers['bin'] == 1]['profit_rel'].mean())\n",
    "print(len(outliers[outliers['bin'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trades as determined by TBM-Strategy\n",
    "data = load_ml_data(path_data_combined, '/v1_tbm_and_external_features_with_fees.csv', vertical_barrier=True)\n",
    "\n",
    "# Pre-Processing\n",
    "# Drop columns as they have too many nan values\n",
    "data = data.drop(['feature_SP500_new_highs_change', 'feature_SP500_new_lows_change'], axis=1)\n",
    "\n",
    "exclude_cols = ['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Select columns to include by subtracting the exclude_cols from all columns\n",
    "include_cols = data.columns.difference(exclude_cols)\n",
    "\n",
    "# Drop rows with NaN values in include_cols\n",
    "data = data.dropna(subset=include_cols, axis=0)\n",
    "\n",
    "df = data.copy(deep=True)\n",
    "df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']] = df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many trades are short/long\n",
    "print(len(df[df['side'] == -1]))\n",
    "print(len(df[df['side'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges\n",
    "ranges = [-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 1200]\n",
    "label_last = f\"> {int(ranges[-2] * 100)}%\"\n",
    "\n",
    "# Create a new column 'range_category' based on the ranges\n",
    "df['range_category'] = pd.cut(df['profit_rel'], ranges)\n",
    "\n",
    "# Count the number of entries in each range for each side\n",
    "count_by_range = df.groupby(['range_category', 'side']).size().unstack(fill_value=0).sort_index()\n",
    "\n",
    "# Define custom labels for x-axis\n",
    "labels = [f\"{int(left * 100)}% to {int(right * 100)}%\" if left != -1 else f\"<={int(right * 100)}%\" for left, right in zip(ranges[:-1], ranges[1:])]\n",
    "labels[-1] = label_last\n",
    "\n",
    "# Extract the count values and range categories for each side\n",
    "counts_negative = count_by_range[-1].values\n",
    "counts_positive = count_by_range[1].values\n",
    "range_categories = count_by_range.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Create a bar plot for negative trades\n",
    "plt.bar(range(len(count_by_range)), counts_negative, label='Short Trades', color=\"lightblue\")\n",
    "\n",
    "# Create a bar plot for positive trades\n",
    "plt.bar(range(len(count_by_range)), counts_positive, label='Long Trades', color='darkblue', bottom=counts_negative)\n",
    "\n",
    "# Set x-axis tick labels and rotate them by 45 degrees\n",
    "plt.xticks(range(len(count_by_range)), labels, rotation=45, fontsize=17)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Relative Profit Range', fontsize=20)\n",
    "plt.ylabel('Number of Trades', fontsize=20)\n",
    "plt.title('Relative Profits Distribution Triple-Barrier Method & Bollinger Bands', fontsize=20)\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "# Adjust the layout to prevent label overlapping if necessary\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "plt.savefig(path_default + '/visuals/v1_TBM_trading_strat_distribution_rel_profits_by_side.svg', dpi='figure', format='svg')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realative profit over all trades\n",
    "print(np.mean(df['profit_rel']) * 100)\n",
    "\n",
    "print(df['profit_rel'].describe())\n",
    "\n",
    "df['duration'] = df['t1'] - df['date']\n",
    "\n",
    "# Calculate the average duration\n",
    "average_duration = df['duration'].mean()\n",
    "maximum_duration = df['duration'].max()\n",
    "minimum_duration = df['duration'].min()\n",
    "\n",
    "\n",
    "# Print the average duration\n",
    "print(f\"Average Duration: {average_duration}\")\n",
    "print(f\"Maximum Duration: {maximum_duration}\")\n",
    "print(f\"Minimum Duration: {minimum_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection absolute profits\n",
    "z_scores = np.abs((df['profit_abs'] - df['profit_abs'].mean()) / df['profit_abs'].std())\n",
    "threshold = 3\n",
    "outliers = df[z_scores > threshold]\n",
    "\n",
    "\n",
    "# Get statistics for outliers distinguised by profitable/unprofitable\n",
    "print(outliers[outliers['bin'] == 0]['profit_rel'].mean())\n",
    "print(len(outliers[outliers['bin'] == 0]))\n",
    "print('-'*50)\n",
    "print(outliers[outliers['bin'] == 1]['profit_rel'].mean())\n",
    "print(len(outliers[outliers['bin'] == 1]))\n",
    "\n",
    "print(np.mean(outliers['profit_rel']))\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_scaled_ml, X_scaled_ho, k = 0.95):\n",
    "    \"\"\"\n",
    "    Application of the Principal Component Analysis on the data (train and hold-out data).\n",
    "\n",
    "    Args:\n",
    "        X_scaled_ml (dataframe): Scaled training data.\n",
    "        X_scaled_ho (dataframe): Scaled hold-out data.\n",
    "        k (float, optional): Percentage of variance explained. Defaults to 0.95 (95%).\n",
    "    Returns:\n",
    "        Tuple: Tuple containing the dataframes for the scaled training and validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    pca_max = PCA(n_components = None, random_state = 42) # None will use all possible components\n",
    "    pca_max.fit(X_scaled_ml)\n",
    "    print('Variance explained by all components:', sum(pca_max.explained_variance_ratio_ *100))\n",
    "\n",
    "    # Visualize the components and their importance\n",
    "    plt.plot(np.cumsum(pca_max.explained_variance_ratio_ * 100))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.show()\n",
    "\n",
    "    pca = PCA(n_components = k, random_state = 42)\n",
    "    pca.fit(X_scaled_ml) # Fit on training data\n",
    "\n",
    "    # Transform the data based on PCA (fitted on trainig data)\n",
    "    X_ml_pca = pca.transform(X_scaled_ml)\n",
    "    X_ho_pca = pca.transform(X_scaled_ho)\n",
    "\n",
    "    # Create dataframe with the principal components\n",
    "    X_ml_pca_df = pd.DataFrame(X_ml_pca, columns=[f'PC{i+1}' for i in range(X_ml_pca.shape[1])])\n",
    "    X_ho_pca_df = pd.DataFrame(X_ho_pca, columns=[f'PC{i+1}' for i in range(X_ho_pca.shape[1])])\n",
    "\n",
    "    return X_ml_pca_df, X_ho_pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSV files (with OHLC data)\n",
    "csv_files = [f for f in os.listdir(path_data_trading_daily) if f.endswith('.csv')]\n",
    "csv_files = sorted(csv_files)\n",
    "dfs = []\n",
    "\n",
    "print('Reading and processing individual files ...')\n",
    "# Loop through the CSV file names and read each file into a DataFrame\n",
    "for file in tqdm(csv_files):\n",
    "    file_path = os.path.join(path_data_trading_daily, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Add CUSIP to be able to group them later\n",
    "    df.insert(0, 'cusip', str(file.split('.')[0]))\n",
    "\n",
    "    # Rename columns to more meaningful names\n",
    "    df = df.rename(columns = {\n",
    "        'prcod': 'open',\n",
    "        'prchd': 'high',\n",
    "        'prccd': 'close',\n",
    "        'prcld' : 'low',\n",
    "        'cshtrd' : 'volume',\n",
    "        'datadate': 'date'\n",
    "    })\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d') # Format dat\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs)\n",
    "\n",
    "combined_df.to_csv(path_data_raw + \"/v1_trading_combined.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined OHLC data\n",
    "combined_data = read_data_csv()\n",
    "\n",
    "combined_data = combined_data.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "tokens = combined_data.index.get_level_values(0).unique() # Get unique CUSIPs\n",
    "\n",
    "tokens = sorted(tokens)\n",
    "\n",
    "dfs_features = []\n",
    "i = 0 # Counter used for buffering\n",
    "\n",
    "for token in tqdm(tokens):\n",
    "    # Get sub-dataframe with all entries for given token (CUSIP)\n",
    "    df_token = combined_data.loc[token] \n",
    "\n",
    "    # Add technical features\n",
    "    df_token = add_certain_ta_lib_features(df_token)\n",
    "\n",
    "    # Add cusip and date again\n",
    "    df_token['cusip'] = token\n",
    "    df_token['date'] = pd.to_datetime(df_token.index)\n",
    "\n",
    "    # Adding the finished assets\n",
    "    dfs_features.append(df_token)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # Buffering\n",
    "    if i % 500 == 0:\n",
    "        combined_data_features = pd.concat(dfs_features)\n",
    "        combined_data_features = combined_data_features.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "        # Add CUSIP and date again\n",
    "        combined_data_features.insert(0, 'date', pd.to_datetime(combined_data_features.index.get_level_values(1).values))\n",
    "        combined_data_features.insert(0, 'cusip', combined_data_features.index.get_level_values(0))\n",
    "\n",
    "        combined_data_features.to_csv(path_data_features + \"/v1_features_part\" + str(i) + '.csv', index = False)\n",
    "\n",
    "        dfs_features = []\n",
    "\n",
    "# Save last entries\n",
    "combined_data_features = pd.concat(dfs_features)\n",
    "combined_data_features = combined_data_features.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "# Add CUSIP and date again\n",
    "combined_data_features.insert(0, 'date', pd.to_datetime(combined_data_features.index.get_level_values(1).values))\n",
    "combined_data_features.insert(0, 'cusip', combined_data_features.index.get_level_values(0))\n",
    "\n",
    "combined_data_features.to_csv(path_data_features + \"/v1_features_part_last.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all buffered feature files\n",
    "csv_files_features = [f for f in os.listdir(path_data_features) if f.endswith('.csv')]\n",
    "dfs_features = []\n",
    "\n",
    "# Read files and concatenate\n",
    "for file in tqdm(csv_files_features):\n",
    "    file_path = os.path.join(path_data_features, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs_features.append(df)\n",
    "\n",
    "combined_dfs_features = pd.concat(dfs_features)\n",
    "\n",
    "# Store all trades including their features into one file\n",
    "combined_dfs_features.to_csv(path_data_features + \"/v1_features_concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OHLC data for all assets\n",
    "df_daily_trades = read_data_raw_csv(full_path = path_data_raw + '/trading_daily_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concatenated features for the BB-Strategy\n",
    "df_combined = pd.read_csv(path_data_features + \"/v1_features_concatenated.csv\")\n",
    "\n",
    "df_combined['date'] = pd.to_datetime(df_combined['date'].dt.strftime('%Y-%m-%d')) # format date to be set as index later\n",
    "\n",
    "df_combined = df_combined.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "# Apply Trading Strategy Long-Only Bollinger Bands\n",
    "tokens = df_combined.index.get_level_values(0).unique()\n",
    "\n",
    "dfs_processed_bb = []\n",
    "\n",
    "for token in tqdm(tokens):\n",
    "    df_token = df_combined.loc[token]\n",
    "    \n",
    "    # Drop hurst as it has been identifed as irrelevant\n",
    "    df_token = df_token.drop('feature_hurst', axis = 1)      \n",
    "\n",
    "    df_daily_trades_token = df_daily_trades[df_daily_trades['cusip'] == str(token)]\n",
    "\n",
    "    if len(df_daily_trades_token) == 0:\n",
    "        print('ERROR: Could not find df_trading_daily for token.')\n",
    "\n",
    "    df_processed = apply_bollinger_bands_v2(df_daily_trades_token, df_token)\n",
    "\n",
    "    if len(df_processed) > 0:\n",
    "            # Set CUSIP and date again\n",
    "            df_processed.loc[:, 'cusip'] = str(token)\n",
    "            df_processed.loc[:, 'date'] = pd.to_datetime(df_processed.index.values)\n",
    "            dfs_processed_bb.append(df_processed)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Save last entries\n",
    "    data_f = pd.concat(dfs_processed_bb)\n",
    "    data_f = data_f.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "    data_f = data_f.dropna()\n",
    "\n",
    "    # Set CUSIP and date again\n",
    "    data_f['cusip'] = data_f.index.get_level_values(0)\n",
    "    data_f['date'] = pd.to_datetime(data_f.index.get_level_values(1).values)\n",
    "    \n",
    "    data_f.to_csv(path_data_strategy + '/v3_w_vert_barrier_applied_bb_.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_features = [f for f in os.listdir(path_data_strategy) if f.startswith('v3_w_vert_barrier')]\n",
    "\n",
    "dfs_features = []\n",
    "\n",
    "for file in tqdm(csv_files_features):\n",
    "    file = file.lstrip('._') # Resolve file system issue\n",
    "    file_path = os.path.join(path_data_features, file)\n",
    "    df = load_ml_data(path_data_strategy, file, vertical_barrier=False)\n",
    "    dfs_features.append(df)\n",
    "\n",
    "# Combine all individual files with trades as determined by strategy\n",
    "combined_dfs_features = pd.concat(dfs_features)\n",
    "\n",
    "combined_dfs_features.to_csv(path_data_strategy + \"/v6_BB_w_vert_barrier_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Trading Strategy TBM\n",
    "combined_dfs_features = pd.read_csv(path_data_features + \"/v1_features_concatenated.csv\")\n",
    "\n",
    "combined_dfs_features = combined_dfs_features.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "\n",
    "tokens = combined_dfs_features.index.get_level_values(0).unique() # Get unique CUSIPs\n",
    "\n",
    "tokens = sorted(tokens)\n",
    "\n",
    "index = 0 # Counter for Buffering\n",
    "\n",
    "dfs_processed_tbm = []\n",
    "for token in tqdm(tokens):\n",
    "    df_token = combined_dfs_features.loc[token]\n",
    "\n",
    "    # Drop hurst as it has been identifed as irrelevant\n",
    "    df_token = df_token.drop('feature_hurst', axis=1)\n",
    "\n",
    "    df_processed = apply_tbm(df_token, 0.0, 10, 2, 2) # Vertical-barrier: 10 days, pt=2, sl=2\n",
    "\n",
    "    # Set CUSIP and date\n",
    "    df_processed['cusip'] = token\n",
    "    df_processed['date'] = pd.to_datetime(df_processed.index.values)\n",
    "\n",
    "    dfs_processed_tbm.append(df_processed)\n",
    "    index += 1\n",
    "\n",
    "    if index % 500 == 0: # Buffering\n",
    "        data_f = pd.concat(dfs_processed_tbm)\n",
    "        data_f = data_f.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "        data_f = data_f.dropna()\n",
    "\n",
    "        # Set CUSIP and date\n",
    "        data_f['cusip'] = data_f.index.get_level_values(0)\n",
    "        data_f['date'] = pd.to_datetime(data_f.index.get_level_values(1).values)\n",
    "\n",
    "        data_f.to_csv(path_data_strategy + \"/v1_applied_tbm_part\" + str(index) + '.csv', index = False)\n",
    "        dfs_processed_tbm = []\n",
    "\n",
    "# Saving last entries \n",
    "data_f = pd.concat(dfs_processed_tbm)\n",
    "data_f = data_f.set_index(['cusip', 'date']).sort_values(['cusip', 'date'])\n",
    "data_f = data_f.dropna()\n",
    "\n",
    "# Set CUSIP and date\n",
    "data_f['cusip'] = data_f.index.get_level_values(0)\n",
    "data_f['date'] = pd.to_datetime(data_f.index.get_level_values(1).values)\n",
    "\n",
    "data_f.to_csv(path_data_strategy + '/v1_applied_tbm_part_last.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all individual files\n",
    "csv_files_tbm = [f for f in os.listdir(path_data_strategy) if f.endswith('.csv')]\n",
    "dfs_tbm = []\n",
    "\n",
    "for file in tqdm(csv_files_tbm):\n",
    "    file_path = os.path.join(path_data_strategy, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs_tbm.append(df)\n",
    "\n",
    "data_tbm_final = pd.concat(dfs_tbm)\n",
    "\n",
    "data_tbm_final.to_csv(path_data_strategy + '/v1_applied_tbm_full.csv', index=False) # saving to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data (FRED & yFinance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set end date for external data is now\n",
    "end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "start_date = '2000-01-01'\n",
    "\n",
    "# Get FRED and yFinance data\n",
    "data_fred = get_fred_data(start_date, end_date)\n",
    "data_yfinance = get_yfinance_data(start_date, end_date)\n",
    "\n",
    "# Concat both data\n",
    "data_apis = pd.concat([data_fred, data_yfinance], axis=1)\n",
    "\n",
    "# Add date column for reference later\n",
    "data_apis.insert(0, 'date', data_apis.index) \n",
    "\n",
    "# Drop all nans besides BTC and CryptoMarketCap as those have many (data starts in 2017)\n",
    "data_apis.dropna(subset=data_apis.columns.difference(['feature_BTC', 'feature_CryptoMarketCap']), inplace = True) \n",
    "\n",
    "# Get all feature columns\n",
    "feature_columns = [col for col in data_apis.columns if col.startswith('feature_')]\n",
    "\n",
    "# Comput percentile and change for all feature columns\n",
    "for feature in tqdm(feature_columns):\n",
    "    data = compute_percentile_and_change(data_apis, feature)\n",
    "\n",
    "# Drop all nans besides BTC and CryptoMarketCap as those have many (data starts in 2017)\n",
    "data.dropna(subset=data_apis.columns.difference(['feature_BTC', 'feature_CryptoMarketCap']), inplace=True) \n",
    "\n",
    "# Store external data to csv\n",
    "data.to_csv(path_data_external + '/fred_yfinance_percentiles_and_changes.csv', index=False) # saving to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Trading and External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading trades and external data\n",
    "df_trading = load_ml_data(path_data_strategy, 'bb/v6_BB_w_vert_barrier_all.csv', vertical_barrier=False) # same for tbm-strat\n",
    "df_external = pd.read_csv(path_data_external + '/fred_yfinance_data_percentiles_and_changes.csv')\n",
    "\n",
    "# Set index for both datasets\n",
    "df_trading = df_trading.set_index(pd.to_datetime(df_trading['date']))\n",
    "df_external = df_external.set_index(pd.to_datetime(df_external['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trading = df_trading.drop('date', axis=1)\n",
    "df_external = df_external.drop('date', axis=1)\n",
    "df_trading['cusip'] = df_trading['cusip'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge trades with external data based on date\n",
    "joined_df = df_trading.merge(df_external, left_index=True, right_index=True)\n",
    "\n",
    "joined_df['date'] = pd.to_datetime(joined_df.index)\n",
    "\n",
    "joined_df.to_csv(path_data_combined + '/v6_bb_w_vert_barrier_and_external_features.csv', index=False) # saving to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (here for BB-Stratey)\n",
    "# For TBM-Strategy it is done in the same way\n",
    "data = load_ml_data(path_data_combined, '/v6_bb_w_vert_barrier_and_external_features.csv', vertical_barrier=False)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = data.isna().sum()\n",
    "nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns as they have too many nan values\n",
    "data = data.drop(['feature_SP500_new_highs_change', 'feature_SP500_new_lows_change'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Select columns to include by subtracting the exclude_cols from all columns\n",
    "include_cols = data.columns.difference(exclude_cols)\n",
    "\n",
    "# Drop rows with NaN values in include_cols\n",
    "data = data.dropna(subset=include_cols, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True) # Use the data from the nan pre-processing step\n",
    "\n",
    "# Fill NaN values in Crypto data with 0\n",
    "df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']] = df[['feature_BTC', 'feature_CryptoMarketCap', 'feature_CryptoMarketCap_rolling_percentile', 'feature_BTC_rolling_percentile', 'feature_CryptoMarketCap_change', 'feature_BTC_change']].fillna(0)\n",
    "\n",
    "# Set holdout date used for out-of-sample dataset\n",
    "hold_out_date = pd.to_datetime('2020-01-01')\n",
    "\n",
    "# Set for training, validation and testing\n",
    "df_selected_ml = df_selected[df_selected['date'] <= pd.to_datetime(hold_out_date)]\n",
    "df_selected_ho = df_selected[df_selected['date'] > pd.to_datetime(hold_out_date)]\n",
    "\n",
    "# Reset Indexes\n",
    "df_selected_ml = df_selected_ml.reset_index(drop=True)\n",
    "df_selected_ho = df_selected_ho.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "\n",
    "# Scalers\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "\n",
    "# Feature Columns\n",
    "subset_cols = df_selected.filter(like='feature_').columns\n",
    "\n",
    "# Get Feature Columns Subset\n",
    "df_ml_features = df_selected_ml[subset_cols]\n",
    "df_ho_features = df_selected_ho[subset_cols]\n",
    "\n",
    "# Standard Scaler\n",
    "df_ml_features_ss = pd.DataFrame(scaler_standard.fit_transform(df_ml_features), columns=df_ml_features.columns)\n",
    "df_ho_features_ss = pd.DataFrame(scaler_standard.transform(df_ho_features), columns=df_ho_features.columns)\n",
    "\n",
    "# Min-Max Scaler\n",
    "df_ml_features_mms = pd.DataFrame(scaler_minmax.fit_transform(df_ml_features), columns=df_ml_features.columns)\n",
    "df_ho_features_mms = pd.DataFrame(scaler_minmax.transform(df_ho_features), columns=df_ho_features.columns)\n",
    "\n",
    "# Robust Scaler\n",
    "df_ml_features_rob = pd.DataFrame(scaler_robust.fit_transform(df_ml_features), columns=df_ml_features.columns)\n",
    "df_ho_features_rob = pd.DataFrame(scaler_robust.transform(df_ho_features), columns=df_ho_features.columns)\n",
    "\n",
    "# Apply PCA\n",
    "df_ml_features_pca_ss, df_ho_features_pca_ss = pca(df_ml_features_ss, df_ho_features_ss)\n",
    "df_ml_features_pca_mms, df_ho_features_pca_mms = pca(df_ml_features_mms, df_ho_features_mms)\n",
    "df_ml_features_pca_rob, df_ho_features_pca_rob = pca(df_ml_features_rob, df_ho_features_rob)\n",
    "\n",
    "print('#Training', len(df_selected_ml))\n",
    "print('#Testing', len(df_selected_ho))\n",
    "print('#Features', len(df_selected_ml.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all of the subsets of the data so we have consistency in our model training\n",
    "df_selected_ml.to_csv(path_data_partitioned_bb_with_fees_train + '/v1_df_ml.csv', index = False)\n",
    "df_selected_ho.to_csv(path_data_partitioned_bb_with_fees_test + '/v1_df_ho.csv', index = False)\n",
    "\n",
    "# Standard\n",
    "df_ml_features_ss.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv', index = False)\n",
    "df_ho_features_ss.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ho_features_scaled_ss.csv', index = False)\n",
    "\n",
    "# MinMax\n",
    "df_ml_features_mms.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ml_features_scaled_mms.csv', index = False)\n",
    "df_ho_features_mms.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ho_features_scaled_mms.csv', index = False)\n",
    "\n",
    "# Robust\n",
    "df_ml_features_rob.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ml_features_scaled_rob.csv', index = False)\n",
    "df_ho_features_rob.to_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ho_features_scaled_rob.csv', index = False)\n",
    "\n",
    "# PCA\n",
    "df_ml_features_pca_ss.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ml_features_pca_ss.csv', index = False)\n",
    "df_ho_features_pca_ss.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ho_features_pca_ss.csv', index = False)\n",
    "df_ml_features_pca_mms.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ml_features_pca_mms.csv', index = False)\n",
    "df_ho_features_pca_mms.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ho_features_pca_mms.csv', index = False)\n",
    "df_ml_features_pca_rob.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ml_features_pca_rob.csv', index = False)\n",
    "df_ho_features_pca_rob.to_csv(path_data_partitioned_bb_with_fees_pca + '/v1_df_ho_features_pca_rob.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the partitioning worked as expected\n",
    "df_1 = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier=False)\n",
    "df_2 = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier=False)\n",
    "\n",
    "duplicate_rows = df_1[df_1.duplicated(subset=['cusip', 'date'], keep=False)]\n",
    "print(len(duplicate_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate the objective function for hyperparameter optimization using Optuna library.\n",
    "    \n",
    "    Args:\n",
    "        trial (instance of Optuna trial class): An instance of the Optuna Trial class.\n",
    "    \n",
    "    Returns:\n",
    "        Float: The mean log loss score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the training data\n",
    "    df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier=False)\n",
    "    \n",
    "    # Split the features and target variables\n",
    "    X = df_train.filter(like='feature_')\n",
    "    y = df_train['bin']\n",
    "    \n",
    "    # Define the cross-validation strategy\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Array to store cross-validation scores\n",
    "    cv_scores = np.empty(3)\n",
    "    \n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17]),\n",
    "        'random_state': 42,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        'nthread': 8\n",
    "    }\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train the XGBoost model\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "        \n",
    "        # Make predictions on the validation set\n",
    "        preds = model.predict(X_val)\n",
    "        \n",
    "        # Calculate the log loss error\n",
    "        error = log_loss(y_val, preds)\n",
    "        cv_scores[idx] = error\n",
    "        \n",
    "    # Return the mean of the log loss\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study with the objective to minimize the log loss\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"XGBoost BB-Strategy\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)\n",
    "\n",
    "# Print best hyperparameter combination\n",
    "print(f\"\\tBest value (precision): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate the objective function for hyperparameter optimization using Optuna library.\n",
    "    \n",
    "    Args:\n",
    "        trial (instance of Optuna trial class): An instance of the Optuna Trial class.\n",
    "    \n",
    "    Returns:\n",
    "        Float: The mean log loss score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the ML data\n",
    "    df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier=False)\n",
    "    X = df_train.filter(like='feature_')\n",
    "    y = df_train['bin']\n",
    "    \n",
    "    # Create cross-validation instance\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores = np.empty(3)\n",
    "\n",
    "    # Set the parameters for LGBM model\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precision\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_threads\": 8,\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 20),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 255),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "    } \n",
    "        \n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        # Load the train and validation data\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create LGBM datasets\n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        # Define pruning callback for early stopping\n",
    "        pruning_callback = LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "\n",
    "        # Train the LGBM model\n",
    "        gbm = lgb.train(param, dtrain, valid_sets=[dvalid], callbacks=[pruning_callback])\n",
    "        \n",
    "        # Make predictions and calculate precision score\n",
    "        preds = gbm.predict(X_val)\n",
    "        pred_labels = np.rint(preds)\n",
    "\n",
    "        # Calculate and store the error\n",
    "        error = log_loss(pred_labels, preds)\n",
    "        cv_scores[idx] = error\n",
    "\n",
    "    # Return the mean precision score\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study with the objective to maximize the precision\n",
    "study = optuna.create_study(direction=\"maximize\", study_name='LGBM BB-Strategy')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "# Print hyperparameter optimization results\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for PyTorch to ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## Load the data\n",
    "train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "train_features = pd.read_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv') # Features\n",
    "train_target = train['bin'] # Targets\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.0001\n",
    "bs = 256\n",
    "epochs = 250\n",
    "\n",
    "# Create path details\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "execution_path = path_data_results_bb_nn + '/' + timestamp\n",
    "\n",
    "try:\n",
    "    os.mkdir(execution_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Could not create path')\n",
    "\n",
    "# Define config that is later stored\n",
    "config = {\n",
    "    'Date' : str(timestamp),\n",
    "    'train_set': str(path_data_partitioned_bb_with_fees_train),\n",
    "    'train_features': 'features-scaled-ss',\n",
    "    'test_set': str(path_data_partitioned_bb_with_fees_test),\n",
    "    'learning_rate': str(learning_rate),\n",
    "    'batch_size': str(bs),\n",
    "    'epochs': str(epochs),\n",
    "}\n",
    "\n",
    "# Create loss function, model, and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "model = CustomMLP_BB(input_shape=train_features.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "structure_dict = get_structure_dict(model)\n",
    "\n",
    "config['network_structure'] = structure_dict\n",
    "config['loss'] = str(loss_fn)\n",
    "config['optimizer'] = str(optimizer)\n",
    "\n",
    "save_config(execution_path, config) # Save config to filesystem\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, train_target, test_size=0.2, random_state=4\n",
    ")\n",
    "print('#Training Examples', len(X_train))\n",
    "print('#Validation Examples', len(X_val))\n",
    "\n",
    "wandb.init(\n",
    "    project = 'master-thesis-nn',\n",
    "    entity= 'lars-s7',\n",
    "    name=f\"Neural_Network_Execution-Path_{execution_path}\", \n",
    "    config = config,\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "train_dataset = CustomStockDataSet(X_train, y_train)\n",
    "val_dataset = CustomStockDataSet(X_val, y_val)\n",
    "\n",
    "# Data Loader\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Train neural network using pre-defined method\n",
    "_, _ = train_model(model, train_dl, val_dl, epochs, optimizer, loss_fn, 0, execution_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate the objective function for hyperparameter optimization using Optuna library.\n",
    "    \n",
    "    Args:\n",
    "        trial (instance of Optuna trial class): An instance of the Optuna Trial class.\n",
    "    \n",
    "    Returns:\n",
    "        Float: The mean log loss score.\n",
    "    \"\"\"\n",
    "    # Load the ML data\n",
    "    df_train = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "    X = df_train.filter(like='feature_') # Features\n",
    "    y = df_train['bin'] # Targets\n",
    "    \n",
    "    # Create k-fold object\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores_loss = np.empty(3)\n",
    "\n",
    "    # Define hyperparameter space\n",
    "    params = {\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17]),\n",
    "        'random_state': 42,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma' : trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        'grow_policy' : trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        'nthread': 8\n",
    "    }\n",
    "        \n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create model and train it\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "        preds = model.predict(X_val)\n",
    "        \n",
    "        error = log_loss(y_val, preds)\n",
    "        cv_scores_loss[idx] = error\n",
    "\n",
    "    # Return the mean log loss   \n",
    "    return np.mean(cv_scores_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study that has the objective of minimizing the log loss\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"XGBoost TBM-Strategy\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)\n",
    "\n",
    "print(f\"\\tBest value (precision): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate the objective function for hyperparameter optimization using Optuna library.\n",
    "    \n",
    "    Args:\n",
    "        trial (instance of Optuna trial class): An instance of the Optuna Trial class.\n",
    "    \n",
    "    Returns:\n",
    "        Float: The mean log loss score.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df_train = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "    df_train_features = df_train.filter(like='feature_') # Features\n",
    "    df_train_target = df_train['bin'] # Targets\n",
    "\n",
    "    X = df_train_features\n",
    "    y = df_train_target\n",
    "    \n",
    "    # Create k-fold object\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_scores_loss = np.empty(3)\n",
    "    \n",
    "    # Define hyperparameter space\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_threads\": 4,\n",
    "        \n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 20),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 255),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True)\n",
    "    } \n",
    "    \n",
    "    config = dict(trial.params)\n",
    "    config['trial_number'] = trial.number\n",
    "    \n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        # Get data for fold\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        pruning_callback = LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "        gbm = lgb.train(params, dtrain, valid_sets=[dvalid], callbacks=[pruning_callback])\n",
    "        \n",
    "        preds = gbm.predict(X_val)\n",
    "        pred_labels = np.rint(preds)\n",
    "        \n",
    "        loss = log_loss(y_val, pred_labels)        \n",
    "        cv_scores_loss[idx] = loss\n",
    "    return np.mean(cv_scores_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study that minimized the log loss\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM TBM-Strategy\")\n",
    "    \n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "print(f\"\\tBest params:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "train = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "train_features = pd.read_csv(path_data_partitioned_tbm_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv') # Features\n",
    "train_target = train['bin'] # Targets\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 350\n",
    "batch_size = 256\n",
    "\n",
    "# Create execution path\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "execution_path = path_data_results_tbm_nn + '/' + timestamp\n",
    "\n",
    "try:\n",
    "    os.mkdir(execution_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Could not create path')\n",
    "\n",
    "config = {\n",
    "    'Date' : str(timestamp),\n",
    "    'train_set': 'Standard Scaler',\n",
    "    'test_set': 'Standard Scaler',\n",
    "    'learning_rate': str(learning_rate),\n",
    "    'epochs': str(epochs),\n",
    "}\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "model = CustomMLP(input_shape=train_features.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "structure_dict = get_structure_dict(model)\n",
    "\n",
    "config['network_structure'] = structure_dict\n",
    "config['loss'] = str(loss_fn)\n",
    "config['optimizer'] = str(optimizer)\n",
    "\n",
    "save_config(execution_path, config)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, train_target, test_size=0.2, random_state=4\n",
    ")\n",
    "\n",
    "print('#Training Examples', len(X_train))\n",
    "print('#Validation Examples', len(X_val))\n",
    "\n",
    "wandb.init(\n",
    "    project = 'master-thesis-nn',\n",
    "    entity= 'lars-s7',\n",
    "    name=f\"TBM_Neural_Network_Execution-Path_{execution_path}\", \n",
    "    config = config,\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "train_dataset = CustomStockDataSet(X_train, y_train)\n",
    "val_dataset = CustomStockDataSet(X_val, y_val)\n",
    "\n",
    "# Data Loader\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train neural network\n",
    "_, _ = train_model(model, train_dl, val_dl, epochs, optimizer, loss_fn, 0, execution_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "df_train_features = df_train.filter(like='feature_')\n",
    "df_train_target = df_train['bin']\n",
    "\n",
    "# Test\n",
    "df_test = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier = False)\n",
    "df_test_features = df_test.filter(like='feature_')\n",
    "df_test_target = df_test['bin']\n",
    "\n",
    "# Config for LGBM (based on Optuna optimization)\n",
    "params_lgbm = {\n",
    "    \"num_threads\": 8,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"lambda_l1\": 2.8269687100646643,\n",
    "    \"lambda_l2\": 0.1132729746179371,\n",
    "    \"num_leaves\": 222,\n",
    "    \"feature_fraction\": 0.6029600980261194,\n",
    "    \"bagging_fraction\": 0.9924346775670325,\n",
    "    \"bagging_freq\": 2,\n",
    "    \"min_child_samples\": 65,\n",
    "    \"max_depth\": 14,\n",
    "    \"max_bin\": 136,\n",
    "    \"learning_rate\": 0.30197097330510875\n",
    "}\n",
    "\n",
    "# Initialize LGBM and set params\n",
    "clf_lgbm = lgb.LGBMClassifier(seed=42)\n",
    "clf_lgbm.set_params(**params_lgbm)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf_lgbm,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"precision\",\n",
    "    min_features_to_select=1,\n",
    ")\n",
    "\n",
    "rfecv.fit(df_train_features, df_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature importances from the underlying decision tree model\n",
    "feature_importances = rfecv.estimator_.feature_importances_\n",
    "\n",
    "# Create a list of feature-importance pairs\n",
    "importance_pairs = list(zip(df_train_features.columns, feature_importances))\n",
    "\n",
    "# Sort the list based on importance values in descending order\n",
    "importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importances in descending order\n",
    "for feature, importance in importance_pairs:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "    \n",
    "sorted_features1, sorted_importances1 = zip(*importance_pairs[:10])\n",
    "sorted_features2, sorted_importances2 = zip(*importance_pairs[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances and save them to the filesystem\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(sorted_features1, sorted_importances1)\n",
    "plt.xlabel('Top 10 Features', fontsize=15)\n",
    "plt.ylabel('Feature Importance', fontsize=15)\n",
    "plt.title('BB-Strategy Feature Importances', fontsize=15)\n",
    "plt.xticks(rotation=90, fontsize=15)\n",
    "plt.yticks(fontsize=15) \n",
    "plt.tight_layout()\n",
    "plt.savefig(path_default + '/visuals/v1_unscaled_with_fees_RFE_feature_importance_top_20_part1.svg', format='svg', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buy & Hold Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trading_daily = read_data_raw_csv() # Get initial raw data\n",
    "\n",
    "df_bb = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier=False) # Get all BB trades in out-of-sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusips = df_bb['cusip'].unique().tolist() # Unique CUSIP identifiers\n",
    "print('# Unique CUSIPs', len(cusips))\n",
    "\n",
    "i = 0 # Counter for buffering\n",
    "rows = [] # Store buy-and-hold performance for each CUSIP\n",
    "\n",
    "for cusip in tqdm(cusips):\n",
    "    # Get first buying entry of BB-Strategy\n",
    "    buying_entry = df_bb[df_bb['cusip'] == cusip].nsmallest(1, 'date')\n",
    "\n",
    "    # Get latest available data point for given CUSIP\n",
    "    selling_entry = df_trading_daily[df_trading_daily['cusip'] == cusip].nlargest(1, 'date')\n",
    "    \n",
    "    # Check if buying entry and selling entry could be obtained\n",
    "    if buying_entry is not None and selling_entry is not None:\n",
    "        buying_price = buying_entry['close'].iloc[0] # Buy price when first BB-Strategy trade was initiated\n",
    "        selling_price = selling_entry['close'].iloc[0] # Selling price is last available data point\n",
    "        relative_profit = (selling_price - buying_price) / buying_price\n",
    "\n",
    "        # Create entry for trade\n",
    "        entry = {\n",
    "            'cusip': str(cusip),\n",
    "            'side': 1,\n",
    "            'date': buying_entry['date'].iloc[0],\n",
    "            't1': selling_entry['date'].iloc[0],\n",
    "            'close': buying_price,\n",
    "            't1_price': selling_price,\n",
    "            'profit_rel': relative_profit,\n",
    "            'bin': 1 if relative_profit > 0 else 0\n",
    "        }\n",
    "        rows.append(entry)\n",
    "    else:\n",
    "        print('Buying-Date or Selling-Date not found!')\n",
    "    i += 1\n",
    "    \n",
    "    # Buffering\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        buy_and_hold_data = pd.DataFrame(rows)\n",
    "        buy_and_hold_data.to_csv(path_data_buy_hold + '/v1_BB_buy_and_hold_' + str(i) + '.csv', index = False)\n",
    "        rows = []\n",
    "buy_and_hold_data = pd.DataFrame(rows)\n",
    "buy_and_hold_data.to_csv(path_data_buy_hold + '/v1_BB_buy_and_hold_' + str(i) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual files that were created because of buffering\n",
    "csv_files_features = [f for f in os.listdir(path_data_buy_hold) if f.startswith('v1_BB_')]\n",
    "dfs_features = []\n",
    "\n",
    "for file in tqdm(csv_files_features):\n",
    "    file_path = os.path.join(path_data_features, file)\n",
    "    df = load_ml_data(path_data_buy_hold, file, vertical_barrier=False)\n",
    "    dfs_features.append(df)\n",
    "\n",
    "combined_buy_n_hold = pd.concat(dfs_features)\n",
    "\n",
    "# Store single dataframe with all buy-and-hold performances\n",
    "combined_buy_n_hold.to_csv(path_data_buy_hold + \"/v1_BB_buy_and_hold_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trading_daily = read_data_raw_csv() # Get raw asset data\n",
    "\n",
    "df_tbm = load_ml_data(path_data_partitioned_tbm_with_fees_test, 'v1_df_ho.csv', vertical_barrier=False) # Get trades as determined by TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cusips = df_tbm['cusip'].unique().tolist() # Get unique CUSIPs\n",
    "print('# Unique CUSIPs', len(cusips))\n",
    "\n",
    "i = 0 # Counter for buffering\n",
    "rows = []\n",
    "\n",
    "for cusip in tqdm(cusips):\n",
    "    # Earliest trade entry as determined by the TBM-Strategy\n",
    "    buying_entry = df_tbm[df_tbm['cusip'] == cusip].nsmallest(1, 'date')\n",
    "    # Latest data point available\n",
    "    selling_entry = df_trading_daily[df_trading_daily['cusip'] == cusip].nlargest(1, 'date')\n",
    "    \n",
    "    if buying_entry is not None and selling_entry is not None:\n",
    "        buying_price = buying_entry['close'].iloc[0]\n",
    "        selling_price = selling_entry['close'].iloc[0]\n",
    "        relative_profit = (selling_price - buying_price) / buying_price\n",
    "\n",
    "        # Create buy-and-hold entry\n",
    "        entry = {\n",
    "            'cusip': str(cusip),\n",
    "            'side': 1,\n",
    "            'date': buying_entry['date'].iloc[0],\n",
    "            't1': selling_entry['date'].iloc[0],\n",
    "            'close': buying_price,\n",
    "            't1_price': selling_price,\n",
    "            'profit_rel': relative_profit,\n",
    "            'bin': 1 if relative_profit > 0 else 0\n",
    "        }\n",
    "\n",
    "        rows.append(entry)\n",
    "    else:\n",
    "        print('Buying-Date or Selling-Date not found!')\n",
    "    i += 1\n",
    "    \n",
    "    # Buffering\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        buy_and_hold_data = pd.DataFrame(rows)\n",
    "        buy_and_hold_data.to_csv(path_data_buy_hold + '/v1_TBM_buy_and_hold_' + str(i) + '.csv', index = False)\n",
    "        rows = []\n",
    "        \n",
    "buy_and_hold_data = pd.DataFrame(rows)\n",
    "buy_and_hold_data.to_csv(path_data_buy_hold + '/v1_TBM_buy_and_hold_' + str(i) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual files due to buffering\n",
    "csv_files_features = [f for f in os.listdir(path_data_buy_hold) if f.startswith('v1_TBM_')]\n",
    "dfs_features = []\n",
    "\n",
    "for file in tqdm(csv_files_features):\n",
    "    file_path = os.path.join(path_data_features, file)\n",
    "    df = load_ml_data(path_data_buy_hold, file, vertical_barrier=False)\n",
    "    dfs_features.append(df)\n",
    "\n",
    "combined_buy_n_hold = pd.concat(dfs_features)\n",
    "\n",
    "# Save combined file\n",
    "combined_buy_n_hold.to_csv(path_data_buy_hold + \"/v1_TBM_buy_and_hold_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check (10 Assets) BB-Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following a sanity check for 10 assets is conducted. Hereby, 10 stocks are chosen and evaluated. Thus, Meta-Labeling is tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 Stock from the data\n",
    "# Read Data\n",
    "########## Datasets ##########\n",
    "# Train\n",
    "df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "df_train_features = df_train.filter(like='feature_')\n",
    "df_train_target = df_train['bin']\n",
    "\n",
    "# Test\n",
    "df_test = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier = False)\n",
    "df_test_features = df_test.filter(like='feature_')\n",
    "df_test_target = df_test['bin']\n",
    "########## Datasets ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_loaded = load_model_sklearn(path_data_final_models, 'BB_LGBM') # Load LGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mdl_loaded.predict(df_test_features) # predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_full_evaluation_w_prediction(pred, df_test, df_test_features, df_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sample(n=10).cusip # Get 10 random CUSIPs\n",
    "\n",
    "# The CUSIPs that were output by the previous line of code\n",
    "cusips_sanity = [\n",
    "    '67092P102',\n",
    "    '683797104',\n",
    "    '70399K933',\n",
    "    '72201R304',\n",
    "    '72201R775',\n",
    "    '767292105',\n",
    "    '78440X507',\n",
    "    '829214105',\n",
    "    '90187B606',\n",
    "    '90214Q766'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanity_trades = df_test[df_test['cusip'].isin(cusips_sanity)]\n",
    "\n",
    "df_sanity_trades_features = df_sanity_trades.filter(like='feature_') # Features\n",
    "df_sanity_trades_target = df_sanity_trades['bin'] # Targets\n",
    "\n",
    "prediction = mdl_loaded.predict(df_sanity_trades_features) # Predict only our selected CUSIPs\n",
    "\n",
    "calc_full_evaluation_w_prediction(prediction, df_sanity_trades, df_sanity_trades_features, df_sanity_trades_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_financial_performance_eval(df_sanity_trades.copy(deep=True), prediction, russel=True)\n",
    "full_financial_performance_eval(df_sanity_trades.copy(deep=True), prediction, russel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanity_included, df_sanity_excluded = get_inclusion_exclusion_df(df_sanity_trades, prediction, russel=False)\n",
    "\n",
    "print(df_sanity_included['profit_rel'].describe())\n",
    "print(df_sanity_excluded['profit_rel'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM as it achieved the highest performance\n",
    "\n",
    "# Load the data\n",
    "# Train\n",
    "df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "df_train_features = df_train.filter(like='feature_')\n",
    "df_train_target = df_train['bin']\n",
    "\n",
    "# Test\n",
    "df_test = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier = False)\n",
    "df_test_features = df_test.filter(like='feature_')\n",
    "df_test_target = df_test['bin']\n",
    "\n",
    "# Config for LGBM (as determined by Optuna)\n",
    "cfg = {\n",
    "    \"num_threads\": 8,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"lambda_l1\": 2.8269687100646643,\n",
    "    \"lambda_l2\": 0.1132729746179371,\n",
    "    \"num_leaves\": 222,\n",
    "    \"feature_fraction\": 0.6029600980261194,\n",
    "    \"bagging_fraction\": 0.9924346775670325,\n",
    "    \"bagging_freq\": 2,\n",
    "    \"min_child_samples\": 65,\n",
    "    \"max_depth\": 14,\n",
    "    \"max_bin\": 136,\n",
    "    \"learning_rate\": 0.30197097330510875,\n",
    "    \"importance_type\": \"gain\"\n",
    "}\n",
    "\n",
    "model_lgbm_final = lgb.LGBMClassifier(seed=42)\n",
    "model_lgbm_final.set_params(**cfg)\n",
    "\n",
    "fi = train_model_k_fold_feature_imp(model_lgbm_final, cfg, df_train_features, df_train_target)\n",
    "\n",
    "fi['Feature'] = fi['Feature'].str.replace('feature_', '')\n",
    "fi.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature importance\n",
    "# Set custom feature names\n",
    "custom_feature_names_gain = [\n",
    "    'Normalized Average True Range (NATR): Period = 4  ',\n",
    "    'NASDAQ Rolling Percentile Close Price  ',\n",
    "    'GDP  ',\n",
    "    'Fractional Differentiated Close Price  ',\n",
    "    'Silver Close Price  ',\n",
    "    'Treasury Yield 5 Years (^FVX)  ',\n",
    "    'Generalized Autoregressive  \\n Conditional Heteroskedasticity Close Price  ',\n",
    "    'Gold Rolling Percentile Close Price  ',\n",
    "    'Treasury Yield 10 Years (^TNX) 30-Day Moving Average  ',\n",
    "    'Inflation Rolling Percentile  ',\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=fi.sort_values(by=\"Value\", ascending=False)[:10], palette='viridis', \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "\n",
    "plt.suptitle('BB-Strategy - LGBM - 10 Most Important Features', fontsize=32)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(range(10), custom_feature_names_gain, fontsize=24)\n",
    "\n",
    "plt.xlabel('Importance', fontsize=30)\n",
    "plt.ylabel('Feature', fontsize=30)\n",
    "\n",
    "plt.subplots_adjust(top=0.9) \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(path_default + '/visuals/bb_feature_importance_lgbm_gain.svg', dpi='figure', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Train\n",
    "df_train = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "df_train_features = df_train.filter(like='feature_')\n",
    "df_train_target = df_train['bin']\n",
    "\n",
    "# Test\n",
    "df_test = load_ml_data(path_data_partitioned_tbm_with_fees_test, 'v1_df_ho.csv', vertical_barrier = True)\n",
    "df_test_features = df_test.filter(like='feature_')\n",
    "df_test_target = df_test['bin']\n",
    "\n",
    "\n",
    "# XGBoost achieved the highest performance\n",
    "model_xgb = xgb.XGBClassifier(n_jobs=8)\n",
    "\n",
    "# Config for XGBoost (as determined by Optuna)\n",
    "cfg_xgb = {\n",
    "    'lambda': 3.822976947706376,\n",
    "    'alpha': 0.4445173954046871,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 17,\n",
    "    'min_child_weight': 13,\n",
    "    'gamma': 0.3490986093851895,\n",
    "    'grow_policy': 'lossguide'\n",
    "}\n",
    "\n",
    "model_xgb.set_params(**cfg_xgb)\n",
    "\n",
    "fi = train_model_k_fold_feature_imp(model_xgb, cfg_xgb, df_train_features, df_train_target)\n",
    "\n",
    "fi['Feature'] = fi['Feature'].str.replace('feature_', '')\n",
    "fi.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and sort the feature importances\n",
    "fi = fi.groupby('Feature')['Value'].mean().reset_index()\n",
    "fi = fi.sort_values(by='Value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Custom feature names list for XGBoost\n",
    "custom_feature_names_xgboost = [\n",
    "    'S&P 500 New Lows: Period = 52 Days  ',\n",
    "    'NASDAQ Rolling Percentile Close Price  ',\n",
    "    'S&P 500 New Lows: Period = 52 Days; Rolling Percentile  ',\n",
    "    'USD Index Rolling Percentile Close Price  ',\n",
    "    'Crude Oil Rolling Percentile Close Price  ',\n",
    "    'Crude Oil Close Price  ',\n",
    "    'Normalized Average True Range (NATR): Period = 4  ',\n",
    "    'USD Index Close Price  ',\n",
    "    'Unemployment Rate  ',\n",
    "    'Treasury Yield 3 Months (^IRX)  '\n",
    "]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=fi[:10], palette='viridis', \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(range(10), custom_feature_names_xgboost, fontsize=24)\n",
    "\n",
    "plt.xlabel('Importance', fontsize=30)\n",
    "plt.ylabel('Feature', fontsize=30)\n",
    "\n",
    "plt.suptitle('TBM-Strategy - XGBoost - 10 Most Important Features', fontsize=32)\n",
    "plt.subplots_adjust(top=0.9)  # Adjust the top parameter as needed\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(path_default + '/visuals/tbm_feature_importance_xgboost.svg', dpi='figure', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10 most important features for BB-Strategy\n",
    "feature_names_bb = [\n",
    "    'feature_NATR',\n",
    "    'feature_NASDAQ_rolling_percentile',\n",
    "    'feature_GDP',\n",
    "    'feature_frac_diff_close',\n",
    "    'feature_Silver',\n",
    "    'feature_Treasury_Yield3',\n",
    "    'feature_garch_vol_close',\n",
    "    'feature_Gold_rolling_percentile',\n",
    "    'feature_treasury_yield1_ma30_rolling_percentile',\n",
    "    'feature_Inflation_rolling_percentile'\n",
    "]\n",
    "\n",
    "# Define 10 most important features for TBM-Strategy\n",
    "feature_names_tbm = [\n",
    "    'feature_SP500_new_lows',\n",
    "    'feature_NASDAQ_rolling_percentile',\n",
    "    'feature_SP500_new_lows_rolling_percentile',\n",
    "    'feature_USDIndex_rolling_percentile',\n",
    "    'feature_CrudeOil_rolling_percentile',\n",
    "    'feature_CrudeOil',\n",
    "    'feature_NATR',\n",
    "    'feature_USDIndex',\n",
    "    'feature_Unemployment',\n",
    "    'feature_Treasury_Yield2'\n",
    "]\n",
    "\n",
    "def inclusion_exclusion_analysis(inclusion, exclusion, feature_names, sides=False):\n",
    "    \"\"\"\n",
    "    Prints the summary statistics for the given features for the validated and rejected trades.\n",
    "\n",
    "    Args:\n",
    "        inclusion (dataframe): DataFrame containing the validated trades.\n",
    "        exclusion (dataframe): DataFrame containing the rejected trades.\n",
    "        feature_names (list): List of feature names, depending on the 10 most important features.\n",
    "        sides (boolean, optional): Whether to print the summary statistics for the two sides (-1 and 1). Defaults to False.\n",
    "    \"\"\"\n",
    "    if sides: # If sides, long and short trades exist and should be considered separately\n",
    "        side_1_inclusion = inclusion[inclusion['side'] == 1]\n",
    "        side_1_exclusion = exclusion[exclusion['side'] == 1]\n",
    "        side_minus_1_inclusion = inclusion[inclusion['side'] == -1]\n",
    "        side_minus_1_exclusion = exclusion[exclusion['side'] == -1]\n",
    "        \n",
    "    for feature in feature_names:\n",
    "        print('Inclusion Overall:')\n",
    "        print(inclusion[feature].describe())\n",
    "\n",
    "        print('Exclusion Overall:')\n",
    "        print(exclusion[feature].describe())\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "        if sides:\n",
    "            print('Inclusion Side 1:')\n",
    "            print(side_1_inclusion[feature].describe())\n",
    "            print('Exclusion Side 1:')\n",
    "            print(side_1_exclusion[feature].describe())\n",
    "\n",
    "            print('-'*50)\n",
    "\n",
    "            print('Inclusion Side -1:')\n",
    "            print(side_minus_1_inclusion[feature].describe())\n",
    "            print('Exclusion Side -1:')\n",
    "            print(side_minus_1_exclusion[feature].describe())\n",
    "            print('#'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Labeling w/ most promiment models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Prediction CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## XGB LGBM ##########\n",
    "# Train\n",
    "df_train = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "df_train_features = df_train.filter(like='feature_')\n",
    "df_train_target = df_train['bin']\n",
    "\n",
    "# Test\n",
    "df_test = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier = False)\n",
    "df_test_features = df_test.filter(like='feature_')\n",
    "df_test_target = df_test['bin']\n",
    "########## XGB LGBM ##########\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "df_train_features_nn = pd.read_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv')\n",
    "df_test_features_nn = pd.read_csv(path_data_partitioned_bb_with_fees_scaled + '/v1_df_ho_features_scaled_ss.csv')\n",
    "\n",
    "test_dataset = CustomStockDataSet(df_test_features_nn, df_test_target)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Load the best model\n",
    "path = '/home/lschiff_ext/data_v5/results/bb/neural_network_final_model/good'\n",
    "model_name = 'epoch_80_2023-07-14-14-40-29'\n",
    "\n",
    "full_path = path + '/' + model_name\n",
    "model = CustomMLP_BB(df_train_features_nn.shape[1])\n",
    "clf = load_model(model, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for all best models XGBoost, LGBM, and the ANN\n",
    "\n",
    "# XGBoost\n",
    "bb_clf_xgb = load_model_sklearn(path_data_final_models, 'BB_XGBoost')\n",
    "bb_pred_xgb = bb_clf_xgb.predict(df_test_features)\n",
    "bb_pred_xgb_proba = bb_clf_xgb.predict_proba(df_test_features)\n",
    "\n",
    "# LGBM\n",
    "bb_clf_lgbm = load_model_sklearn(path_data_final_models, 'BB_LGBM')\n",
    "bb_pred_lgbm = bb_clf_lgbm.predict(df_test_features)\n",
    "bb_pred_lgbm_proba = bb_clf_lgbm.predict_proba(df_test_features)\n",
    "\n",
    "# Neural Network\n",
    "clf.eval()\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "    for xb_test, yb_test in test_dl:                \n",
    "        pred_test = clf(xb_test)\n",
    "        threshold = 0.5\n",
    "        predicted_labels = (pred_test >= threshold).int()\n",
    "\n",
    "        predicted_labels_np = predicted_labels.cpu().numpy().flatten()  # or predicted_labels.squeeze()\n",
    "        predicted_labels_list.append(predicted_labels_np)\n",
    "pred_nn_bb = np.concatenate(predicted_labels_list)\n",
    "\n",
    "prediction_majority_bb = majority_vote(bb_pred_xgb, bb_pred_lgbm, pred_nn_bb)\n",
    "prediction_aggregation_bb = aggregation_vote(bb_pred_xgb, bb_pred_lgbm, pred_nn_bb)\n",
    "\n",
    "# Save predictions for aggregation and majority vote\n",
    "store_prediction_csv(prediction_majority_bb, 'BB_MAJORITY.csv')\n",
    "store_prediction_csv(prediction_aggregation_bb, 'BB_AGGREGATION.csv')\n",
    "\n",
    "# Save predictions and probability predictions\n",
    "store_prediction_csv(bb_pred_xgb, 'BB_XGB.csv')\n",
    "store_prediction_csv(bb_pred_xgb_proba, 'BB_XGB_PROBA.csv')\n",
    "store_prediction_csv(bb_pred_lgbm, 'BB_LGBM.csv')\n",
    "store_prediction_csv(bb_pred_lgbm_proba, 'BB_LGBM_PROBA.csv')\n",
    "store_prediction_csv(pred_nn_bb, 'BB_NN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data for the different machine learning algorithms\n",
    "\n",
    "########## Dataset XGB ##########\n",
    "# Train\n",
    "df_train_xgb = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "df_train_features_xgb = df_train_xgb.filter(like='feature_')\n",
    "df_train_target_xgb = df_train_xgb['bin']\n",
    "\n",
    "# Test\n",
    "df_test_xgb = load_ml_data(path_data_partitioned_tbm_with_fees_test, 'v1_df_ho.csv', vertical_barrier = True)\n",
    "df_test_features_xgb = df_test_xgb.filter(like='feature_')\n",
    "df_test_target_xgb = df_test_xgb['bin']\n",
    "########## Dataset XGB ##########\n",
    "\n",
    "########## Dataset LGBM ##########\n",
    "# Train\n",
    "df_train_lgbm = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "df_train_features_lgbm = pd.read_csv(path_data_partitioned_tbm_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv')\n",
    "df_train_target_lgbm = df_train_lgbm['bin']\n",
    "\n",
    "# Test\n",
    "df_test_lgbm = load_ml_data(path_data_partitioned_tbm_with_fees_test, 'v1_df_ho.csv', vertical_barrier = True)\n",
    "df_test_features_lgbm = pd.read_csv(path_data_partitioned_tbm_with_fees_scaled + '/v1_df_ho_features_scaled_ss.csv')\n",
    "df_test_target_lgbm = df_test_lgbm['bin']\n",
    "########## Dataset LGBM ##########\n",
    "\n",
    "# Neural Network\n",
    "df_train_features_nn = pd.read_csv(path_data_partitioned_tbm_with_fees_scaled + '/v1_df_ml_features_scaled_ss.csv')\n",
    "df_test_features_nn = pd.read_csv(path_data_partitioned_tbm_with_fees_scaled + '/v1_df_ho_features_scaled_ss.csv')\n",
    "\n",
    "test_dataset = CustomStockDataSet(df_test_features_nn, df_test_target_xgb)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "path = '/home/lschiff_ext/data_v5/results/tbm/neural_network_final_model'\n",
    "model_name = 'epoch_250'\n",
    "\n",
    "full_path = path + '/' + model_name\n",
    "model = CustomMLP_TBM(df_train_features_nn.shape[1])\n",
    "clf = load_model(model, full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "# XGBoost\n",
    "tbm_clf_xgb = load_model_sklearn(path_data_final_models, 'TBM_XGBoost')\n",
    "\n",
    "pred_xgb = tbm_clf_xgb.predict(df_test_features_xgb)\n",
    "\n",
    "# LGBM\n",
    "tbm_clf_lgbm = load_model_sklearn(path_data_final_models, 'TBM_LGBM')\n",
    "\n",
    "pred_lgbm = tbm_clf_lgbm.predict(df_test_features_lgbm)\n",
    "\n",
    "# Neural Network\n",
    "clf.eval()\n",
    "\n",
    "predicted_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb_test, yb_test in test_dl:                \n",
    "        pred_test = clf(xb_test)\n",
    "        threshold = 0.5\n",
    "        predicted_labels = (pred_test >= threshold).int()\n",
    "\n",
    "        predicted_labels_np = predicted_labels.cpu().numpy().flatten()  # or predicted_labels.squeeze()\n",
    "        predicted_labels_list.append(predicted_labels_np)\n",
    "pred_nn = np.concatenate(predicted_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for XGBoost, LGBM, and the ANN\n",
    "store_prediction_csv(pred_xgb, 'TBM_XGB.csv')\n",
    "store_prediction_csv(pred_lgbm, 'TBM_LGBM.csv')\n",
    "store_prediction_csv(pred_nn, 'TBM_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority and aggregation vote\n",
    "prediction_majority = majority_vote(pred_xgb, pred_lgbm, pred_nn)\n",
    "prediction_aggregation = aggregation_vote(pred_xgb, pred_lgbm, pred_nn)\n",
    "\n",
    "store_prediction_csv(prediction_majority, 'TBM_MAJORITY.csv')\n",
    "store_prediction_csv(prediction_aggregation, 'TBM_AGGREGATION.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted probabilities\n",
    "pred_xgb_proba = tbm_clf_xgb.predict_proba(df_test_features_xgb)\n",
    "pred_lgbm_proba = tbm_clf_lgbm.predict_proba(df_test_features_lgbm)\n",
    "store_prediction_csv(pred_xgb_proba, 'TBM_XGB_PROBA.csv')\n",
    "store_prediction_csv(pred_lgbm_proba, 'TBM_LGBM_PROBA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Meta-Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BB##\n",
    "# Train\n",
    "df_train_bb = load_ml_data(path_data_partitioned_bb_with_fees_train, 'v1_df_ml.csv', vertical_barrier = False)\n",
    "df_train_features_bb = df_train_bb.filter(like='feature_')\n",
    "df_train_target_bb = df_train_bb['bin']\n",
    "\n",
    "# Test\n",
    "df_test_bb = load_ml_data(path_data_partitioned_bb_with_fees_test, 'v1_df_ho.csv', vertical_barrier = False)\n",
    "df_test_features_bb = df_test_bb.filter(like='feature_')\n",
    "df_test_target_bb = df_test_bb['bin']\n",
    "\n",
    "##TBM##\n",
    "# Train\n",
    "df_train_tbm = load_ml_data(path_data_partitioned_tbm_with_fees_train, 'v1_df_ml.csv', vertical_barrier = True)\n",
    "df_train_features_tbm = df_train_tbm.filter(like='feature_')\n",
    "df_train_target_tbm = df_train_tbm['bin']\n",
    "\n",
    "# Test\n",
    "df_test_tbm = load_ml_data(path_data_partitioned_tbm_with_fees_test, 'v1_df_ho.csv', vertical_barrier = True)\n",
    "df_test_features_tbm = df_test_tbm.filter(like='feature_')\n",
    "df_test_target_tbm = df_test_tbm['bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with default threshold\n",
    "prediction_lgbm_bb = load_prediction_csv('BB_LGBM_0.5.csv')\n",
    "prediction_xgb_bb = load_prediction_csv('BB_XGB_0.5.csv')\n",
    "prediction_nn_bb = load_prediction_csv('BB_NN_0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to chose from\n",
    "prediction_bb = prediction_lgbm_bb\n",
    "# prediction_bb = prediction_xgb_bb\n",
    "# prediction_bb = prediction_nn_bb\n",
    "\n",
    "calc_full_evaluation_w_prediction(prediction_bb, df_test_bb, df_test_features_bb, df_test_target_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_financial_performance_eval(df_test_bb, prediction_lgbm_bb, fraction=False, russel=True)\n",
    "full_financial_performance_eval(df_test_bb, prediction_lgbm_bb, fraction=False, russel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with default threshold\n",
    "\n",
    "prediction_xgb_tbm= load_prediction_csv('TBM_XGB_0.5.csv')\n",
    "prediction_lgbm_tbm= load_prediction_csv('TBM_LGBM_0.5.csv')\n",
    "prediction_nn_tbm= load_prediction_csv('TBM_NN_0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to chose from\n",
    "prediction_tbm = prediction_xgb_tbm\n",
    "# prediction_tbm = prediction_lgbm_tbm\n",
    "# prediction_tbm = prediction_nn_tbm\n",
    "\n",
    "calc_full_evaluation_w_prediction(prediction_tbm, df_test_tbm, df_test_features_tbm, df_test_target_tbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_financial_performance_eval(df_test_tbm, prediction_xgb_tbm, fraction=False, russel=True)\n",
    "full_financial_performance_eval(df_test_tbm, prediction_xgb_tbm, fraction=False, russel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trader Types (heavy-, medium-, low-cautious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predicted probability for LGBM\n",
    "pred_proba_LGBM_bb = load_prediction_csv('BB_LGBM_PROBA.csv', proba=True)\n",
    "\n",
    "# Get trades based on thresholds\n",
    "betsizing_prediction_06 = label_predictions(pred_proba_LGBM_bb, 0.6)\n",
    "betsizing_prediction_07 = label_predictions(pred_proba_LGBM_bb, 0.7)\n",
    "betsizing_prediction_08 = label_predictions(pred_proba_LGBM_bb, 0.8)\n",
    "\n",
    "# Financial performance evaluation\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_06, fraction=True, russel=True)\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_06, fraction=True, russel=False)\n",
    "\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_07, fraction=True, russel=True)\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_07, fraction=True, russel=False)\n",
    "\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_08, fraction=True, russel=True)\n",
    "full_financial_performance_eval(df_test_bb, betsizing_prediction_08, fraction=True, russel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predicted probability for XGBoost\n",
    "pred_proba_XGB_tbm = load_prediction_csv('TBM_XGB_PROBA.csv', proba=True)\n",
    "\n",
    "# Get trades based on thresholds\n",
    "betsizing_prediction_06_tbm = label_predictions(pred_proba_XGB_tbm, 0.6)\n",
    "betsizing_prediction_07_tbm = label_predictions(pred_proba_XGB_tbm, 0.7)\n",
    "\n",
    "# Financial performance evaluation\n",
    "full_financial_performance_eval(df_test_tbm, betsizing_prediction_06_tbm, fraction=True, russel=True)\n",
    "full_financial_performance_eval(df_test_tbm, betsizing_prediction_06_tbm, fraction=True, russel=False)\n",
    "\n",
    "full_financial_performance_eval(df_test_tbm, betsizing_prediction_07_tbm, fraction=True, russel=True)\n",
    "full_financial_performance_eval(df_test_tbm, betsizing_prediction_07_tbm, fraction=True, russel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractional Trader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predicted probability for LGBM\n",
    "pred_proba_LGBM_bb = load_prediction_csv('BB_LGBM_PROBA.csv', proba=True)\n",
    "\n",
    "# Fractional investments\n",
    "betsizing_prediction_abs_frac_bb = label_predictions_fraction(pred_proba_LGBM_bb, 0.5, 0.9)\n",
    "\n",
    "# Evaluation\n",
    "full_financial_performance_eval(df_test_bb.copy(deep=True), betsizing_prediction_abs_frac_bb, fraction=True, russel=False)\n",
    "full_financial_performance_eval(df_test_bb.copy(deep=True), betsizing_prediction_abs_frac_bb, fraction=True, russel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predicted probability for XGBoost\n",
    "pred_proba_tbm_xgb = load_prediction_csv('TBM_XGB_PROBA.csv', proba=True)\n",
    "\n",
    "# Fractional investments\n",
    "betsizing_prediction_abs_frac_tbm = label_predictions_fraction(pred_proba_tbm_xgb, 0.5, 0.68)\n",
    "\n",
    "# Evaluation\n",
    "full_financial_performance_eval(df_test_tbm.copy(deep=True), betsizing_prediction_abs_frac_tbm, fraction=True, russel=False)\n",
    "full_financial_performance_eval(df_test_tbm.copy(deep=True), betsizing_prediction_abs_frac_tbm, fraction=True, russel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BB-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets \n",
    "df_train_before_2017 = df_train_bb[df_train_bb['date'] <= pd.to_datetime('2017-01-01')]\n",
    "df_train_after_2017 = df_train_bb[df_train_bb['date'] > pd.to_datetime('2017-01-01')]\n",
    "\n",
    "# Data for training the machine learning model\n",
    "df_train_before_2017_features = df_train_before_2017.filter(like='feature_')\n",
    "df_train_before_2017_target = df_train_before_2017['bin']\n",
    "\n",
    "# Data for determining the optimal threshold\n",
    "df_train_after_2017_features = df_train_after_2017.filter(like='feature_')\n",
    "df_train_after_2017_target = df_train_after_2017['bin']\n",
    "\n",
    "print(len(df_train_before_2017))\n",
    "print(len(df_train_after_2017))\n",
    "print('Size Training:', 1 - (len(df_train_after_2017) / len(df_train_before_2017)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Train LGBM as it is the most prominent model\n",
    "config_part_lgbm = {\n",
    "    \"num_threads\": 8,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"lambda_l1\": 2.8269687100646643,\n",
    "    \"lambda_l2\": 0.1132729746179371,\n",
    "    \"num_leaves\": 222,\n",
    "    \"feature_fraction\": 0.6029600980261194,\n",
    "    \"bagging_fraction\": 0.9924346775670325,\n",
    "    \"bagging_freq\": 2,\n",
    "    \"min_child_samples\": 65,\n",
    "    \"max_depth\": 14,\n",
    "    \"max_bin\": 136,\n",
    "    \"learning_rate\": 0.30197097330510875\n",
    "}\n",
    "\n",
    "betsizing_part = lgb.LGBMClassifier(seed=42)\n",
    "betsizing_part_trained = train_model(betsizing_part, config_part_lgbm, df_train_before_2017_features, df_train_before_2017_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betsizing_part_pred = betsizing_part_trained.predict_proba(df_train_after_2017_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_and_visualize_optimal_threshold(betsizing_part_pred, df_train_after_2017_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TBM-Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets \n",
    "df_train_before_2017_tbm = df_train_tbm[df_train_tbm['date'] <= pd.to_datetime('2017-01-01')]\n",
    "df_train_after_2017_tbm = df_train_tbm[df_train_tbm['date'] > pd.to_datetime('2017-01-01')]\n",
    "\n",
    "# Data for training the machine learning model\n",
    "df_train_before_2017_tbm_features = df_train_before_2017_tbm.filter(like='feature_')\n",
    "df_train_before_2017_tbm_target = df_train_before_2017_tbm['bin']\n",
    "\n",
    "# Data for determining the optimal threshold\n",
    "df_train_after_2017_tbm_features = df_train_after_2017_tbm.filter(like='feature_')\n",
    "df_train_after_2017_tbm_target = df_train_after_2017_tbm['bin']\n",
    "\n",
    "print(len(df_train_before_2017_tbm))\n",
    "print(len(df_train_after_2017_tbm))\n",
    "print('Size Training:', 1 - (len(df_train_after_2017_tbm) / len(df_train_before_2017_tbm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_part_xgb_tbm = {\n",
    "    'lambda': 3.822976947706376,\n",
    "    'alpha': 0.4445173954046871,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 17,\n",
    "    'min_child_weight': 13,\n",
    "    'gamma': 0.3490986093851895,\n",
    "    'grow_policy': 'lossguide'\n",
    "}\n",
    "\n",
    "betsizing_part_tbm = xgb.XGBClassifier(n_jobs=8)\n",
    "betsizing_part_trained_tbm = train_model(betsizing_part_tbm, config_part_xgb_tbm, df_train_before_2017_tbm_features, df_train_before_2017_tbm_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betsizing_part_pred_tbm = betsizing_part_trained_tbm.predict_proba(df_train_after_2017_tbm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_and_visualize_optimal_threshold(betsizing_part_pred_tbm, df_train_after_2017_tbm_target, 'TBM_OPT_THRESHOLD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Performance Meta-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "prediction_loaded_bb = load_prediction_csv('BB_LGBM_0.9.csv')\n",
    "prediction_loaded_tbm = load_prediction_csv('TBM_XGB_0.6.csv')\n",
    "\n",
    "# Visualize the performance of Meta-Labeling\n",
    "visualize_performance_meta_labeling(df_test_bb, prediction_loaded_bb, show_exclusion=True, bb=True)\n",
    "visualize_performance_meta_labeling(df_test_tbm, prediction_loaded_tbm, show_exclusion=True, bb=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-py-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
